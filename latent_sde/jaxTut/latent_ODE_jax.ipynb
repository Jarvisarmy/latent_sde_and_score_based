{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf2fbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import diffrax\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b3c6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    scale: jnp.ndarray\n",
    "    mlp: eqx.nn.MLP\n",
    "    def __call__(self, t, y,args):\n",
    "        return self.scale*self.mlp(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f05c251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentODE(eqx.Module):\n",
    "    func: Func\n",
    "    rnn_cell: eqx.nn.GRUCell\n",
    "        \n",
    "    hidden_to_latent: eqx.nn.Linear\n",
    "    latent_to_hidden: eqx.nn.MLP\n",
    "    hidden_to_data: eqx.nn.Linear\n",
    "    \n",
    "    hidden_size: int\n",
    "    latent_size: int\n",
    "    \n",
    "    def __init__(self, *, data_size, hidden_size, latent_size, width_size, depth, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        mkey, gkey, hlkey,lhkey, hdkey=jrandom.split(key,5)\n",
    "        \n",
    "        scale = jnp.ones(())\n",
    "        mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size,\n",
    "            out_size=hidden_size,\n",
    "            width_size = width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.softplus,\n",
    "            final_activation=jnn.tanh,\n",
    "            key=mkey)\n",
    "        self.func = Func(scale, mlp)\n",
    "        self.rnn_cell = eqx.nn.GRUCell(data_size+1,hidden_size,key=gkey)\n",
    "        self.hidden_to_latent = eqx.nn.Linear(hidden_size, 2*latent_size, key=hlkey)\n",
    "        self.latent_to_hidden = eqx.nn.MLP(\n",
    "            latent_size, hidden_size, width_size=width_size, depth=depth,key=lhkey)\n",
    "        self.hidden_to_data = eqx.nn.Linear(hidden_size, data_size, key=hdkey)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "    # Encoder of the VAE\n",
    "    def _latent(self,ts,ys,key):\n",
    "        print(ys.shape)\n",
    "        data = jnp.concatenate([ts[:,None],ys],axis=1)\n",
    "        hidden = jnp.zeros((self.hidden_size,))\n",
    "        for data_i in reversed(data):\n",
    "            hidden=self.rnn_cell(data_i,hidden)\n",
    "        context = self.hidden_to_latent(hidden)\n",
    "        mean, logstd = context[: self.latent_size], context[self.latent_size:]\n",
    "        std = jnp.exp(logstd)\n",
    "        latent = mean + jrandom.normal(key,(self.latent_size,))+std\n",
    "        return latent, mean, std\n",
    "    \n",
    "    # Decoder of the VAE\n",
    "    def _sample(self, ts, latent):\n",
    "        dt0 = 0.4\n",
    "        y0 = self.latent_to_hidden(latent)\n",
    "        sol = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(self.func),\n",
    "        diffrax.Tsit5(),\n",
    "        ts[0],\n",
    "        ts[-1],\n",
    "        dt0,\n",
    "        y0,\n",
    "        saveat = diffrax.SaveAt(ts=ts),\n",
    "        )\n",
    "        return jax.vmap(self.hidden_to_data)(sol.ys)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _loss(ys, pred_ys,mean, std):\n",
    "        #-log p_theta with Gaussian p_theta\n",
    "        reconstruction_loss = 0.5 * jnp.sum((ys-pred_ys)**2)\n",
    "        #KL(N(mean, std^2)||N(0,1))\n",
    "        variational_loss = 0.5 * jnp.sum(mean**2+std**2-2*jnp.log(std)-1)\n",
    "        return reconstruction_loss+variational_loss\n",
    "    \n",
    "    #run both encoder and decoder during training\n",
    "    def train(self, ts, ys, *, key):\n",
    "        latent, mean, std = self._latent(ts, ys, key)\n",
    "        pred_ys = self._sample(ts, latent)\n",
    "        print(self.hidden_size)\n",
    "        print(self.latent_size)\n",
    "        return self._loss(ys, pred_ys, mean, std)\n",
    "    \n",
    "    def sample(self, ts, *, key):\n",
    "        latent = jrandom.normal(key, (self.latent_size,))\n",
    "        return self._sample(ts, latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae7a18af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset_size, *, key):\n",
    "    ykey,tkey1,tkey2 = jrandom.split(key,3)\n",
    "    y0 = jrandom.normal(ykey,(dataset_size,2))\n",
    "    \n",
    "    t0 = 0\n",
    "    t1 = 2 + jrandom.uniform(tkey1, (dataset_size,))\n",
    "    ts = jrandom.uniform(tkey2, (dataset_size, 20))*(t1[:,None]-t0)+t0\n",
    "    ts = jnp.sort(ts)\n",
    "    dt0 = 0.1\n",
    "    \n",
    "    def func(t,y,args):\n",
    "        return jnp.array([[-0.1,1.3],[-1,-0.1]])@y\n",
    "    def solve(ts,y0):\n",
    "        sol = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(func),\n",
    "            diffrax.Tsit5(),\n",
    "            ts[0],\n",
    "            ts[-1],\n",
    "            dt0,\n",
    "            y0,\n",
    "            saveat=diffrax.SaveAt(ts=ts))\n",
    "        return sol.ys\n",
    "    ys = jax.vmap(solve)(ts,y0)\n",
    "    return ts, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7cb37b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key,1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while start < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb362ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=10000,\n",
    "    batch_size=256,\n",
    "    lr=1e-2,\n",
    "    steps=250,\n",
    "    save_every=50,\n",
    "    hidden_size=16,\n",
    "    latent_size=16,\n",
    "    width_size=16,\n",
    "    depth=2,\n",
    "    seed=5678):\n",
    "    key=jrandom.PRNGKey(seed)\n",
    "    data_key, model_key, loader_key, train_key, sample_key = jrandom.split(key,5)\n",
    "    \n",
    "    ts, ys = get_data(dataset_size, key=data_key)\n",
    "    \n",
    "    model = LatentODE(\n",
    "        data_size = ys.shape[-1],\n",
    "        hidden_size = hidden_size,\n",
    "        latent_size = latent_size,\n",
    "        width_size = width_size,\n",
    "        depth = depth,\n",
    "        key=model_key\n",
    "    )\n",
    "    \n",
    "    @eqx.filter_value_and_grad\n",
    "    def loss(model, ts_i, ys_i, key_i):\n",
    "        batch_size, _ = ts_i.shape\n",
    "        key_i = jrandom.split(key_i, batch_size)\n",
    "        loss = jax.vmap(model.train)(ts_i, ys_i, key=key_i)\n",
    "        return jnp.mean(loss)\n",
    "    \n",
    "    @eqx.filter_jit\n",
    "    def make_step(model, opt_state, ts_i, ys_i, key_i):\n",
    "        value, grads = loss(model,ts_i,ys_i,key_i)\n",
    "        key_i = jrandom.split(key_i,1)[0]\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model=eqx.apply_updates(model,updates)\n",
    "        return value, model, opt_state, key_i\n",
    "    optim=optax.adam(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    \n",
    "    # plot results\n",
    "    num_plots = 1 + (steps-1)//save_every\n",
    "    if ((steps-1) % save_every) != 0:\n",
    "        num_plots += 1\n",
    "    fig, axs = plt.subplots(1, num_plots, figsize=(num_plots*8,8))\n",
    "    axs[0].set_ylabel(\"x\")\n",
    "    axs = iter(axs)\n",
    "    for step, (ts_i, ys_i) in zip(\n",
    "        range(steps), dataloader((ts, ys), batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        value, model, opt_state, train_key = make_step(\n",
    "            model, opt_state, ts_i, ys_i, train_key\n",
    "        )\n",
    "        end = time.time()\n",
    "        print(f\"Step: {step}, Loss: {value}, Computation time: {end-start}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9c9028c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n",
      "16\n",
      "16\n",
      "Step: 0, Loss: 19.482112884521484, Computation time: 30.831073999404907\n",
      "Step: 1, Loss: 17.960769653320312, Computation time: 0.21074295043945312\n",
      "Step: 2, Loss: 16.635345458984375, Computation time: 0.19555902481079102\n",
      "Step: 3, Loss: 17.64173126220703, Computation time: 0.2215440273284912\n",
      "Step: 4, Loss: 15.866716384887695, Computation time: 0.22096920013427734\n",
      "Step: 5, Loss: 15.14842414855957, Computation time: 0.21159720420837402\n",
      "Step: 6, Loss: 16.314502716064453, Computation time: 0.20553207397460938\n",
      "Step: 7, Loss: 16.91912841796875, Computation time: 0.2049100399017334\n",
      "Step: 8, Loss: 15.079574584960938, Computation time: 0.24156713485717773\n",
      "Step: 9, Loss: 16.18557357788086, Computation time: 0.21551895141601562\n",
      "Step: 10, Loss: 16.1568660736084, Computation time: 0.23908090591430664\n",
      "Step: 11, Loss: 16.7288818359375, Computation time: 0.21463394165039062\n",
      "Step: 12, Loss: 14.11604118347168, Computation time: 0.203779935836792\n",
      "Step: 13, Loss: 13.984370231628418, Computation time: 0.22671723365783691\n",
      "Step: 14, Loss: 13.773767471313477, Computation time: 0.236008882522583\n",
      "Step: 15, Loss: 14.47520923614502, Computation time: 0.2924790382385254\n",
      "Step: 16, Loss: 15.190902709960938, Computation time: 0.27584409713745117\n",
      "Step: 17, Loss: 13.979574203491211, Computation time: 0.2042992115020752\n",
      "Step: 18, Loss: 13.951742172241211, Computation time: 0.20758986473083496\n",
      "Step: 19, Loss: 14.24600887298584, Computation time: 0.2015681266784668\n",
      "Step: 20, Loss: 14.177043914794922, Computation time: 0.20928096771240234\n",
      "Step: 21, Loss: 14.978548049926758, Computation time: 0.20642471313476562\n",
      "Step: 22, Loss: 14.111551284790039, Computation time: 0.20915699005126953\n",
      "Step: 23, Loss: 15.105043411254883, Computation time: 0.21117472648620605\n",
      "Step: 24, Loss: 13.52741813659668, Computation time: 0.20541691780090332\n",
      "Step: 25, Loss: 13.38006591796875, Computation time: 0.20393109321594238\n",
      "Step: 26, Loss: 13.81917953491211, Computation time: 0.21855998039245605\n",
      "Step: 27, Loss: 14.907642364501953, Computation time: 0.20557522773742676\n",
      "Step: 28, Loss: 13.553312301635742, Computation time: 0.2020552158355713\n",
      "Step: 29, Loss: 14.09337043762207, Computation time: 0.19494009017944336\n",
      "Step: 30, Loss: 13.352798461914062, Computation time: 0.2152388095855713\n",
      "Step: 31, Loss: 12.109443664550781, Computation time: 0.23874592781066895\n",
      "Step: 32, Loss: 12.48763370513916, Computation time: 0.2194352149963379\n",
      "Step: 33, Loss: 13.558029174804688, Computation time: 0.20797204971313477\n",
      "Step: 34, Loss: 12.688719749450684, Computation time: 0.20362496376037598\n",
      "Step: 35, Loss: 12.306097030639648, Computation time: 0.20048999786376953\n",
      "Step: 36, Loss: 11.536935806274414, Computation time: 0.24251604080200195\n",
      "Step: 37, Loss: 11.18437385559082, Computation time: 0.20534110069274902\n",
      "Step: 38, Loss: 11.177680969238281, Computation time: 0.2029402256011963\n",
      "(20, 2)\n",
      "16\n",
      "16\n",
      "Step: 39, Loss: 11.254154205322266, Computation time: 31.914369106292725\n",
      "Step: 40, Loss: 12.288330078125, Computation time: 0.24042201042175293\n",
      "Step: 41, Loss: 9.671284675598145, Computation time: 0.20868730545043945\n",
      "Step: 42, Loss: 10.078980445861816, Computation time: 0.21303391456604004\n",
      "Step: 43, Loss: 10.117303848266602, Computation time: 0.21967577934265137\n",
      "Step: 44, Loss: 9.991775512695312, Computation time: 0.2192211151123047\n",
      "Step: 45, Loss: 11.018047332763672, Computation time: 0.21270108222961426\n",
      "Step: 46, Loss: 10.483514785766602, Computation time: 0.21459674835205078\n",
      "Step: 47, Loss: 9.940736770629883, Computation time: 0.20038294792175293\n",
      "Step: 48, Loss: 9.906524658203125, Computation time: 0.19660592079162598\n",
      "Step: 49, Loss: 9.909563064575195, Computation time: 0.20226001739501953\n",
      "Step: 50, Loss: 9.62540054321289, Computation time: 0.19977402687072754\n",
      "Step: 51, Loss: 11.206018447875977, Computation time: 0.18886089324951172\n",
      "Step: 52, Loss: 9.731892585754395, Computation time: 0.18918085098266602\n",
      "Step: 53, Loss: 9.581342697143555, Computation time: 0.19111204147338867\n",
      "Step: 54, Loss: 9.665020942687988, Computation time: 0.19161772727966309\n",
      "Step: 55, Loss: 9.84173583984375, Computation time: 0.18962502479553223\n",
      "Step: 56, Loss: 9.729458808898926, Computation time: 0.2507200241088867\n",
      "Step: 57, Loss: 9.474752426147461, Computation time: 0.2091832160949707\n",
      "Step: 58, Loss: 9.988285064697266, Computation time: 0.20656895637512207\n",
      "Step: 59, Loss: 8.900632858276367, Computation time: 0.19557809829711914\n",
      "Step: 60, Loss: 9.387994766235352, Computation time: 0.21939420700073242\n",
      "Step: 61, Loss: 9.667349815368652, Computation time: 0.22928285598754883\n",
      "Step: 62, Loss: 8.435627937316895, Computation time: 0.23487305641174316\n",
      "Step: 63, Loss: 8.260770797729492, Computation time: 0.20024490356445312\n",
      "Step: 64, Loss: 7.668797492980957, Computation time: 0.21094012260437012\n",
      "Step: 65, Loss: 8.57659912109375, Computation time: 0.21805191040039062\n",
      "Step: 66, Loss: 8.467872619628906, Computation time: 0.20946407318115234\n",
      "Step: 67, Loss: 7.628918647766113, Computation time: 0.24769806861877441\n",
      "Step: 68, Loss: 7.288234233856201, Computation time: 0.2826979160308838\n",
      "Step: 69, Loss: 7.488124847412109, Computation time: 0.2166299819946289\n",
      "Step: 70, Loss: 6.948555946350098, Computation time: 0.21284198760986328\n",
      "Step: 71, Loss: 6.838623046875, Computation time: 0.2286221981048584\n",
      "Step: 72, Loss: 7.05958890914917, Computation time: 0.21346116065979004\n",
      "Step: 73, Loss: 6.989760875701904, Computation time: 0.2094128131866455\n",
      "Step: 74, Loss: 6.4438371658325195, Computation time: 0.2077808380126953\n",
      "Step: 75, Loss: 6.995630741119385, Computation time: 0.20392680168151855\n",
      "Step: 76, Loss: 7.254870414733887, Computation time: 0.20949912071228027\n",
      "Step: 77, Loss: 7.419741153717041, Computation time: 0.21355795860290527\n",
      "Step: 78, Loss: 7.013830661773682, Computation time: 0.21161580085754395\n",
      "Step: 79, Loss: 6.318853855133057, Computation time: 0.011527776718139648\n",
      "Step: 80, Loss: 7.146777629852295, Computation time: 0.20041608810424805\n",
      "Step: 81, Loss: 7.7252912521362305, Computation time: 0.20940494537353516\n",
      "Step: 82, Loss: 5.966458320617676, Computation time: 0.20714187622070312\n",
      "Step: 83, Loss: 6.713625907897949, Computation time: 0.19173312187194824\n",
      "Step: 84, Loss: 6.868672847747803, Computation time: 0.19046592712402344\n",
      "Step: 85, Loss: 7.439686298370361, Computation time: 0.21486592292785645\n",
      "Step: 86, Loss: 6.879055976867676, Computation time: 0.19808006286621094\n",
      "Step: 87, Loss: 6.823169231414795, Computation time: 0.2057170867919922\n",
      "Step: 88, Loss: 6.504678726196289, Computation time: 0.20717835426330566\n",
      "Step: 89, Loss: 6.466638565063477, Computation time: 0.21713566780090332\n",
      "Step: 90, Loss: 6.7744550704956055, Computation time: 0.22379207611083984\n",
      "Step: 91, Loss: 6.803152561187744, Computation time: 0.21091294288635254\n",
      "Step: 92, Loss: 6.207840919494629, Computation time: 0.21088314056396484\n",
      "Step: 93, Loss: 6.422861099243164, Computation time: 0.19193124771118164\n",
      "Step: 94, Loss: 6.797328472137451, Computation time: 0.20771503448486328\n",
      "Step: 95, Loss: 6.814284324645996, Computation time: 0.2038428783416748\n",
      "Step: 96, Loss: 6.746269226074219, Computation time: 0.21340489387512207\n",
      "Step: 97, Loss: 6.1337995529174805, Computation time: 0.1996920108795166\n",
      "Step: 98, Loss: 6.361691474914551, Computation time: 0.19580292701721191\n",
      "Step: 99, Loss: 7.002104759216309, Computation time: 0.20477294921875\n",
      "Step: 100, Loss: 6.391733169555664, Computation time: 0.19961023330688477\n",
      "Step: 101, Loss: 6.520180702209473, Computation time: 0.1983029842376709\n",
      "Step: 102, Loss: 6.553465843200684, Computation time: 0.19739985466003418\n",
      "Step: 103, Loss: 6.818975448608398, Computation time: 0.19636893272399902\n",
      "Step: 104, Loss: 6.854165077209473, Computation time: 0.2552947998046875\n",
      "Step: 105, Loss: 6.874214172363281, Computation time: 0.23953986167907715\n",
      "Step: 106, Loss: 6.183367729187012, Computation time: 0.26305198669433594\n",
      "Step: 107, Loss: 6.335947513580322, Computation time: 0.30252885818481445\n",
      "Step: 108, Loss: 6.750389099121094, Computation time: 0.27503466606140137\n",
      "Step: 109, Loss: 6.10984992980957, Computation time: 0.27321386337280273\n",
      "Step: 110, Loss: 6.228828430175781, Computation time: 0.29695582389831543\n",
      "Step: 111, Loss: 5.960825443267822, Computation time: 0.32848215103149414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 112, Loss: 6.169217109680176, Computation time: 0.30501699447631836\n",
      "Step: 113, Loss: 6.406663417816162, Computation time: 0.28444576263427734\n",
      "Step: 114, Loss: 6.378798484802246, Computation time: 0.2903621196746826\n",
      "Step: 115, Loss: 6.351482391357422, Computation time: 0.31445908546447754\n",
      "Step: 116, Loss: 6.595810890197754, Computation time: 0.29839491844177246\n",
      "Step: 117, Loss: 6.161502838134766, Computation time: 0.2770218849182129\n",
      "Step: 118, Loss: 6.804346561431885, Computation time: 0.3129448890686035\n",
      "Step: 119, Loss: 7.225455284118652, Computation time: 0.020895957946777344\n",
      "Step: 120, Loss: 6.257194519042969, Computation time: 0.3013949394226074\n",
      "Step: 121, Loss: 7.068605422973633, Computation time: 0.28760290145874023\n",
      "Step: 122, Loss: 6.232015132904053, Computation time: 0.2793281078338623\n",
      "Step: 123, Loss: 6.145999431610107, Computation time: 0.2897300720214844\n",
      "Step: 124, Loss: 5.845921516418457, Computation time: 0.28724217414855957\n",
      "Step: 125, Loss: 6.306858062744141, Computation time: 0.28586292266845703\n",
      "Step: 126, Loss: 5.96650505065918, Computation time: 0.28011584281921387\n",
      "Step: 127, Loss: 6.534751892089844, Computation time: 0.19453692436218262\n",
      "Step: 128, Loss: 6.45329475402832, Computation time: 0.19577288627624512\n",
      "Step: 129, Loss: 6.116827964782715, Computation time: 0.20404696464538574\n",
      "Step: 130, Loss: 5.576370716094971, Computation time: 0.19287610054016113\n",
      "Step: 131, Loss: 6.225164890289307, Computation time: 0.20307183265686035\n",
      "Step: 132, Loss: 5.987030506134033, Computation time: 0.1879420280456543\n",
      "Step: 133, Loss: 5.972967624664307, Computation time: 0.19210004806518555\n",
      "Step: 134, Loss: 6.407665252685547, Computation time: 0.20610713958740234\n",
      "Step: 135, Loss: 7.033924102783203, Computation time: 0.19305801391601562\n",
      "Step: 136, Loss: 6.198084354400635, Computation time: 0.18807315826416016\n",
      "Step: 137, Loss: 6.716902256011963, Computation time: 0.1887040138244629\n",
      "Step: 138, Loss: 6.0700860023498535, Computation time: 0.1945178508758545\n",
      "Step: 139, Loss: 6.21695613861084, Computation time: 0.1973743438720703\n",
      "Step: 140, Loss: 6.8034257888793945, Computation time: 0.2159442901611328\n",
      "Step: 141, Loss: 6.786716461181641, Computation time: 0.21730303764343262\n",
      "Step: 142, Loss: 5.388322353363037, Computation time: 0.21234512329101562\n",
      "Step: 143, Loss: 6.450357437133789, Computation time: 0.2163100242614746\n",
      "Step: 144, Loss: 6.014034748077393, Computation time: 0.20240998268127441\n",
      "Step: 145, Loss: 6.531123638153076, Computation time: 0.2191481590270996\n",
      "Step: 146, Loss: 6.749584197998047, Computation time: 0.19672608375549316\n",
      "Step: 147, Loss: 6.521866798400879, Computation time: 0.1949162483215332\n",
      "Step: 148, Loss: 6.134194374084473, Computation time: 0.19783282279968262\n",
      "Step: 149, Loss: 6.083452224731445, Computation time: 0.19353008270263672\n",
      "Step: 150, Loss: 6.674676895141602, Computation time: 0.19237923622131348\n",
      "Step: 151, Loss: 5.912102699279785, Computation time: 0.19360804557800293\n",
      "Step: 152, Loss: 6.596961975097656, Computation time: 0.19958901405334473\n",
      "Step: 153, Loss: 5.76674747467041, Computation time: 0.19100522994995117\n",
      "Step: 154, Loss: 5.982477188110352, Computation time: 0.19170618057250977\n",
      "Step: 155, Loss: 5.938075542449951, Computation time: 0.19074583053588867\n",
      "Step: 156, Loss: 6.593103408813477, Computation time: 0.19124817848205566\n",
      "Step: 157, Loss: 5.98663330078125, Computation time: 0.1882638931274414\n",
      "Step: 158, Loss: 5.80424690246582, Computation time: 0.20144200325012207\n",
      "Step: 159, Loss: 4.621593952178955, Computation time: 0.011787891387939453\n",
      "Step: 160, Loss: 6.911352157592773, Computation time: 0.18883204460144043\n",
      "Step: 161, Loss: 6.846405506134033, Computation time: 0.1879560947418213\n",
      "Step: 162, Loss: 6.381555557250977, Computation time: 0.18669390678405762\n",
      "Step: 163, Loss: 6.468010902404785, Computation time: 0.18947672843933105\n",
      "Step: 164, Loss: 6.460968971252441, Computation time: 0.187089204788208\n",
      "Step: 165, Loss: 6.237325668334961, Computation time: 0.2043771743774414\n",
      "Step: 166, Loss: 7.082381248474121, Computation time: 0.18791890144348145\n",
      "Step: 167, Loss: 6.345493793487549, Computation time: 0.19615697860717773\n",
      "Step: 168, Loss: 6.737118721008301, Computation time: 0.19745373725891113\n",
      "Step: 169, Loss: 6.574723243713379, Computation time: 0.19724082946777344\n",
      "Step: 170, Loss: 5.956143856048584, Computation time: 0.1987602710723877\n",
      "Step: 171, Loss: 6.342442989349365, Computation time: 0.2060708999633789\n",
      "Step: 172, Loss: 6.123729705810547, Computation time: 0.19494080543518066\n",
      "Step: 173, Loss: 6.312410354614258, Computation time: 0.21683883666992188\n",
      "Step: 174, Loss: 6.130875587463379, Computation time: 0.20211005210876465\n",
      "Step: 175, Loss: 6.448850631713867, Computation time: 0.24134016036987305\n",
      "Step: 176, Loss: 6.089473724365234, Computation time: 0.23274493217468262\n",
      "Step: 177, Loss: 6.137579917907715, Computation time: 0.18841099739074707\n",
      "Step: 178, Loss: 7.088449478149414, Computation time: 0.21424198150634766\n",
      "Step: 179, Loss: 6.866613388061523, Computation time: 0.1921710968017578\n",
      "Step: 180, Loss: 6.356998443603516, Computation time: 0.19646096229553223\n",
      "Step: 181, Loss: 7.023855209350586, Computation time: 0.2082052230834961\n",
      "Step: 182, Loss: 6.520721435546875, Computation time: 0.20049571990966797\n",
      "Step: 183, Loss: 6.102374076843262, Computation time: 0.2104499340057373\n",
      "Step: 184, Loss: 6.207854270935059, Computation time: 0.20697498321533203\n",
      "Step: 185, Loss: 6.35212516784668, Computation time: 0.1936180591583252\n",
      "Step: 186, Loss: 6.2846174240112305, Computation time: 0.19039511680603027\n",
      "Step: 187, Loss: 6.088983535766602, Computation time: 0.19943785667419434\n",
      "Step: 188, Loss: 6.206953048706055, Computation time: 0.19184494018554688\n",
      "Step: 189, Loss: 5.91361141204834, Computation time: 0.18884897232055664\n",
      "Step: 190, Loss: 6.183475494384766, Computation time: 0.1940479278564453\n",
      "Step: 191, Loss: 6.029545783996582, Computation time: 0.1886909008026123\n",
      "Step: 192, Loss: 5.704067230224609, Computation time: 0.1895291805267334\n",
      "Step: 193, Loss: 5.782899856567383, Computation time: 0.20336413383483887\n",
      "Step: 194, Loss: 6.208311080932617, Computation time: 0.19832086563110352\n",
      "Step: 195, Loss: 5.99731969833374, Computation time: 0.21603012084960938\n",
      "Step: 196, Loss: 5.509870529174805, Computation time: 0.22677397727966309\n",
      "Step: 197, Loss: 6.134128093719482, Computation time: 0.19077515602111816\n",
      "Step: 198, Loss: 6.59400749206543, Computation time: 0.19460701942443848\n",
      "Step: 199, Loss: 5.332633018493652, Computation time: 0.01129293441772461\n",
      "Step: 200, Loss: 6.4422407150268555, Computation time: 0.20917606353759766\n",
      "Step: 201, Loss: 6.8016157150268555, Computation time: 0.21127700805664062\n",
      "Step: 202, Loss: 6.687093734741211, Computation time: 0.20969414710998535\n",
      "Step: 203, Loss: 5.952511310577393, Computation time: 0.19074511528015137\n",
      "Step: 204, Loss: 5.996792793273926, Computation time: 0.19060301780700684\n",
      "Step: 205, Loss: 6.591979026794434, Computation time: 0.1869950294494629\n",
      "Step: 206, Loss: 6.556117057800293, Computation time: 0.19257426261901855\n",
      "Step: 207, Loss: 6.40277099609375, Computation time: 0.19431495666503906\n",
      "Step: 208, Loss: 7.13128662109375, Computation time: 0.1959688663482666\n",
      "Step: 209, Loss: 5.827635765075684, Computation time: 0.18914580345153809\n",
      "Step: 210, Loss: 6.3435444831848145, Computation time: 0.19219207763671875\n",
      "Step: 211, Loss: 6.404075622558594, Computation time: 0.1908562183380127\n",
      "Step: 212, Loss: 6.115781784057617, Computation time: 0.19046902656555176\n",
      "Step: 213, Loss: 6.465731620788574, Computation time: 0.18807387351989746\n",
      "Step: 214, Loss: 6.46750545501709, Computation time: 0.1897118091583252\n",
      "Step: 215, Loss: 6.637653827667236, Computation time: 0.18939208984375\n",
      "Step: 216, Loss: 6.395382404327393, Computation time: 0.1890430450439453\n",
      "Step: 217, Loss: 6.006659507751465, Computation time: 0.20255589485168457\n",
      "Step: 218, Loss: 6.179080963134766, Computation time: 0.1916511058807373\n",
      "Step: 219, Loss: 6.746583461761475, Computation time: 0.20334100723266602\n",
      "Step: 220, Loss: 5.847721099853516, Computation time: 0.18860960006713867\n",
      "Step: 221, Loss: 6.2990007400512695, Computation time: 0.19582486152648926\n",
      "Step: 222, Loss: 6.183192253112793, Computation time: 0.1965627670288086\n",
      "Step: 223, Loss: 5.620089530944824, Computation time: 0.2249600887298584\n",
      "Step: 224, Loss: 6.7282233238220215, Computation time: 0.19200801849365234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 225, Loss: 5.872195243835449, Computation time: 0.19579219818115234\n",
      "Step: 226, Loss: 6.237074375152588, Computation time: 0.19539308547973633\n",
      "Step: 227, Loss: 6.00838565826416, Computation time: 0.19423294067382812\n",
      "Step: 228, Loss: 5.898082256317139, Computation time: 0.19440197944641113\n",
      "Step: 229, Loss: 6.299516201019287, Computation time: 0.20044398307800293\n",
      "Step: 230, Loss: 5.771559715270996, Computation time: 0.19233369827270508\n",
      "Step: 231, Loss: 6.503746509552002, Computation time: 0.20082616806030273\n",
      "Step: 232, Loss: 6.475078105926514, Computation time: 0.2140789031982422\n",
      "Step: 233, Loss: 6.124886989593506, Computation time: 0.19525694847106934\n",
      "Step: 234, Loss: 5.548612594604492, Computation time: 0.2600209712982178\n",
      "Step: 235, Loss: 6.255526542663574, Computation time: 0.2736990451812744\n",
      "Step: 236, Loss: 5.459768295288086, Computation time: 0.22895479202270508\n",
      "Step: 237, Loss: 6.938872337341309, Computation time: 0.20845794677734375\n",
      "Step: 238, Loss: 6.075936794281006, Computation time: 0.20508575439453125\n",
      "Step: 239, Loss: 6.180604934692383, Computation time: 0.011507987976074219\n",
      "Step: 240, Loss: 5.635254383087158, Computation time: 0.20830798149108887\n",
      "Step: 241, Loss: 7.051305294036865, Computation time: 0.20722508430480957\n",
      "Step: 242, Loss: 6.218793869018555, Computation time: 0.20776033401489258\n",
      "Step: 243, Loss: 5.98191499710083, Computation time: 0.2007429599761963\n",
      "Step: 244, Loss: 6.060275554656982, Computation time: 0.21353411674499512\n",
      "Step: 245, Loss: 5.876873970031738, Computation time: 0.1977090835571289\n",
      "Step: 246, Loss: 5.945865154266357, Computation time: 0.19495177268981934\n",
      "Step: 247, Loss: 6.047980308532715, Computation time: 0.23755192756652832\n",
      "Step: 248, Loss: 5.965570449829102, Computation time: 0.2135024070739746\n",
      "Step: 249, Loss: 6.053548336029053, Computation time: 0.20889711380004883\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3456x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83acd8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
