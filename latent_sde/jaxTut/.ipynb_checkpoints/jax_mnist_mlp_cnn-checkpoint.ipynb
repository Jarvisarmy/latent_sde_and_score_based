{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "710e8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "from jax.experimental import optimizers\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12485429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c95e5a991f4b97b78122c8b6969612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1321236416248e2af40f92a0a16c61b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abce8fb435dd4b3aabd1e2e639f0731d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e78d4ae84024f27b9f5e64f9ba9c356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data',train=True, download=True,transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,),(0.3081,))\n",
    "    ])),batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data',train=False, download=True,transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,),(0.3081,))\n",
    "    ])),batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc50f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e86610fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d80b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_layer(params, x):\n",
    "    return ReLU(np.dot(params[0],x)+params[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ad84c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_mlp(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    def initialize_layer(m,n,key,scale=1e-2):\n",
    "        w_key, b_key = random.split(key)\n",
    "        return scale*random.normal(w_key,(n,m)), scale*random.normal(b_key,(n,))\n",
    "    return [initialize_layer(m,n,k) for m,n,k in zip(sizes[:-1],sizes[1:],keys)]\n",
    "\n",
    "layer_sizes = [784,512,512,10]\n",
    "params = initialize_mlp(layer_sizes,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9dab1dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 784)\n",
      "(512,)\n",
      "(512, 512)\n",
      "(512,)\n",
      "(10, 512)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(params[0][0].shape)\n",
    "print(params[0][1].shape)\n",
    "print(params[1][0].shape)\n",
    "print(params[1][1].shape)\n",
    "print(params[2][0].shape)\n",
    "print(params[2][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546df7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33925bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(params, in_array):\n",
    "    ''' compute the forward pass for each example individually'''\n",
    "    activations = in_array\n",
    "    # loop over the ReLU hidden layers\n",
    "    for w,b in params[:-1]:\n",
    "        activations = relu_layer([w,b],activations)\n",
    "    # performs final trafo to logits\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = np.dot(final_w,activations)+final_b\n",
    "    return logits-logsumexp(logits)\n",
    "\n",
    "batch_forward = vmap(forward_pass, in_axes=(None,0),out_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "467ce9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x,k,dtype=np.float32):\n",
    "    ''' create a one=hot encoding of x of size k'''\n",
    "    return np.array(x[:,None]==np.arange(k),dtype)\n",
    "def loss(params, in_arrays, targets):\n",
    "    '''compute the multi-class cross-entorpy loss'''\n",
    "    preds = batch_forward(params, in_arrays)\n",
    "    return -np.sum(preds*targets)\n",
    "def accuracy(params, data_loader):\n",
    "    acc_total=0\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        images = np.array(data).reshape(data.size(0),28*28)\n",
    "        targets = one_hot(np.array(target),num_classes)\n",
    "        \n",
    "        target_class = np.argmax(targets, axis=1)\n",
    "        predicted_class = np.argmax(batch_forward(params,images),axis=1)\n",
    "        acc_total += np.sum(predicted_class==target_class)\n",
    "    return acc_total/len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f10cd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "jax.value_and_grad(fun) create a function that evaluates both fun and the gradient of fun\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "    init_func(params): \n",
    "        args: \n",
    "            params: pytree representing the initial parameters\n",
    "        returns:\n",
    "            a pytree representing the initial optimizer state, which includes the initial parameters and may also include auxiliary values like initial momentum\n",
    "    update_fun(step, grads, opt_state)\n",
    "        args:\n",
    "            step: integer representing the step index\n",
    "            grads: a pytree representing the gradients to be used in updating the optimizer state\n",
    "            opt_state: a pytree representing the optimizer state to be updated\n",
    "        returns:\n",
    "            a pytree with the same structure as the 'opt_state' argument representing the updated optimizer state\n",
    "    get_params(opt_state):\n",
    "        args:\n",
    "            opt_state: pytree representing an optimizer state\n",
    "        returns:\n",
    "            a pytree representing the parameters extracted from 'opt_state', such that the invariant 'params==get_params(init_fun(params))' holds true\n",
    "        \n",
    "'''\n",
    "@jit\n",
    "def update(params, x, y, opt_state):\n",
    "    ''' compute the gradient for a batch and update the parameter'''\n",
    "    value, grads = value_and_grad(loss)(params, x, y)\n",
    "    opt_state = opt_update(0,grads,opt_state)\n",
    "    return get_params(opt_state),opt_state, value\n",
    "step_size=1e-3\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "num_epochs = 10\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7957ab54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | T: 12.47 | Train A: 0.970 | Test A: 0.963\n",
      "Epoch 2 | T: 13.18 | Train A: 0.983 | Test A: 0.975\n",
      "Epoch 3 | T: 12.52 | Train A: 0.990 | Test A: 0.979\n",
      "Epoch 4 | T: 12.51 | Train A: 0.991 | Test A: 0.978\n",
      "Epoch 5 | T: 12.73 | Train A: 0.994 | Test A: 0.980\n",
      "Epoch 6 | T: 12.70 | Train A: 0.996 | Test A: 0.981\n",
      "Epoch 7 | T: 12.42 | Train A: 0.997 | Test A: 0.983\n",
      "Epoch 8 | T: 12.50 | Train A: 0.996 | Test A: 0.979\n",
      "Epoch 9 | T: 12.87 | Train A: 0.997 | Test A: 0.979\n",
      "Epoch 10 | T: 12.83 | Train A: 0.998 | Test A: 0.980\n"
     ]
    }
   ],
   "source": [
    "def run_mnist_training_loop(num_epochs, opt_state, net_type=\"MLP\"):\n",
    "    '''implements a learning loop over epochs'''\n",
    "    # initialize placeholder for loggin\n",
    "    log_acc_train, log_acc_test, train_loss = [], [], []\n",
    "    # get the initial set of parameters\n",
    "    params = get_params(opt_state)\n",
    "    \n",
    "    # get initial accuracy after random init\n",
    "    train_acc = accuracy(params, train_loader)\n",
    "    test_acc = accuracy(params, test_loader)\n",
    "    log_acc_train.append(train_acc)\n",
    "    log_acc_test.append(test_acc)\n",
    "    \n",
    "    # loop over the training epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if net_type==\"MLP\":\n",
    "                x = np.array(data).reshape(data.size(0),28*28)\n",
    "            elif net_type == \"CNN\":\n",
    "                x=np.array(data)\n",
    "            y = one_hot(np.array(target),num_classes)\n",
    "            params, opt_state, loss = update(params, x, y, opt_state)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "        epoch_time = time.time() - start_time\n",
    "        train_acc = accuracy(params, train_loader)\n",
    "        test_acc = accuracy(params, test_loader)\n",
    "        log_acc_train.append(train_acc)\n",
    "        log_acc_test.append(test_acc)\n",
    "        print(\"Epoch {} | T: {:0.2f} | Train A: {:0.3f} | Test A: {:0.3f}\".format(epoch+1, epoch_time,train_acc, test_acc))\n",
    "    return train_loss, log_acc_train, log_acc_test\n",
    "\n",
    "train_loss, train_log, test_log = run_mnist_training_loop(num_epochs, opt_state, net_type=\"MLP\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf2d03f",
   "metadata": {},
   "source": [
    "# using the stax API to build Sequential Models - case study: A CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b2df8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/jax/experimental/stax.py:28: FutureWarning: jax.experimental.stax is deprecated, import jax.example_libraries.stax instead\n",
      "  warnings.warn('jax.experimental.stax is deprecated, '\n"
     ]
    }
   ],
   "source": [
    "from jax.experimental import stax\n",
    "from jax.experimental.stax import (BatchNorm, Conv, Dense, Flatten, Relu, LogSoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a44623e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe output returns a function to initialize the parameters of the network as well as a function to apply the forward pass through\\nthe network with. WHen initializing we have to specify the shape of the desired input as well as the batch dimension. Similarly\\nas before we can then proceed to define the loss and accuracy. The only difference compared to the MLP case is that we no longer flatten the image\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_fun, conv_net = stax.serial(Conv(32,(5,5),(2,2),padding=\"SAME\"),\n",
    "                                BatchNorm(),Relu,\n",
    "                                Conv(32, (5,5), (2,2), padding=\"SAME\"),\n",
    "                                BatchNorm(),Relu,\n",
    "                                Conv(10, (3,3),(2,2),padding=\"SAME\"),\n",
    "                                BatchNorm(),Relu,\n",
    "                                Conv(10, (3,3),(2,2),padding=\"SAME\"),Relu,\n",
    "                                Flatten,\n",
    "                                Dense(num_classes),\n",
    "                                LogSoftmax)\n",
    "_, params = init_fun(key, (batch_size, 1,28,28))\n",
    "'''\n",
    "The output returns a function to initialize the parameters of the network as well as a function to apply the forward pass through\n",
    "the network with. WHen initializing we have to specify the shape of the desired input as well as the batch dimension. Similarly\n",
    "as before we can then proceed to define the loss and accuracy. The only difference compared to the MLP case is that we no longer flatten the image\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af657604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 28, 32)\n",
      "(1, 1, 1, 32)\n"
     ]
    }
   ],
   "source": [
    "print(params[0][0].shape)\n",
    "print(params[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2af8166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(params, data_loader):\n",
    "    \"\"\"\n",
    "    compute the accuracy for the CNN case (no flattening of input)\n",
    "    \"\"\"\n",
    "    acc_total=0\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        images = np.array(data)\n",
    "        targets = one_hot(np.array(target),num_classes)\n",
    "        target_class = np.argmax(targets, axis=1)\n",
    "        predicted_class = np.argmax(conv_net(params,images),axis=1)\n",
    "        acc_total += np.sum(predicted_class == target_class)\n",
    "    return acc_total/len(data_loader.dataset)\n",
    "\n",
    "def loss(params, images, targets):\n",
    "    preds = conv_net(params, images)\n",
    "    return -np.sum(preds*targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f05b470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | T: 16.85 | Train A: 0.970 | Test A: 0.965\n",
      "Epoch 2 | T: 13.81 | Train A: 0.980 | Test A: 0.976\n",
      "Epoch 3 | T: 13.80 | Train A: 0.984 | Test A: 0.977\n",
      "Epoch 4 | T: 13.89 | Train A: 0.987 | Test A: 0.979\n",
      "Epoch 5 | T: 14.52 | Train A: 0.989 | Test A: 0.981\n",
      "Epoch 6 | T: 13.85 | Train A: 0.990 | Test A: 0.981\n",
      "Epoch 7 | T: 14.69 | Train A: 0.992 | Test A: 0.981\n",
      "Epoch 8 | T: 14.07 | Train A: 0.993 | Test A: 0.982\n",
      "Epoch 9 | T: 14.24 | Train A: 0.993 | Test A: 0.983\n",
      "Epoch 10 | T: 14.16 | Train A: 0.994 | Test A: 0.981\n"
     ]
    }
   ],
   "source": [
    "step_size = 1e-3\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "opt_state = opt_init(params)\n",
    "num_epochs = 10\n",
    "\n",
    "train_loss, train_log, test_log = run_mnist_training_loop(num_epochs,opt_state,net_type=\"CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b48b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
