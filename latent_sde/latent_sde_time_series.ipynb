{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4dd7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from typing import Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import distributions, nn, optim\n",
    "\n",
    "import torchsde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0aeebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the gpu is available or not, if yes, use gpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2902c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the tuple for data, \n",
    "Data = namedtuple('Data', ['ts_', 'ts_ext_', 'ts_vis_', 'ts', 'ts_ext', 'ts_vis', 'ys', 'ys_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9198a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"train_iters\": 1000,\n",
    "    \"pause_iters\": 50,\n",
    "    \"hide_ticks\": False,\n",
    "    \"save_ckpt\":True,\n",
    "    \"likelihood\":\"laplace\",\n",
    "    \"scale\": 0.001,\n",
    "    \"adjoint\": True,\n",
    "    \"debug\": True,\n",
    "    \"seed\": 42,\n",
    "    'data':'segmented_cosine',\n",
    "    \"dt\": 1e-2,\n",
    "    \"batch_size\": 256,\n",
    "    \"method\": 'euler',\n",
    "    \"adaptive\": 'False',\n",
    "    \"rtol\": 1e-3,\n",
    "    \"atol\": 1e-3\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40420fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output values from 0 to maxval with iters steps\n",
    "class LinearScheduler(object):\n",
    "    def __init__(self, iters, maxval=1.0):\n",
    "        self._iters = max(1,iters)\n",
    "        self._val = maxval/self._iters\n",
    "        self._maxval = maxval\n",
    "    \n",
    "    def step(self):\n",
    "        self._val = min(self._maxval, self._val+self._maxval/self._iters)\n",
    "        \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93c09c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMAMetric(object):\n",
    "    def __init__(self, gamma: Optional[float]=0.99):\n",
    "        super(EMAMetric, self).__init__()\n",
    "        self._val=0\n",
    "        self._gamma = gamma\n",
    "    def step(self, x:Union[torch.Tensor, np.ndarray]):\n",
    "        x = x.detach().cpu().numpy() if torch.is_tensor(x) else x\n",
    "        self._val = self._gamma * self._val + (1-self._gamma)*x\n",
    "        return self._val\n",
    "    \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2a4ec86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "def manual_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98a2f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the dvision is stable\n",
    "def _stable_division(a,b,epsilon=1e-7):\n",
    "    b = torch.where(b.abs().detach() > epsilon, b, torch.full_like(b, fill_value=epsilon)*b.sign())\n",
    "    return a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "522d3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentSDE(torchsde.SDEIto): # sde with ito calculus\n",
    "    def __init__(self, theta=1.0, mu=0.0, sigma=1):\n",
    "        super(LatentSDE, self).__init__(noise_type=\"diagonal\")\n",
    "        logvar = math.log(sigma ** 2/(2.*theta))\n",
    "        \n",
    "        # prior drift\n",
    "        #self.register_buffer(\"theta\",torch.tensor([[theta]])) # prior parameters, register 成buffer, 参数不会进行更新\n",
    "        self.register_buffer(\"mu\", torch.tensor([[mu]]))\n",
    "        self.register_buffer(\"sigma\",torch.tensor([[sigma]]))\n",
    "        \n",
    "        # p(y0)\n",
    "        self.register_buffer(\"py0_mean\", torch.tensor([[mu]]))\n",
    "        self.register_buffer(\"py0_logvar\", torch.tensor([[logvar]]))\n",
    "        \n",
    "        # approximate posterior drift: Takes in 2 positional encodings and the state\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3,200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200,200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200,1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialization the parameters\n",
    "        self.net[-1].weight.data.fill_(0.) # 初始化最后一层的参数\n",
    "        self.net[-1].bias.data.fill_(0.)\n",
    "            \n",
    "        # q(y0)\n",
    "        self.qy0_mean = nn.Parameter(torch.tensor([[mu]]), requires_grad=True) # 创建parameters\n",
    "        self.qy0_logvar = nn.Parameter(torch.tensor([[logvar]]), requires_grad=True) # 创建parameters\n",
    "        self.theta = nn.Parameter(torch.tensor([[theta]]),requires_grad=True)\n",
    "        #self.theta = nn.Parameter(torch.tensor([[theta]]),requires_grad=True)\n",
    "            \n",
    "    def f(self, t, y):  # Approximate posterior drift.\n",
    "        if t.dim() == 0:\n",
    "            t = torch.full_like(y, fill_value=t) # create a tensor of t\n",
    "        # Positional encoding in transformers for time-inhomogeneous posterior.\n",
    "        return self.net(torch.cat((torch.sin(t), torch.cos(t), y), dim=-1))\n",
    "\n",
    "    def g(self, t, y):  # Shared diffusion.\n",
    "        return self.sigma.repeat(y.size(0), 1) # 重复复制, 创建一个size为[y.size[0],1]\n",
    "\n",
    "    def h(self, t, y):  # Prior drift.\n",
    "        return self.theta * (self.mu - y)\n",
    "\n",
    "    def f_aug(self, t, y):  # Drift for augmented dynamics with logqp term.\n",
    "        y = y[:, 0:1] # 提取第一列，保持列的形态\n",
    "        f, g, h = self.f(t, y), self.g(t, y), self.h(t, y)\n",
    "        u = _stable_division(f - h, g) # 计算u(z,t)\n",
    "        f_logqp = .5 * (u ** 2).sum(dim=1, keepdim=True) # 计算integral\n",
    "        return torch.cat([f, f_logqp], dim=1)\n",
    "\n",
    "    def g_aug(self, t, y):  # Diffusion for augmented dynamics with logqp term.\n",
    "        y = y[:, 0:1]\n",
    "        g = self.g(t, y)\n",
    "        g_logqp = torch.zeros_like(y)\n",
    "        return torch.cat([g, g_logqp], dim=1)\n",
    "\n",
    "    def forward(self, ts, batch_size, eps=None):\n",
    "        eps = torch.randn(batch_size, 1).to(self.qy0_std) if eps is None else eps\n",
    "        y0 = self.qy0_mean + eps * self.qy0_std # the latent variable\n",
    "        qy0 = distributions.Normal(loc=self.qy0_mean, scale=self.qy0_std) # approximate posterior distribution\n",
    "        py0 = distributions.Normal(loc=self.py0_mean, scale=self.py0_std) # prior distribution\n",
    "        logqp0 = distributions.kl_divergence(qy0, py0).sum(dim=1)  # KL(t=0). calculate the kl divergence\n",
    "        #print(y0.size()) # (256, 1)\n",
    "        aug_y0 = torch.cat([y0, torch.zeros(batch_size, 1).to(y0)], dim=1) # create the augmented initial value\n",
    "        #print(aug_y0.size()) # [256, 2]\n",
    "        aug_ys = sdeint_fn(\n",
    "            sde=self,\n",
    "            y0=aug_y0,\n",
    "            ts=ts,\n",
    "            method=args['method'],\n",
    "            dt=args['dt'],\n",
    "            adaptive=args['adaptive'],\n",
    "            rtol=args['rtol'],\n",
    "            atol=args['atol'],\n",
    "            names={'drift': 'f_aug', 'diffusion': 'g_aug'}\n",
    "        ) # call the sde solver to \n",
    "        # print(aug_ys.size()) # [22, 256, 2]\n",
    "        ys, logqp_path = aug_ys[:, :, 0:1], aug_ys[-1, :, 1] # get the integral of the u(z,t) at the last time\n",
    "        \n",
    "        logqp = (logqp0 + logqp_path).mean(dim=0)  # KL(t=0) + KL(path).\n",
    "        return ys, logqp\n",
    "\n",
    "    def sample_p(self, ts, batch_size, eps=None, bm=None):\n",
    "        eps = torch.randn(batch_size, 1).to(self.py0_mean) if eps is None else eps\n",
    "        y0 = self.py0_mean + eps * self.py0_std\n",
    "        return sdeint_fn(self, y0, ts, bm=bm, method=args['method'], dt=args['dt'], names={'drift': 'h'}) # prior sde\n",
    "\n",
    "    def sample_q(self, ts, batch_size, eps=None, bm=None):\n",
    "        eps = torch.randn(batch_size, 1).to(self.qy0_mean) if eps is None else eps\n",
    "        y0 = self.qy0_mean + eps * self.qy0_std\n",
    "        return sdeint_fn(self, y0, ts, bm=bm, method=args['method'], dt=args['dt']) # posterior sde\n",
    "\n",
    "    @property\n",
    "    def py0_std(self):\n",
    "        return torch.exp(.5 * self.py0_logvar)\n",
    "\n",
    "    @property\n",
    "    def qy0_std(self):\n",
    "        return torch.exp(.5 * self.qy0_logvar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80736fff",
   "metadata": {},
   "source": [
    "## 5. Simulation of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd15aaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = 30\n",
    "#beta = 0.1694\n",
    "beta=1\n",
    "def make_time_series():\n",
    "    \n",
    "    \n",
    "    phi = 0.9805\n",
    "    sigma_v = 0.003342\n",
    "    sigma_u = 0.00528\n",
    "    rho = -0.856\n",
    "    cov_uv = rho * sigma_u * sigma_v\n",
    "\n",
    "    # generating shocks\n",
    "    mu = [0,0]\n",
    "    cov = [[sigma_u**2, cov_uv], [cov_uv, sigma_v**2]]\n",
    "    shocks = np.random.multivariate_normal(mu, cov, T)\n",
    "\n",
    "    z0 = np.random.normal(0, sigma_u**2/(1-phi**2),1)\n",
    "    r0 = shocks[0][0]\n",
    "\n",
    "    z = np.zeros(T)\n",
    "    r = np.zeros(T)\n",
    "    z[0] = z0\n",
    "    r[0] = r0\n",
    "\n",
    "    for idx_t in range(T-1):\n",
    "        z[idx_t+1] = phi*z[idx_t] + shocks[idx_t+1][1]\n",
    "        r[idx_t+1] = beta*z[idx_t] + shocks[idx_t+1][0]\n",
    "    return z, r\n",
    "z, r = make_time_series()\n",
    "plt.figure(figsize=(15,5))\n",
    "xvalues = np.array(range(T))\n",
    "plt.plot(xvalues, r, linestyle='-', color='g', label=\"observed $r_t$\")\n",
    "plt.plot(xvalues, z, linestyle=\"--\", color=\"r\", label=r\"hidden variable $\\beta * z_t$\", linewidth=3.0)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('%')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Simulated $r_t$ and hidden $\\beta * z_t$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0cf9fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    ts_ = np.linspace(0.1,3.0,30)\n",
    "    ts_ext_ = np.array([0.] + list(ts_) + [3.1])\n",
    "    ts_vis_ = np.linspace(0.1, 3.1, 31)\n",
    "    ys_ = r[:,None]\n",
    "    ts = torch.tensor(ts_).float()\n",
    "    ts_ext = torch.tensor(ts_ext_).float()\n",
    "    ts_vis = torch.tensor(ts_vis_).float()\n",
    "    ys = torch.tensor(ys_).float().to(device)\n",
    "    return Data(ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "99a1fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Dataset\n",
    "    ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_ = make_data()\n",
    "    mu = torch.mean(ys)\n",
    "    sigma = torch.std(ys)\n",
    "    \n",
    "    # plotting parameters\n",
    "    vis_batch_size = 1024\n",
    "    ylims = (-0.02, 0.02)\n",
    "    alphas = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55]\n",
    "    percentiles = [0.999, 0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    vis_idx = np.random.permutation(vis_batch_size)\n",
    "    sample_colors = ('#8c96c6', '#8c6bb1', '#810f7c')\n",
    "    fill_color = '#9ebcda'\n",
    "    mean_color = '#4d004b'\n",
    "    num_samples = len(sample_colors)\n",
    "    \n",
    "    eps = torch.randn(vis_batch_size, 1).to(device)\n",
    "    bm = torchsde.BrownianInterval(\n",
    "        t0=ts_vis[0],\n",
    "        t1=ts_vis[-1],\n",
    "        size=(vis_batch_size,1),\n",
    "        device=device,\n",
    "        levy_area_approximation=\"space-time\")\n",
    "    \n",
    "    # Model\n",
    "    model = LatentSDE(theta=0.01,mu=mu,sigma=sigma).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    kl_scheduler = LinearScheduler(iters=100)\n",
    "    criterion = nn.BCELoss(reduction='sum')\n",
    "    \n",
    "    logpy_metric = EMAMetric()\n",
    "    kl_metric = EMAMetric()\n",
    "    loss_metric = EMAMetric()\n",
    "    \n",
    "    \n",
    "    # show prior\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        zs = model.sample_p(ts=ts_vis, batch_size = vis_batch_size, eps = eps, bm=bm).squeeze()\n",
    "       \n",
    "        ts_vis_, zs_ = ts_vis.cpu().numpy(), zs.cpu().numpy()\n",
    "        #plt.scatter(ts_vis_, ys_, color='r', label=\"prior\")\n",
    "        zs_ = np.sort(zs_,axis=1)\n",
    "        img_dir = os.path.join('./sim/','prior.png')\n",
    "        plt.subplot(frameon=False)\n",
    "        for alpha, percentile in zip(alphas, percentiles):\n",
    "            idx = int((1 - percentile) / 2. * vis_batch_size)\n",
    "            zs_bot_ = zs_[:, idx]\n",
    "            zs_top_ = zs_[:, -idx]\n",
    "            plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
    "\n",
    "        # `zorder` determines who's on top; the larger the more at the top.\n",
    "        \n",
    "        plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # Data.\n",
    "        plt.plot(ts_, ys_, marker='x', zorder=3, color='k',label=\"observed $r_t$\")\n",
    "        plt.plot(ts_, z[:,None], color='g', linewidth=3.0, label=r\"hidden variable $z_t$\")\n",
    "        plt.ylim(ylims)\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$Y_t$')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.savefig(img_dir, dpi=300)\n",
    "        plt.close()\n",
    "        logging.info(f'Saved prior figure at: {img_dir}')\n",
    "    \n",
    "    \n",
    "    for global_step in tqdm.tqdm(range(args['train_iters'])):\n",
    "        \n",
    "        # Plot and save.\n",
    "        if global_step % args['pause_iters'] == 0:\n",
    "            img_path = os.path.join(\"./sim/\", f'global_step_{global_step}.png')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                zs = model.sample_q(ts=ts_vis, batch_size=vis_batch_size, eps=eps, bm=bm).squeeze()\n",
    "                samples = zs[:, vis_idx]\n",
    "                ts_vis_, zs_, samples_ = ts_vis.cpu().numpy(), zs.cpu().numpy(), samples.cpu().numpy()\n",
    "                zs_ = np.sort(zs_, axis=1)\n",
    "                plt.subplot(frameon=False)\n",
    "\n",
    "                if True: # args.show_percentiles:\n",
    "                    for alpha, percentile in zip(alphas, percentiles):\n",
    "                        idx = int((1 - percentile) / 2. * vis_batch_size)\n",
    "                        zs_bot_, zs_top_ = zs_[:, idx], zs_[:, -idx]\n",
    "                        plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
    "\n",
    "                if True: #args.show_mean:\n",
    "                    plt.plot(ts_vis_, zs_.mean(axis=1), color=mean_color)\n",
    "\n",
    "                if True: #args.show_samples:\n",
    "                    #for j in range(num_samples):\n",
    "                    #    plt.plot(ts_vis_, samples_[:, j], color=sample_colors[j], linewidth=1.0)\n",
    "                    plt.plot(ts_, z[:,None], color='g', linewidth=3.0, label=r\"hidden variable $z_t$\")\n",
    "                    plt.plot(ts_vis_, samples_.mean(axis=1), color='r',label=r'mean of latent variables')\n",
    "\n",
    "                if True: #args.show_arrows:\n",
    "                    num, dt = 3, 0.12\n",
    "                    t, y = torch.meshgrid(\n",
    "                        [torch.linspace(0, 3, num).to(device), torch.linspace(-0.3, 0.3, num).to(device)]\n",
    "                    )\n",
    "                    t, y = t.reshape(-1, 1), y.reshape(-1, 1)\n",
    "                    fty = model.f(t=t, y=y).reshape(num, num)\n",
    "                    dt = torch.zeros(num, num).fill_(dt).to(device)\n",
    "                    dy = fty * dt\n",
    "                    dt_, dy_, t_, y_ = dt.cpu().numpy(), dy.cpu().numpy(), t.cpu().numpy(), y.cpu().numpy()\n",
    "                    plt.quiver(t_, y_, dt_, dy_, alpha=0.3, edgecolors='k', width=0.0035, scale=50)\n",
    "\n",
    "                if False: #args.hide_ticks:\n",
    "                    plt.xticks([], [])\n",
    "                    plt.yticks([], [])\n",
    "\n",
    "                plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # Data.\n",
    "                plt.plot(ts_, ys_, marker='x', zorder=3, color='k', label=\"observed $r_t$ \") # new added\n",
    "            \n",
    "\n",
    "                \n",
    "                plt.ylim(ylims)\n",
    "                plt.xlabel('$t$')\n",
    "                plt.ylabel('$Y_t$')\n",
    "                plt.tight_layout()\n",
    "                plt.legend()\n",
    "                plt.savefig(img_path, dpi=300)\n",
    "                plt.close()\n",
    "                logging.info(f'Saved figure at: {img_path}')\n",
    "\n",
    "                \n",
    "        \n",
    "        # Train.\n",
    "        optimizer.zero_grad() # zero the gradient\n",
    "        zs, kl = model(ts=ts_ext, batch_size=args['batch_size']) # pass through the model, ys, logqp\n",
    "        zs = zs.squeeze() # remove the dimensions of input of size 1\n",
    "        zs = zs[1:-1]  # Drop first and last which are only used to penalize out-of-data region and spread uncertainty.\n",
    "\n",
    "        likelihood_constructor = {\"laplace\": distributions.Laplace, \"normal\": distributions.Normal}[args['likelihood']]\n",
    "        likelihood = likelihood_constructor(loc=zs, scale=args['scale']) # create the laplace distribution\n",
    "        logpy = likelihood.log_prob(ys).sum(dim=0).mean(dim=0) # got the log likelihood\n",
    "\n",
    "        loss = -logpy + kl * kl_scheduler.val\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        kl_scheduler.step()\n",
    "\n",
    "        logpy_metric.step(logpy)\n",
    "        kl_metric.step(kl)\n",
    "        loss_metric.step(loss)\n",
    "\n",
    "        logging.info(\n",
    "            f'global_step: {global_step}, '\n",
    "            f'logpy: {logpy_metric.val:.3f}, '\n",
    "            f'kl: {kl_metric.val:.3f}, '\n",
    "            f'loss: {loss_metric.val:.3f}'\n",
    "        )\n",
    "    torch.save(\n",
    "        {'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'kl_scheduler': kl_scheduler},\n",
    "        os.path.join('./sim/', f'global_step_{global_step}.ckpt')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "74b55c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved prior figure at: ./sim/prior.png\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]INFO:root:Saved figure at: ./sim/global_step_0.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 0, logpy: -13.885, kl: 0.000, loss: 13.885\n",
      "  0%|          | 1/1000 [00:02<37:27,  2.25s/it]INFO:root:global_step: 1, logpy: -146.428, kl: 11.745, loss: 146.663\n",
      "  0%|          | 2/1000 [00:03<32:00,  1.92s/it]INFO:root:global_step: 2, logpy: -309.519, kl: 29.783, loss: 310.296\n",
      "  0%|          | 3/1000 [00:04<27:50,  1.68s/it]INFO:root:global_step: 3, logpy: -319.507, kl: 29.634, loss: 320.282\n",
      "  0%|          | 4/1000 [00:05<25:27,  1.53s/it]INFO:root:global_step: 4, logpy: -768.355, kl: 200.808, loss: 777.696\n",
      "  0%|          | 5/1000 [00:26<1:59:29,  7.21s/it]INFO:root:global_step: 5, logpy: -1003.608, kl: 242.296, loss: 1015.466\n",
      "  1%|          | 6/1000 [00:27<1:29:15,  5.39s/it]INFO:root:global_step: 6, logpy: -1032.761, kl: 242.276, loss: 1044.668\n",
      "  1%|          | 7/1000 [00:28<1:09:24,  4.19s/it]INFO:root:global_step: 7, logpy: -1071.296, kl: 245.513, loss: 1083.537\n",
      "  1%|          | 8/1000 [00:29<54:42,  3.31s/it]  INFO:root:global_step: 8, logpy: -1122.319, kl: 251.078, loss: 1135.159\n",
      "  1%|          | 9/1000 [00:31<44:29,  2.69s/it]INFO:root:global_step: 9, logpy: -1154.440, kl: 255.318, loss: 1167.827\n",
      "  1%|          | 10/1000 [00:32<37:23,  2.27s/it]INFO:root:global_step: 10, logpy: -1157.625, kl: 256.705, loss: 1171.312\n",
      "  1%|          | 11/1000 [00:33<33:09,  2.01s/it]INFO:root:global_step: 11, logpy: -1163.729, kl: 256.153, loss: 1177.520\n",
      "  1%|          | 12/1000 [00:35<30:32,  1.85s/it]INFO:root:global_step: 12, logpy: -1177.797, kl: 256.124, loss: 1191.780\n",
      "  1%|▏         | 13/1000 [00:36<28:50,  1.75s/it]INFO:root:global_step: 13, logpy: -1192.939, kl: 258.528, loss: 1207.477\n",
      "  1%|▏         | 14/1000 [00:38<27:29,  1.67s/it]INFO:root:global_step: 14, logpy: -1206.688, kl: 263.496, loss: 1222.214\n",
      "  2%|▏         | 15/1000 [00:39<26:12,  1.60s/it]INFO:root:global_step: 15, logpy: -1219.198, kl: 268.889, loss: 1235.853\n",
      "  2%|▏         | 16/1000 [00:41<25:32,  1.56s/it]INFO:root:global_step: 16, logpy: -1228.125, kl: 272.254, loss: 1245.642\n",
      "  2%|▏         | 17/1000 [00:42<25:31,  1.56s/it]INFO:root:global_step: 17, logpy: -1230.735, kl: 273.631, loss: 1248.815\n",
      "  2%|▏         | 18/1000 [00:44<25:48,  1.58s/it]INFO:root:global_step: 18, logpy: -1226.838, kl: 272.379, loss: 1245.019\n",
      "  2%|▏         | 19/1000 [00:46<27:17,  1.67s/it]INFO:root:global_step: 19, logpy: -1220.323, kl: 270.718, loss: 1238.535\n",
      "  2%|▏         | 20/1000 [00:48<31:19,  1.92s/it]INFO:root:global_step: 20, logpy: -1219.613, kl: 269.871, loss: 1238.034\n",
      "  2%|▏         | 21/1000 [00:50<30:26,  1.87s/it]INFO:root:global_step: 21, logpy: -1223.829, kl: 269.877, loss: 1242.661\n",
      "  2%|▏         | 22/1000 [00:52<29:46,  1.83s/it]INFO:root:global_step: 22, logpy: -1228.478, kl: 269.803, loss: 1247.725\n",
      "  2%|▏         | 23/1000 [00:54<29:53,  1.84s/it]INFO:root:global_step: 23, logpy: -1229.862, kl: 269.378, loss: 1249.462\n",
      "  2%|▏         | 24/1000 [00:56<30:03,  1.85s/it]INFO:root:global_step: 24, logpy: -1225.223, kl: 268.332, loss: 1245.040\n",
      "  2%|▎         | 25/1000 [00:58<30:36,  1.88s/it]INFO:root:global_step: 25, logpy: -1214.538, kl: 267.274, loss: 1234.578\n",
      "  3%|▎         | 26/1000 [01:00<31:16,  1.93s/it]INFO:root:global_step: 26, logpy: -1206.890, kl: 268.103, loss: 1227.675\n",
      "  3%|▎         | 27/1000 [01:01<30:49,  1.90s/it]INFO:root:global_step: 27, logpy: -1202.445, kl: 270.187, loss: 1224.357\n",
      "  3%|▎         | 28/1000 [01:03<30:03,  1.86s/it]INFO:root:global_step: 28, logpy: -1198.730, kl: 272.907, loss: 1221.995\n",
      "  3%|▎         | 29/1000 [01:05<29:16,  1.81s/it]INFO:root:global_step: 29, logpy: -1193.753, kl: 275.654, loss: 1218.428\n",
      "  3%|▎         | 30/1000 [01:07<29:21,  1.82s/it]INFO:root:global_step: 30, logpy: -1187.021, kl: 277.504, loss: 1212.878\n",
      "  3%|▎         | 31/1000 [01:09<29:42,  1.84s/it]INFO:root:global_step: 31, logpy: -1178.594, kl: 277.639, loss: 1205.123\n",
      "  3%|▎         | 32/1000 [01:11<30:21,  1.88s/it]INFO:root:global_step: 32, logpy: -1167.305, kl: 277.034, loss: 1194.285\n",
      "  3%|▎         | 33/1000 [01:13<31:14,  1.94s/it]INFO:root:global_step: 33, logpy: -1159.043, kl: 275.828, loss: 1186.286\n",
      "  3%|▎         | 34/1000 [01:15<32:20,  2.01s/it]INFO:root:global_step: 34, logpy: -1152.813, kl: 274.678, loss: 1180.346\n",
      "  4%|▎         | 35/1000 [01:17<33:06,  2.06s/it]INFO:root:global_step: 35, logpy: -1146.535, kl: 273.810, loss: 1174.468\n",
      "  4%|▎         | 36/1000 [01:19<33:03,  2.06s/it]INFO:root:global_step: 36, logpy: -1138.582, kl: 272.847, loss: 1166.893\n",
      "  4%|▎         | 37/1000 [01:21<33:24,  2.08s/it]INFO:root:global_step: 37, logpy: -1129.632, kl: 272.180, loss: 1158.443\n",
      "  4%|▍         | 38/1000 [01:23<33:26,  2.09s/it]INFO:root:global_step: 38, logpy: -1120.112, kl: 271.374, loss: 1149.383\n",
      "  4%|▍         | 39/1000 [01:25<33:17,  2.08s/it]INFO:root:global_step: 39, logpy: -1110.682, kl: 270.974, loss: 1140.585\n",
      "  4%|▍         | 40/1000 [01:27<33:10,  2.07s/it]INFO:root:global_step: 40, logpy: -1101.996, kl: 270.979, loss: 1132.714\n",
      "  4%|▍         | 41/1000 [01:29<33:19,  2.08s/it]INFO:root:global_step: 41, logpy: -1093.670, kl: 270.680, loss: 1125.093\n",
      "  4%|▍         | 42/1000 [01:32<33:51,  2.12s/it]INFO:root:global_step: 42, logpy: -1084.775, kl: 270.349, loss: 1116.905\n",
      "  4%|▍         | 43/1000 [01:34<33:31,  2.10s/it]INFO:root:global_step: 43, logpy: -1074.994, kl: 269.787, loss: 1107.745\n",
      "  4%|▍         | 44/1000 [01:36<34:02,  2.14s/it]INFO:root:global_step: 44, logpy: -1064.948, kl: 268.908, loss: 1098.190\n",
      "  4%|▍         | 45/1000 [01:38<34:20,  2.16s/it]INFO:root:global_step: 45, logpy: -1056.331, kl: 268.111, loss: 1090.111\n",
      "  5%|▍         | 46/1000 [01:40<33:58,  2.14s/it]INFO:root:global_step: 46, logpy: -1047.560, kl: 267.028, loss: 1081.753\n",
      "  5%|▍         | 47/1000 [01:42<34:22,  2.16s/it]INFO:root:global_step: 47, logpy: -1037.381, kl: 266.388, loss: 1072.207\n",
      "  5%|▍         | 48/1000 [01:45<34:24,  2.17s/it]INFO:root:global_step: 48, logpy: -1027.471, kl: 266.247, loss: 1063.185\n",
      "  5%|▍         | 49/1000 [01:47<34:13,  2.16s/it]INFO:root:global_step: 49, logpy: -1018.280, kl: 266.106, loss: 1054.898\n",
      "  5%|▌         | 50/1000 [01:49<34:14,  2.16s/it]INFO:root:Saved figure at: ./sim/global_step_50.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 50, logpy: -1008.660, kl: 265.655, loss: 1046.039\n",
      "  5%|▌         | 51/1000 [01:52<40:28,  2.56s/it]INFO:root:global_step: 51, logpy: -999.247, kl: 265.396, loss: 1037.499\n",
      "  5%|▌         | 52/1000 [01:55<38:59,  2.47s/it]INFO:root:global_step: 52, logpy: -989.605, kl: 264.780, loss: 1028.554\n",
      "  5%|▌         | 53/1000 [01:57<38:21,  2.43s/it]INFO:root:global_step: 53, logpy: -980.026, kl: 264.050, loss: 1019.621\n",
      "  5%|▌         | 54/1000 [01:59<37:24,  2.37s/it]INFO:root:global_step: 54, logpy: -971.050, kl: 263.234, loss: 1011.252\n",
      "  6%|▌         | 55/1000 [02:02<37:36,  2.39s/it]INFO:root:global_step: 55, logpy: -961.883, kl: 262.535, loss: 1002.767\n",
      "  6%|▌         | 56/1000 [02:04<37:18,  2.37s/it]INFO:root:global_step: 56, logpy: -952.504, kl: 261.707, loss: 994.003\n",
      "  6%|▌         | 57/1000 [02:06<36:47,  2.34s/it]INFO:root:global_step: 57, logpy: -943.303, kl: 260.985, loss: 985.486\n",
      "  6%|▌         | 58/1000 [02:08<35:53,  2.29s/it]INFO:root:global_step: 58, logpy: -934.060, kl: 260.546, loss: 977.102\n",
      "  6%|▌         | 59/1000 [02:11<35:18,  2.25s/it]INFO:root:global_step: 59, logpy: -925.184, kl: 259.790, loss: 968.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 60/1000 [02:13<35:42,  2.28s/it]INFO:root:global_step: 60, logpy: -916.223, kl: 259.139, loss: 960.695\n",
      "  6%|▌         | 61/1000 [02:15<35:27,  2.27s/it]INFO:root:global_step: 61, logpy: -907.116, kl: 258.172, loss: 952.150\n",
      "  6%|▌         | 62/1000 [02:17<35:13,  2.25s/it]INFO:root:global_step: 62, logpy: -898.340, kl: 257.467, loss: 944.107\n",
      "  6%|▋         | 63/1000 [02:20<34:46,  2.23s/it]INFO:root:global_step: 63, logpy: -889.664, kl: 256.756, loss: 936.166\n",
      "  6%|▋         | 64/1000 [02:22<34:29,  2.21s/it]INFO:root:global_step: 64, logpy: -880.907, kl: 255.963, loss: 928.097\n",
      "  6%|▋         | 65/1000 [02:24<34:44,  2.23s/it]INFO:root:global_step: 65, logpy: -872.211, kl: 254.986, loss: 919.973\n",
      "  7%|▋         | 66/1000 [02:26<35:00,  2.25s/it]INFO:root:global_step: 66, logpy: -863.597, kl: 254.303, loss: 912.133\n",
      "  7%|▋         | 67/1000 [02:29<34:33,  2.22s/it]INFO:root:global_step: 67, logpy: -855.162, kl: 253.494, loss: 904.392\n",
      "  7%|▋         | 68/1000 [02:31<34:20,  2.21s/it]INFO:root:global_step: 68, logpy: -846.776, kl: 252.980, loss: 896.907\n",
      "  7%|▋         | 69/1000 [02:33<34:33,  2.23s/it]INFO:root:global_step: 69, logpy: -838.324, kl: 252.023, loss: 889.055\n",
      "  7%|▋         | 70/1000 [02:35<34:39,  2.24s/it]INFO:root:global_step: 70, logpy: -830.015, kl: 251.128, loss: 881.393\n",
      "  7%|▋         | 71/1000 [02:38<34:51,  2.25s/it]INFO:root:global_step: 71, logpy: -821.809, kl: 250.370, loss: 873.935\n",
      "  7%|▋         | 72/1000 [02:40<34:50,  2.25s/it]INFO:root:global_step: 72, logpy: -813.717, kl: 249.690, loss: 866.653\n",
      "  7%|▋         | 73/1000 [02:42<34:22,  2.23s/it]INFO:root:global_step: 73, logpy: -805.634, kl: 248.998, loss: 859.377\n",
      "  7%|▋         | 74/1000 [02:44<33:55,  2.20s/it]INFO:root:global_step: 74, logpy: -797.581, kl: 248.188, loss: 852.047\n",
      "  8%|▊         | 75/1000 [02:46<34:18,  2.23s/it]INFO:root:global_step: 75, logpy: -789.737, kl: 247.363, loss: 844.917\n",
      "  8%|▊         | 76/1000 [02:49<34:16,  2.23s/it]INFO:root:global_step: 76, logpy: -781.850, kl: 246.426, loss: 837.661\n",
      "  8%|▊         | 77/1000 [02:51<34:27,  2.24s/it]INFO:root:global_step: 77, logpy: -774.081, kl: 245.534, loss: 830.560\n",
      "  8%|▊         | 78/1000 [02:53<34:04,  2.22s/it]INFO:root:global_step: 78, logpy: -766.340, kl: 244.577, loss: 823.438\n",
      "  8%|▊         | 79/1000 [02:55<33:55,  2.21s/it]INFO:root:global_step: 79, logpy: -758.717, kl: 243.603, loss: 816.422\n",
      "  8%|▊         | 80/1000 [02:57<34:12,  2.23s/it]INFO:root:global_step: 80, logpy: -751.123, kl: 242.744, loss: 809.528\n",
      "  8%|▊         | 81/1000 [03:00<34:33,  2.26s/it]INFO:root:global_step: 81, logpy: -743.615, kl: 241.852, loss: 802.696\n",
      "  8%|▊         | 82/1000 [03:02<34:26,  2.25s/it]INFO:root:global_step: 82, logpy: -736.210, kl: 240.836, loss: 795.864\n",
      "  8%|▊         | 83/1000 [03:04<34:37,  2.27s/it]INFO:root:global_step: 83, logpy: -728.867, kl: 239.830, loss: 789.102\n",
      "  8%|▊         | 84/1000 [03:07<34:22,  2.25s/it]INFO:root:global_step: 84, logpy: -721.545, kl: 238.948, loss: 782.466\n",
      "  8%|▊         | 85/1000 [03:09<33:50,  2.22s/it]INFO:root:global_step: 85, logpy: -714.333, kl: 238.086, loss: 775.959\n",
      "  9%|▊         | 86/1000 [03:11<33:41,  2.21s/it]INFO:root:global_step: 86, logpy: -707.200, kl: 237.061, loss: 769.389\n",
      "  9%|▊         | 87/1000 [03:13<33:45,  2.22s/it]INFO:root:global_step: 87, logpy: -700.099, kl: 236.210, loss: 763.004\n",
      "  9%|▉         | 88/1000 [03:15<33:19,  2.19s/it]INFO:root:global_step: 88, logpy: -693.079, kl: 235.305, loss: 756.651\n",
      "  9%|▉         | 89/1000 [03:17<33:18,  2.19s/it]INFO:root:global_step: 89, logpy: -686.137, kl: 234.381, loss: 750.359\n",
      "  9%|▉         | 90/1000 [03:20<33:28,  2.21s/it]INFO:root:global_step: 90, logpy: -679.252, kl: 233.408, loss: 744.080\n",
      "  9%|▉         | 91/1000 [03:22<33:13,  2.19s/it]INFO:root:global_step: 91, logpy: -672.410, kl: 232.489, loss: 737.891\n",
      "  9%|▉         | 92/1000 [03:24<33:22,  2.20s/it]INFO:root:global_step: 92, logpy: -665.636, kl: 231.460, loss: 731.667\n",
      "  9%|▉         | 93/1000 [03:26<33:31,  2.22s/it]INFO:root:global_step: 93, logpy: -658.942, kl: 230.616, loss: 725.695\n",
      "  9%|▉         | 94/1000 [03:29<33:26,  2.21s/it]INFO:root:global_step: 94, logpy: -652.296, kl: 229.682, loss: 719.686\n",
      " 10%|▉         | 95/1000 [03:31<33:17,  2.21s/it]INFO:root:global_step: 95, logpy: -645.738, kl: 228.562, loss: 713.584\n",
      " 10%|▉         | 96/1000 [03:33<33:26,  2.22s/it]INFO:root:global_step: 96, logpy: -639.225, kl: 227.543, loss: 707.622\n",
      " 10%|▉         | 97/1000 [03:35<33:13,  2.21s/it]INFO:root:global_step: 97, logpy: -632.786, kl: 226.592, loss: 701.796\n",
      " 10%|▉         | 98/1000 [03:37<33:05,  2.20s/it]INFO:root:global_step: 98, logpy: -626.402, kl: 225.317, loss: 695.703\n",
      " 10%|▉         | 99/1000 [03:40<32:58,  2.20s/it]INFO:root:global_step: 99, logpy: -620.078, kl: 224.318, loss: 689.939\n",
      " 10%|█         | 100/1000 [03:42<32:50,  2.19s/it]INFO:root:Saved figure at: ./sim/global_step_100.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 100, logpy: -613.832, kl: 223.245, loss: 684.166\n",
      " 10%|█         | 101/1000 [03:45<38:24,  2.56s/it]INFO:root:global_step: 101, logpy: -607.640, kl: 222.225, loss: 678.482\n",
      " 10%|█         | 102/1000 [03:47<37:00,  2.47s/it]INFO:root:global_step: 102, logpy: -601.510, kl: 221.162, loss: 672.803\n",
      " 10%|█         | 103/1000 [03:50<35:36,  2.38s/it]INFO:root:global_step: 103, logpy: -595.438, kl: 220.022, loss: 667.090\n",
      " 10%|█         | 104/1000 [03:52<35:10,  2.35s/it]INFO:root:global_step: 104, logpy: -589.449, kl: 219.073, loss: 661.636\n",
      " 10%|█         | 105/1000 [03:54<34:38,  2.32s/it]INFO:root:global_step: 105, logpy: -583.501, kl: 217.992, loss: 656.075\n",
      " 11%|█         | 106/1000 [03:56<34:15,  2.30s/it]INFO:root:global_step: 106, logpy: -577.616, kl: 216.831, loss: 650.484\n",
      " 11%|█         | 107/1000 [03:59<34:17,  2.30s/it]INFO:root:global_step: 107, logpy: -571.783, kl: 215.710, loss: 644.969\n",
      " 11%|█         | 108/1000 [04:01<33:56,  2.28s/it]INFO:root:global_step: 108, logpy: -566.010, kl: 214.523, loss: 639.434\n",
      " 11%|█         | 109/1000 [04:03<33:36,  2.26s/it]INFO:root:global_step: 109, logpy: -560.292, kl: 213.270, loss: 633.874\n",
      " 11%|█         | 110/1000 [04:05<33:34,  2.26s/it]INFO:root:global_step: 110, logpy: -554.651, kl: 212.168, loss: 628.528\n",
      " 11%|█         | 111/1000 [04:08<33:20,  2.25s/it]INFO:root:global_step: 111, logpy: -549.051, kl: 210.932, loss: 623.075\n",
      " 11%|█         | 112/1000 [04:10<33:24,  2.26s/it]INFO:root:global_step: 112, logpy: -543.515, kl: 209.809, loss: 617.786\n",
      " 11%|█▏        | 113/1000 [04:12<33:05,  2.24s/it]INFO:root:global_step: 113, logpy: -538.028, kl: 208.643, loss: 612.488\n",
      " 11%|█▏        | 114/1000 [04:14<32:42,  2.21s/it]INFO:root:global_step: 114, logpy: -532.609, kl: 207.603, loss: 607.371\n",
      " 12%|█▏        | 115/1000 [04:16<32:39,  2.21s/it]INFO:root:global_step: 115, logpy: -527.228, kl: 206.421, loss: 602.137\n",
      " 12%|█▏        | 116/1000 [04:19<32:39,  2.22s/it]INFO:root:global_step: 116, logpy: -521.908, kl: 205.276, loss: 596.985\n",
      " 12%|█▏        | 117/1000 [04:21<32:55,  2.24s/it]INFO:root:global_step: 117, logpy: -516.662, kl: 204.218, loss: 591.985\n",
      " 12%|█▏        | 118/1000 [04:23<32:24,  2.20s/it]INFO:root:global_step: 118, logpy: -511.456, kl: 203.071, loss: 586.920\n",
      " 12%|█▏        | 119/1000 [04:25<32:36,  2.22s/it]INFO:root:global_step: 119, logpy: -506.303, kl: 201.946, loss: 581.918\n",
      " 12%|█▏        | 120/1000 [04:28<32:32,  2.22s/it]INFO:root:global_step: 120, logpy: -501.213, kl: 200.825, loss: 576.971\n",
      " 12%|█▏        | 121/1000 [04:30<32:37,  2.23s/it]INFO:root:global_step: 121, logpy: -496.172, kl: 199.723, loss: 572.079\n",
      " 12%|█▏        | 122/1000 [04:32<32:20,  2.21s/it]INFO:root:global_step: 122, logpy: -491.183, kl: 198.522, loss: 567.126\n",
      " 12%|█▏        | 123/1000 [04:34<32:22,  2.21s/it]INFO:root:global_step: 123, logpy: -486.241, kl: 197.392, loss: 562.281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 124/1000 [04:36<32:15,  2.21s/it]INFO:root:global_step: 124, logpy: -481.359, kl: 196.295, loss: 557.515\n",
      " 12%|█▎        | 125/1000 [04:39<32:26,  2.22s/it]INFO:root:global_step: 125, logpy: -476.519, kl: 195.096, loss: 552.677\n",
      " 13%|█▎        | 126/1000 [04:41<32:36,  2.24s/it]INFO:root:global_step: 126, logpy: -471.724, kl: 193.899, loss: 547.874\n",
      " 13%|█▎        | 127/1000 [04:43<32:52,  2.26s/it]INFO:root:global_step: 127, logpy: -466.975, kl: 192.689, loss: 543.093\n",
      " 13%|█▎        | 128/1000 [04:46<32:52,  2.26s/it]INFO:root:global_step: 128, logpy: -462.274, kl: 191.517, loss: 538.386\n",
      " 13%|█▎        | 129/1000 [04:48<32:54,  2.27s/it]INFO:root:global_step: 129, logpy: -457.621, kl: 190.386, loss: 533.756\n",
      " 13%|█▎        | 130/1000 [04:50<32:49,  2.26s/it]INFO:root:global_step: 130, logpy: -453.040, kl: 189.279, loss: 529.210\n",
      " 13%|█▎        | 131/1000 [04:52<32:41,  2.26s/it]INFO:root:global_step: 131, logpy: -448.513, kl: 188.281, loss: 524.817\n",
      " 13%|█▎        | 132/1000 [04:55<33:16,  2.30s/it]INFO:root:global_step: 132, logpy: -444.015, kl: 187.143, loss: 520.299\n",
      " 13%|█▎        | 133/1000 [04:57<33:01,  2.29s/it]INFO:root:global_step: 133, logpy: -439.579, kl: 185.958, loss: 515.787\n",
      " 13%|█▎        | 134/1000 [04:59<32:58,  2.29s/it]INFO:root:global_step: 134, logpy: -435.192, kl: 184.876, loss: 511.417\n",
      " 14%|█▎        | 135/1000 [05:01<32:47,  2.27s/it]INFO:root:global_step: 135, logpy: -430.825, kl: 183.723, loss: 506.982\n",
      " 14%|█▎        | 136/1000 [05:04<32:49,  2.28s/it]INFO:root:global_step: 136, logpy: -426.527, kl: 182.608, loss: 502.645\n",
      " 14%|█▎        | 137/1000 [05:06<32:34,  2.26s/it]INFO:root:global_step: 137, logpy: -422.263, kl: 181.496, loss: 498.335\n",
      " 14%|█▍        | 138/1000 [05:08<32:35,  2.27s/it]INFO:root:global_step: 138, logpy: -418.024, kl: 180.364, loss: 494.017\n",
      " 14%|█▍        | 139/1000 [05:11<32:39,  2.28s/it]INFO:root:global_step: 139, logpy: -413.846, kl: 179.306, loss: 489.825\n",
      " 14%|█▍        | 140/1000 [05:13<32:39,  2.28s/it]INFO:root:global_step: 140, logpy: -409.725, kl: 178.277, loss: 485.708\n",
      " 14%|█▍        | 141/1000 [05:15<32:28,  2.27s/it]INFO:root:global_step: 141, logpy: -405.646, kl: 177.273, loss: 481.649\n",
      " 14%|█▍        | 142/1000 [05:17<32:32,  2.28s/it]INFO:root:global_step: 142, logpy: -401.573, kl: 176.121, loss: 477.436\n",
      " 14%|█▍        | 143/1000 [05:20<32:35,  2.28s/it]INFO:root:global_step: 143, logpy: -397.565, kl: 175.093, loss: 473.403\n",
      " 14%|█▍        | 144/1000 [05:22<32:47,  2.30s/it]INFO:root:global_step: 144, logpy: -393.592, kl: 173.975, loss: 469.304\n",
      " 14%|█▍        | 145/1000 [05:24<32:20,  2.27s/it]INFO:root:global_step: 145, logpy: -389.682, kl: 172.931, loss: 465.334\n",
      " 15%|█▍        | 146/1000 [05:26<32:12,  2.26s/it]INFO:root:global_step: 146, logpy: -385.794, kl: 171.786, loss: 461.272\n",
      " 15%|█▍        | 147/1000 [05:29<32:14,  2.27s/it]INFO:root:global_step: 147, logpy: -381.944, kl: 170.675, loss: 457.275\n",
      " 15%|█▍        | 148/1000 [05:31<32:17,  2.27s/it]INFO:root:global_step: 148, logpy: -378.142, kl: 169.615, loss: 453.367\n",
      " 15%|█▍        | 149/1000 [05:33<32:12,  2.27s/it]INFO:root:global_step: 149, logpy: -374.374, kl: 168.529, loss: 449.456\n",
      " 15%|█▌        | 150/1000 [05:36<32:24,  2.29s/it]INFO:root:Saved figure at: ./sim/global_step_150.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 150, logpy: -370.636, kl: 167.453, loss: 445.576\n",
      " 15%|█▌        | 151/1000 [05:39<37:36,  2.66s/it]INFO:root:global_step: 151, logpy: -366.938, kl: 166.386, loss: 441.737\n",
      " 15%|█▌        | 152/1000 [05:41<35:50,  2.54s/it]INFO:root:global_step: 152, logpy: -363.296, kl: 165.395, loss: 438.019\n",
      " 15%|█▌        | 153/1000 [05:44<34:43,  2.46s/it]INFO:root:global_step: 153, logpy: -359.682, kl: 164.348, loss: 434.265\n",
      " 15%|█▌        | 154/1000 [05:46<34:05,  2.42s/it]INFO:root:global_step: 154, logpy: -356.106, kl: 163.272, loss: 430.511\n",
      " 16%|█▌        | 155/1000 [05:48<34:08,  2.42s/it]INFO:root:global_step: 155, logpy: -352.562, kl: 162.229, loss: 426.813\n",
      " 16%|█▌        | 156/1000 [05:51<33:12,  2.36s/it]INFO:root:global_step: 156, logpy: -349.057, kl: 161.214, loss: 423.172\n",
      " 16%|█▌        | 157/1000 [05:53<32:57,  2.35s/it]INFO:root:global_step: 157, logpy: -345.590, kl: 160.214, loss: 419.576\n",
      " 16%|█▌        | 158/1000 [05:55<32:48,  2.34s/it]INFO:root:global_step: 158, logpy: -342.170, kl: 159.217, loss: 416.022\n",
      " 16%|█▌        | 159/1000 [05:58<32:47,  2.34s/it]INFO:root:global_step: 159, logpy: -338.782, kl: 158.229, loss: 412.500\n",
      " 16%|█▌        | 160/1000 [06:00<32:26,  2.32s/it]INFO:root:global_step: 160, logpy: -335.435, kl: 157.293, loss: 409.062\n",
      " 16%|█▌        | 161/1000 [06:02<32:22,  2.32s/it]INFO:root:global_step: 161, logpy: -332.120, kl: 156.346, loss: 405.636\n",
      " 16%|█▌        | 162/1000 [06:05<32:54,  2.36s/it]INFO:root:global_step: 162, logpy: -328.808, kl: 155.284, loss: 402.090\n",
      " 16%|█▋        | 163/1000 [06:07<34:03,  2.44s/it]INFO:root:global_step: 163, logpy: -325.539, kl: 154.292, loss: 398.650\n",
      " 16%|█▋        | 164/1000 [06:10<34:24,  2.47s/it]INFO:root:global_step: 164, logpy: -322.323, kl: 153.379, loss: 395.332\n",
      " 16%|█▋        | 165/1000 [06:12<34:05,  2.45s/it]INFO:root:global_step: 165, logpy: -319.117, kl: 152.376, loss: 391.927\n",
      " 17%|█▋        | 166/1000 [06:15<33:26,  2.41s/it]INFO:root:global_step: 166, logpy: -315.955, kl: 151.398, loss: 388.583\n",
      " 17%|█▋        | 167/1000 [06:17<33:20,  2.40s/it]INFO:root:global_step: 167, logpy: -312.834, kl: 150.464, loss: 385.315\n",
      " 17%|█▋        | 168/1000 [06:19<32:49,  2.37s/it]INFO:root:global_step: 168, logpy: -309.730, kl: 149.547, loss: 382.075\n",
      " 17%|█▋        | 169/1000 [06:21<32:24,  2.34s/it]INFO:root:global_step: 169, logpy: -306.651, kl: 148.562, loss: 378.781\n",
      " 17%|█▋        | 170/1000 [06:24<32:03,  2.32s/it]INFO:root:global_step: 170, logpy: -303.614, kl: 147.633, loss: 375.581\n",
      " 17%|█▋        | 171/1000 [06:26<31:41,  2.29s/it]INFO:root:global_step: 171, logpy: -300.605, kl: 146.736, loss: 372.430\n",
      " 17%|█▋        | 172/1000 [06:28<31:38,  2.29s/it]INFO:root:global_step: 172, logpy: -297.630, kl: 145.819, loss: 369.288\n",
      " 17%|█▋        | 173/1000 [06:31<31:34,  2.29s/it]INFO:root:global_step: 173, logpy: -294.671, kl: 144.865, loss: 366.117\n",
      " 17%|█▋        | 174/1000 [06:33<31:36,  2.30s/it]INFO:root:global_step: 174, logpy: -291.739, kl: 143.938, loss: 362.991\n",
      " 18%|█▊        | 175/1000 [06:35<31:32,  2.29s/it]INFO:root:global_step: 175, logpy: -288.843, kl: 142.998, loss: 359.882\n",
      " 18%|█▊        | 176/1000 [06:38<31:42,  2.31s/it]INFO:root:global_step: 176, logpy: -285.998, kl: 142.121, loss: 356.880\n",
      " 18%|█▊        | 177/1000 [06:40<31:32,  2.30s/it]INFO:root:global_step: 177, logpy: -283.158, kl: 141.209, loss: 353.841\n",
      " 18%|█▊        | 178/1000 [06:42<31:24,  2.29s/it]INFO:root:global_step: 178, logpy: -280.350, kl: 140.282, loss: 350.811\n",
      " 18%|█▊        | 179/1000 [06:44<31:18,  2.29s/it]INFO:root:global_step: 179, logpy: -277.557, kl: 139.365, loss: 347.799\n",
      " 18%|█▊        | 180/1000 [06:47<31:45,  2.32s/it]INFO:root:global_step: 180, logpy: -274.820, kl: 138.468, loss: 344.857\n",
      " 18%|█▊        | 181/1000 [06:49<31:24,  2.30s/it]INFO:root:global_step: 181, logpy: -272.087, kl: 137.564, loss: 341.904\n",
      " 18%|█▊        | 182/1000 [06:51<31:28,  2.31s/it]INFO:root:global_step: 182, logpy: -269.397, kl: 136.716, loss: 339.043\n",
      " 18%|█▊        | 183/1000 [06:54<31:31,  2.31s/it]INFO:root:global_step: 183, logpy: -266.736, kl: 135.873, loss: 336.210\n",
      " 18%|█▊        | 184/1000 [06:56<31:36,  2.32s/it]INFO:root:global_step: 184, logpy: -264.072, kl: 134.992, loss: 333.329\n",
      " 18%|█▊        | 185/1000 [06:58<31:33,  2.32s/it]INFO:root:global_step: 185, logpy: -261.456, kl: 134.157, loss: 330.536\n",
      " 19%|█▊        | 186/1000 [07:01<31:14,  2.30s/it]INFO:root:global_step: 186, logpy: -258.863, kl: 133.338, loss: 327.774\n",
      " 19%|█▊        | 187/1000 [07:03<32:06,  2.37s/it]INFO:root:global_step: 187, logpy: -256.271, kl: 132.433, loss: 324.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 188/1000 [07:06<32:47,  2.42s/it]INFO:root:global_step: 188, logpy: -253.716, kl: 131.600, loss: 322.172\n",
      " 19%|█▉        | 189/1000 [07:09<35:19,  2.61s/it]INFO:root:global_step: 189, logpy: -251.203, kl: 130.766, loss: 319.456\n",
      " 19%|█▉        | 190/1000 [07:12<36:49,  2.73s/it]INFO:root:global_step: 190, logpy: -248.735, kl: 129.963, loss: 316.810\n",
      " 19%|█▉        | 191/1000 [07:15<38:46,  2.88s/it]INFO:root:global_step: 191, logpy: -246.317, kl: 129.176, loss: 314.223\n",
      " 19%|█▉        | 192/1000 [07:18<39:16,  2.92s/it]INFO:root:global_step: 192, logpy: -243.908, kl: 128.364, loss: 311.616\n",
      " 19%|█▉        | 193/1000 [07:21<39:05,  2.91s/it]INFO:root:global_step: 193, logpy: -241.491, kl: 127.520, loss: 308.961\n",
      " 19%|█▉        | 194/1000 [07:24<38:40,  2.88s/it]INFO:root:global_step: 194, logpy: -239.093, kl: 126.740, loss: 306.384\n",
      " 20%|█▉        | 195/1000 [07:26<37:04,  2.76s/it]INFO:root:global_step: 195, logpy: -236.708, kl: 125.982, loss: 303.835\n",
      " 20%|█▉        | 196/1000 [07:28<35:11,  2.63s/it]INFO:root:global_step: 196, logpy: -234.364, kl: 125.223, loss: 301.321\n",
      " 20%|█▉        | 197/1000 [07:31<33:54,  2.53s/it]INFO:root:global_step: 197, logpy: -232.073, kl: 124.458, loss: 298.847\n",
      " 20%|█▉        | 198/1000 [07:33<33:08,  2.48s/it]INFO:root:global_step: 198, logpy: -229.816, kl: 123.710, loss: 296.419\n",
      " 20%|█▉        | 199/1000 [07:36<33:06,  2.48s/it]INFO:root:global_step: 199, logpy: -227.552, kl: 122.919, loss: 293.935\n",
      " 20%|██        | 200/1000 [07:38<33:32,  2.52s/it]INFO:root:Saved figure at: ./sim/global_step_200.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 200, logpy: -225.305, kl: 122.157, loss: 291.491\n",
      " 20%|██        | 201/1000 [07:42<38:35,  2.90s/it]INFO:root:global_step: 201, logpy: -223.075, kl: 121.404, loss: 289.068\n",
      " 20%|██        | 202/1000 [07:45<37:38,  2.83s/it]INFO:root:global_step: 202, logpy: -220.855, kl: 120.660, loss: 286.658\n",
      " 20%|██        | 203/1000 [07:48<37:42,  2.84s/it]INFO:root:global_step: 203, logpy: -218.699, kl: 120.026, loss: 284.417\n",
      " 20%|██        | 204/1000 [07:50<37:15,  2.81s/it]INFO:root:global_step: 204, logpy: -216.536, kl: 119.305, loss: 282.076\n",
      " 20%|██        | 205/1000 [07:53<36:36,  2.76s/it]INFO:root:global_step: 205, logpy: -214.418, kl: 118.584, loss: 279.774\n",
      " 21%|██        | 206/1000 [07:56<36:40,  2.77s/it]INFO:root:global_step: 206, logpy: -212.360, kl: 117.855, loss: 277.520\n",
      " 21%|██        | 207/1000 [07:58<35:51,  2.71s/it]INFO:root:global_step: 207, logpy: -210.285, kl: 117.125, loss: 275.242\n",
      " 21%|██        | 208/1000 [08:01<34:43,  2.63s/it]INFO:root:global_step: 208, logpy: -208.207, kl: 116.432, loss: 272.993\n",
      " 21%|██        | 209/1000 [08:03<34:06,  2.59s/it]INFO:root:global_step: 209, logpy: -206.136, kl: 115.719, loss: 270.725\n",
      " 21%|██        | 210/1000 [08:06<34:46,  2.64s/it]INFO:root:global_step: 210, logpy: -204.082, kl: 115.003, loss: 268.466\n",
      " 21%|██        | 211/1000 [08:09<34:28,  2.62s/it]INFO:root:global_step: 211, logpy: -202.099, kl: 114.341, loss: 266.327\n",
      " 21%|██        | 212/1000 [08:11<34:21,  2.62s/it]INFO:root:global_step: 212, logpy: -200.143, kl: 113.652, loss: 264.184\n",
      " 21%|██▏       | 213/1000 [08:14<33:51,  2.58s/it]INFO:root:global_step: 213, logpy: -198.232, kl: 112.993, loss: 262.109\n",
      " 21%|██▏       | 214/1000 [08:16<33:33,  2.56s/it]INFO:root:global_step: 214, logpy: -196.302, kl: 112.350, loss: 260.027\n",
      " 22%|██▏       | 215/1000 [08:19<33:25,  2.56s/it]INFO:root:global_step: 215, logpy: -194.346, kl: 111.708, loss: 257.916\n",
      " 22%|██▏       | 216/1000 [08:21<32:30,  2.49s/it]INFO:root:global_step: 216, logpy: -192.414, kl: 111.036, loss: 255.793\n",
      " 22%|██▏       | 217/1000 [08:23<32:18,  2.48s/it]INFO:root:global_step: 217, logpy: -190.497, kl: 110.353, loss: 253.670\n",
      " 22%|██▏       | 218/1000 [08:26<33:29,  2.57s/it]INFO:root:global_step: 218, logpy: -188.634, kl: 109.714, loss: 251.640\n",
      " 22%|██▏       | 219/1000 [08:29<33:10,  2.55s/it]INFO:root:global_step: 219, logpy: -186.797, kl: 109.069, loss: 249.625\n",
      " 22%|██▏       | 220/1000 [08:31<32:55,  2.53s/it]INFO:root:global_step: 220, logpy: -184.990, kl: 108.406, loss: 247.618\n",
      " 22%|██▏       | 221/1000 [08:34<32:43,  2.52s/it]INFO:root:global_step: 221, logpy: -183.185, kl: 107.775, loss: 245.639\n",
      " 22%|██▏       | 222/1000 [08:36<32:57,  2.54s/it]INFO:root:global_step: 222, logpy: -181.389, kl: 107.157, loss: 243.679\n",
      " 22%|██▏       | 223/1000 [08:39<33:00,  2.55s/it]INFO:root:global_step: 223, logpy: -179.595, kl: 106.561, loss: 241.736\n",
      " 22%|██▏       | 224/1000 [08:41<33:03,  2.56s/it]INFO:root:global_step: 224, logpy: -177.796, kl: 105.912, loss: 239.733\n",
      " 22%|██▎       | 225/1000 [08:44<32:34,  2.52s/it]INFO:root:global_step: 225, logpy: -176.024, kl: 105.288, loss: 237.777\n",
      " 23%|██▎       | 226/1000 [08:46<32:40,  2.53s/it]INFO:root:global_step: 226, logpy: -174.274, kl: 104.691, loss: 235.866\n",
      " 23%|██▎       | 227/1000 [08:49<32:51,  2.55s/it]INFO:root:global_step: 227, logpy: -172.539, kl: 104.077, loss: 233.948\n",
      " 23%|██▎       | 228/1000 [08:52<32:47,  2.55s/it]INFO:root:global_step: 228, logpy: -170.810, kl: 103.454, loss: 232.022\n",
      " 23%|██▎       | 229/1000 [08:54<32:20,  2.52s/it]INFO:root:global_step: 229, logpy: -169.092, kl: 102.858, loss: 230.131\n",
      " 23%|██▎       | 230/1000 [08:56<31:58,  2.49s/it]INFO:root:global_step: 230, logpy: -167.403, kl: 102.265, loss: 228.267\n",
      " 23%|██▎       | 231/1000 [08:59<31:50,  2.48s/it]INFO:root:global_step: 231, logpy: -165.733, kl: 101.681, loss: 226.427\n",
      " 23%|██▎       | 232/1000 [09:01<31:29,  2.46s/it]INFO:root:global_step: 232, logpy: -164.063, kl: 101.109, loss: 224.595\n",
      " 23%|██▎       | 233/1000 [09:04<31:19,  2.45s/it]INFO:root:global_step: 233, logpy: -162.428, kl: 100.538, loss: 222.794\n",
      " 23%|██▎       | 234/1000 [09:07<32:32,  2.55s/it]INFO:root:global_step: 234, logpy: -160.800, kl: 99.946, loss: 220.975\n",
      " 24%|██▎       | 235/1000 [09:09<32:21,  2.54s/it]INFO:root:global_step: 235, logpy: -159.176, kl: 99.354, loss: 219.157\n",
      " 24%|██▎       | 236/1000 [09:12<32:04,  2.52s/it]INFO:root:global_step: 236, logpy: -157.568, kl: 98.777, loss: 217.366\n",
      " 24%|██▎       | 237/1000 [09:14<31:34,  2.48s/it]INFO:root:global_step: 237, logpy: -155.975, kl: 98.196, loss: 215.583\n",
      " 24%|██▍       | 238/1000 [09:16<31:14,  2.46s/it]INFO:root:global_step: 238, logpy: -154.408, kl: 97.642, loss: 213.846\n",
      " 24%|██▍       | 239/1000 [09:19<31:38,  2.50s/it]INFO:root:global_step: 239, logpy: -152.852, kl: 97.095, loss: 212.126\n",
      " 24%|██▍       | 240/1000 [09:21<31:04,  2.45s/it]INFO:root:global_step: 240, logpy: -151.309, kl: 96.512, loss: 210.378\n",
      " 24%|██▍       | 241/1000 [09:24<30:52,  2.44s/it]INFO:root:global_step: 241, logpy: -149.820, kl: 95.969, loss: 208.721\n",
      " 24%|██▍       | 242/1000 [09:26<30:23,  2.41s/it]INFO:root:global_step: 242, logpy: -148.396, kl: 95.439, loss: 207.138\n",
      " 24%|██▍       | 243/1000 [09:28<30:07,  2.39s/it]INFO:root:global_step: 243, logpy: -147.053, kl: 94.901, loss: 205.623\n",
      " 24%|██▍       | 244/1000 [09:31<29:57,  2.38s/it]INFO:root:global_step: 244, logpy: -145.630, kl: 94.354, loss: 204.017\n",
      " 24%|██▍       | 245/1000 [09:33<29:48,  2.37s/it]INFO:root:global_step: 245, logpy: -144.178, kl: 93.844, loss: 202.415\n",
      " 25%|██▍       | 246/1000 [09:36<30:11,  2.40s/it]INFO:root:global_step: 246, logpy: -142.751, kl: 93.343, loss: 200.842\n",
      " 25%|██▍       | 247/1000 [09:38<30:18,  2.42s/it]INFO:root:global_step: 247, logpy: -141.385, kl: 92.852, loss: 199.337\n",
      " 25%|██▍       | 248/1000 [09:40<30:19,  2.42s/it]INFO:root:global_step: 248, logpy: -140.166, kl: 92.322, loss: 197.937\n",
      " 25%|██▍       | 249/1000 [09:43<30:13,  2.42s/it]INFO:root:global_step: 249, logpy: -138.809, kl: 91.792, loss: 196.397\n",
      " 25%|██▌       | 250/1000 [09:45<29:53,  2.39s/it]INFO:root:Saved figure at: ./sim/global_step_250.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:global_step: 250, logpy: -137.434, kl: 91.325, loss: 194.896\n",
      " 25%|██▌       | 251/1000 [09:49<34:55,  2.80s/it]INFO:root:global_step: 251, logpy: -136.302, kl: 90.861, loss: 193.639\n",
      " 25%|██▌       | 252/1000 [09:51<33:36,  2.70s/it]INFO:root:global_step: 252, logpy: -135.084, kl: 90.403, loss: 192.299\n",
      " 25%|██▌       | 253/1000 [09:54<33:33,  2.70s/it]INFO:root:global_step: 253, logpy: -133.776, kl: 89.909, loss: 190.828\n",
      " 25%|██▌       | 254/1000 [09:57<33:18,  2.68s/it]INFO:root:global_step: 254, logpy: -132.446, kl: 89.441, loss: 189.358\n",
      " 26%|██▌       | 255/1000 [09:59<32:09,  2.59s/it]INFO:root:global_step: 255, logpy: -131.262, kl: 88.966, loss: 188.025\n",
      " 26%|██▌       | 256/1000 [10:02<31:32,  2.54s/it]INFO:root:global_step: 256, logpy: -130.143, kl: 88.498, loss: 186.761\n",
      " 26%|██▌       | 257/1000 [10:04<30:58,  2.50s/it]INFO:root:global_step: 257, logpy: -128.910, kl: 88.043, loss: 185.392\n",
      " 26%|██▌       | 258/1000 [10:06<30:39,  2.48s/it]INFO:root:global_step: 258, logpy: -127.641, kl: 87.588, loss: 183.983\n",
      " 26%|██▌       | 259/1000 [10:09<30:11,  2.44s/it]INFO:root:global_step: 259, logpy: -126.585, kl: 87.126, loss: 182.777\n",
      " 26%|██▌       | 260/1000 [10:11<29:41,  2.41s/it]INFO:root:global_step: 260, logpy: -125.400, kl: 86.659, loss: 181.434\n",
      " 26%|██▌       | 261/1000 [10:13<29:29,  2.39s/it]INFO:root:global_step: 261, logpy: -124.168, kl: 86.175, loss: 180.024\n",
      " 26%|██▌       | 262/1000 [10:16<29:30,  2.40s/it]INFO:root:global_step: 262, logpy: -122.941, kl: 85.719, loss: 178.645\n",
      " 26%|██▋       | 263/1000 [10:18<29:39,  2.41s/it]INFO:root:global_step: 263, logpy: -121.756, kl: 85.241, loss: 177.282\n",
      " 26%|██▋       | 264/1000 [10:21<29:28,  2.40s/it]INFO:root:global_step: 264, logpy: -120.644, kl: 84.770, loss: 175.997\n",
      " 26%|██▋       | 265/1000 [10:23<29:16,  2.39s/it]INFO:root:global_step: 265, logpy: -119.484, kl: 84.303, loss: 174.663\n",
      " 27%|██▋       | 266/1000 [10:25<29:08,  2.38s/it]INFO:root:global_step: 266, logpy: -118.275, kl: 83.851, loss: 173.293\n",
      " 27%|██▋       | 267/1000 [10:28<29:17,  2.40s/it]INFO:root:global_step: 267, logpy: -117.099, kl: 83.437, loss: 171.993\n",
      " 27%|██▋       | 268/1000 [10:30<29:13,  2.40s/it]INFO:root:global_step: 268, logpy: -115.997, kl: 82.993, loss: 170.732\n",
      " 27%|██▋       | 269/1000 [10:33<29:18,  2.41s/it]INFO:root:global_step: 269, logpy: -115.053, kl: 82.558, loss: 169.635\n",
      " 27%|██▋       | 270/1000 [10:35<29:00,  2.38s/it]INFO:root:global_step: 270, logpy: -113.940, kl: 82.099, loss: 168.343\n",
      " 27%|██▋       | 271/1000 [10:37<29:04,  2.39s/it]INFO:root:global_step: 271, logpy: -112.806, kl: 81.705, loss: 167.091\n",
      " 27%|██▋       | 272/1000 [10:40<28:45,  2.37s/it]INFO:root:global_step: 272, logpy: -111.885, kl: 81.277, loss: 166.017\n",
      " 27%|██▋       | 273/1000 [10:42<28:35,  2.36s/it]INFO:root:global_step: 273, logpy: -110.824, kl: 80.858, loss: 164.808\n",
      " 27%|██▋       | 274/1000 [10:44<28:29,  2.35s/it]INFO:root:global_step: 274, logpy: -109.733, kl: 80.437, loss: 163.565\n",
      " 28%|██▊       | 275/1000 [10:47<28:31,  2.36s/it]INFO:root:global_step: 275, logpy: -108.649, kl: 80.042, loss: 162.352\n",
      " 28%|██▊       | 276/1000 [10:49<28:42,  2.38s/it]INFO:root:global_step: 276, logpy: -107.679, kl: 79.637, loss: 161.241\n",
      " 28%|██▊       | 277/1000 [10:51<28:22,  2.36s/it]INFO:root:global_step: 277, logpy: -106.802, kl: 79.232, loss: 160.220\n",
      " 28%|██▊       | 278/1000 [10:54<28:21,  2.36s/it]INFO:root:global_step: 278, logpy: -105.784, kl: 78.884, loss: 159.111\n",
      " 28%|██▊       | 279/1000 [10:56<28:27,  2.37s/it]INFO:root:global_step: 279, logpy: -104.708, kl: 78.478, loss: 157.885\n",
      " 28%|██▊       | 280/1000 [10:59<28:17,  2.36s/it]INFO:root:global_step: 280, logpy: -103.735, kl: 78.063, loss: 156.750\n",
      " 28%|██▊       | 281/1000 [11:01<28:21,  2.37s/it]INFO:root:global_step: 281, logpy: -102.892, kl: 77.725, loss: 155.819\n",
      " 28%|██▊       | 282/1000 [11:03<28:09,  2.35s/it]INFO:root:global_step: 282, logpy: -101.980, kl: 77.371, loss: 154.801\n",
      " 28%|██▊       | 283/1000 [11:06<27:58,  2.34s/it]INFO:root:global_step: 283, logpy: -100.974, kl: 77.000, loss: 153.670\n",
      " 28%|██▊       | 284/1000 [11:08<28:21,  2.38s/it]INFO:root:global_step: 284, logpy: -99.993, kl: 76.604, loss: 152.536\n",
      " 28%|██▊       | 285/1000 [11:10<28:17,  2.37s/it]INFO:root:global_step: 285, logpy: -99.254, kl: 76.219, loss: 151.652\n",
      " 29%|██▊       | 286/1000 [11:13<28:13,  2.37s/it]INFO:root:global_step: 286, logpy: -98.262, kl: 75.846, loss: 150.525\n",
      " 29%|██▊       | 287/1000 [11:15<28:03,  2.36s/it]INFO:root:global_step: 287, logpy: -97.281, kl: 75.454, loss: 149.388\n",
      " 29%|██▉       | 288/1000 [11:18<28:29,  2.40s/it]INFO:root:global_step: 288, logpy: -96.415, kl: 75.118, loss: 148.420\n",
      " 29%|██▉       | 289/1000 [11:20<28:16,  2.39s/it]INFO:root:global_step: 289, logpy: -95.564, kl: 74.768, loss: 147.450\n",
      " 29%|██▉       | 290/1000 [11:22<28:01,  2.37s/it]INFO:root:global_step: 290, logpy: -94.655, kl: 74.429, loss: 146.431\n",
      " 29%|██▉       | 291/1000 [11:25<27:45,  2.35s/it]INFO:root:global_step: 291, logpy: -93.691, kl: 74.049, loss: 145.313\n",
      " 29%|██▉       | 292/1000 [11:27<27:52,  2.36s/it]INFO:root:global_step: 292, logpy: -92.707, kl: 73.674, loss: 144.179\n",
      " 29%|██▉       | 293/1000 [11:29<28:01,  2.38s/it]INFO:root:global_step: 293, logpy: -91.787, kl: 73.314, loss: 143.121\n",
      " 29%|██▉       | 294/1000 [11:32<27:50,  2.37s/it]INFO:root:global_step: 294, logpy: -90.903, kl: 72.939, loss: 142.081\n",
      " 30%|██▉       | 295/1000 [11:34<27:44,  2.36s/it]INFO:root:global_step: 295, logpy: -90.131, kl: 72.603, loss: 141.190\n",
      " 30%|██▉       | 296/1000 [11:36<27:35,  2.35s/it]INFO:root:global_step: 296, logpy: -89.243, kl: 72.281, loss: 140.196\n",
      " 30%|██▉       | 297/1000 [11:39<27:32,  2.35s/it]INFO:root:global_step: 297, logpy: -88.331, kl: 71.957, loss: 139.174\n",
      " 30%|██▉       | 298/1000 [11:41<27:34,  2.36s/it]INFO:root:global_step: 298, logpy: -87.564, kl: 71.617, loss: 138.278\n",
      " 30%|██▉       | 299/1000 [11:44<27:40,  2.37s/it]INFO:root:global_step: 299, logpy: -86.802, kl: 71.289, loss: 137.397\n",
      " 30%|███       | 300/1000 [11:46<27:37,  2.37s/it]INFO:root:Saved figure at: ./sim/global_step_300.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 300, logpy: -85.950, kl: 70.924, loss: 136.387\n",
      " 30%|███       | 301/1000 [11:50<31:58,  2.74s/it]INFO:root:global_step: 301, logpy: -85.048, kl: 70.586, loss: 135.352\n",
      " 30%|███       | 302/1000 [11:52<31:03,  2.67s/it]INFO:root:global_step: 302, logpy: -84.332, kl: 70.274, loss: 134.527\n",
      " 30%|███       | 303/1000 [11:54<30:24,  2.62s/it]INFO:root:global_step: 303, logpy: -83.691, kl: 69.952, loss: 133.764\n",
      " 30%|███       | 304/1000 [11:57<29:45,  2.57s/it]INFO:root:global_step: 304, logpy: -82.871, kl: 69.613, loss: 132.804\n",
      " 30%|███       | 305/1000 [11:59<29:04,  2.51s/it]INFO:root:global_step: 305, logpy: -82.131, kl: 69.325, loss: 131.973\n",
      " 31%|███       | 306/1000 [12:02<28:47,  2.49s/it]INFO:root:global_step: 306, logpy: -81.654, kl: 69.014, loss: 131.380\n",
      " 31%|███       | 307/1000 [12:04<28:38,  2.48s/it]INFO:root:global_step: 307, logpy: -80.793, kl: 68.692, loss: 130.390\n",
      " 31%|███       | 308/1000 [12:07<28:24,  2.46s/it]INFO:root:global_step: 308, logpy: -80.171, kl: 68.379, loss: 129.646\n",
      " 31%|███       | 309/1000 [12:09<28:01,  2.43s/it]INFO:root:global_step: 309, logpy: -79.697, kl: 68.066, loss: 129.048\n",
      " 31%|███       | 310/1000 [12:12<28:28,  2.48s/it]INFO:root:global_step: 310, logpy: -78.849, kl: 67.744, loss: 128.065\n",
      " 31%|███       | 311/1000 [12:14<28:06,  2.45s/it]INFO:root:global_step: 311, logpy: -78.665, kl: 67.454, loss: 127.776\n",
      " 31%|███       | 312/1000 [12:16<28:02,  2.45s/it]INFO:root:global_step: 312, logpy: -77.953, kl: 67.158, loss: 126.951\n",
      " 31%|███▏      | 313/1000 [12:19<27:36,  2.41s/it]INFO:root:global_step: 313, logpy: -77.315, kl: 66.860, loss: 126.198\n",
      " 31%|███▏      | 314/1000 [12:21<27:42,  2.42s/it]INFO:root:global_step: 314, logpy: -76.848, kl: 66.574, loss: 125.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 315/1000 [12:24<27:33,  2.41s/it]INFO:root:global_step: 315, logpy: -76.063, kl: 66.280, loss: 124.723\n",
      " 32%|███▏      | 316/1000 [12:26<27:26,  2.41s/it]INFO:root:global_step: 316, logpy: -75.553, kl: 65.992, loss: 124.101\n",
      " 32%|███▏      | 317/1000 [12:28<27:11,  2.39s/it]INFO:root:global_step: 317, logpy: -74.953, kl: 65.724, loss: 123.408\n",
      " 32%|███▏      | 318/1000 [12:31<27:05,  2.38s/it]INFO:root:global_step: 318, logpy: -74.182, kl: 65.419, loss: 122.504\n",
      " 32%|███▏      | 319/1000 [12:33<27:06,  2.39s/it]INFO:root:global_step: 319, logpy: -73.410, kl: 65.166, loss: 121.649\n",
      " 32%|███▏      | 320/1000 [12:35<27:01,  2.38s/it]INFO:root:global_step: 320, logpy: -72.804, kl: 64.899, loss: 120.947\n",
      " 32%|███▏      | 321/1000 [12:38<27:15,  2.41s/it]INFO:root:global_step: 321, logpy: -72.195, kl: 64.636, loss: 120.242\n",
      " 32%|███▏      | 322/1000 [12:40<27:32,  2.44s/it]INFO:root:global_step: 322, logpy: -71.484, kl: 64.347, loss: 119.408\n",
      " 32%|███▏      | 323/1000 [12:43<27:35,  2.45s/it]INFO:root:global_step: 323, logpy: -70.793, kl: 64.080, loss: 118.614\n",
      " 32%|███▏      | 324/1000 [12:45<27:20,  2.43s/it]INFO:root:global_step: 324, logpy: -70.241, kl: 63.834, loss: 117.978\n",
      " 32%|███▎      | 325/1000 [12:48<27:06,  2.41s/it]INFO:root:global_step: 325, logpy: -69.548, kl: 63.575, loss: 117.188\n",
      " 33%|███▎      | 326/1000 [12:50<27:16,  2.43s/it]INFO:root:global_step: 326, logpy: -68.829, kl: 63.332, loss: 116.385\n",
      " 33%|███▎      | 327/1000 [12:52<27:01,  2.41s/it]INFO:root:global_step: 327, logpy: -68.137, kl: 63.082, loss: 115.600\n",
      " 33%|███▎      | 328/1000 [12:55<26:55,  2.40s/it]INFO:root:global_step: 328, logpy: -67.535, kl: 62.842, loss: 114.915\n",
      " 33%|███▎      | 329/1000 [12:57<26:44,  2.39s/it]INFO:root:global_step: 329, logpy: -66.949, kl: 62.603, loss: 114.245\n",
      " 33%|███▎      | 330/1000 [13:00<27:03,  2.42s/it]INFO:root:global_step: 330, logpy: -66.248, kl: 62.354, loss: 113.448\n",
      " 33%|███▎      | 331/1000 [13:02<26:54,  2.41s/it]INFO:root:global_step: 331, logpy: -65.540, kl: 62.079, loss: 112.616\n",
      " 33%|███▎      | 332/1000 [13:04<26:44,  2.40s/it]INFO:root:global_step: 332, logpy: -64.874, kl: 61.824, loss: 111.845\n",
      " 33%|███▎      | 333/1000 [13:07<26:51,  2.42s/it]INFO:root:global_step: 333, logpy: -64.224, kl: 61.572, loss: 111.092\n",
      " 33%|███▎      | 334/1000 [13:10<27:16,  2.46s/it]INFO:root:global_step: 334, logpy: -63.562, kl: 61.350, loss: 110.355\n",
      " 34%|███▎      | 335/1000 [13:12<27:14,  2.46s/it]INFO:root:global_step: 335, logpy: -62.879, kl: 61.131, loss: 109.599\n",
      " 34%|███▎      | 336/1000 [13:14<27:09,  2.45s/it]INFO:root:global_step: 336, logpy: -62.207, kl: 60.866, loss: 108.806\n",
      " 34%|███▎      | 337/1000 [13:17<27:00,  2.44s/it]INFO:root:global_step: 337, logpy: -61.622, kl: 60.618, loss: 108.115\n",
      " 34%|███▍      | 338/1000 [13:19<27:06,  2.46s/it]INFO:root:global_step: 338, logpy: -61.041, kl: 60.368, loss: 107.426\n",
      " 34%|███▍      | 339/1000 [13:22<26:42,  2.42s/it]INFO:root:global_step: 339, logpy: -60.433, kl: 60.153, loss: 106.741\n",
      " 34%|███▍      | 340/1000 [13:24<26:23,  2.40s/it]INFO:root:global_step: 340, logpy: -59.754, kl: 59.895, loss: 105.944\n",
      " 34%|███▍      | 341/1000 [13:26<26:12,  2.39s/it]INFO:root:global_step: 341, logpy: -59.124, kl: 59.664, loss: 105.219\n",
      " 34%|███▍      | 342/1000 [13:29<25:55,  2.36s/it]INFO:root:global_step: 342, logpy: -58.576, kl: 59.422, loss: 104.565\n",
      " 34%|███▍      | 343/1000 [13:31<26:09,  2.39s/it]INFO:root:global_step: 343, logpy: -57.997, kl: 59.161, loss: 103.860\n",
      " 34%|███▍      | 344/1000 [13:34<26:12,  2.40s/it]INFO:root:global_step: 344, logpy: -57.402, kl: 58.935, loss: 103.172\n",
      " 34%|███▍      | 345/1000 [13:36<26:16,  2.41s/it]INFO:root:global_step: 345, logpy: -56.771, kl: 58.679, loss: 102.417\n",
      " 35%|███▍      | 346/1000 [13:38<26:30,  2.43s/it]INFO:root:global_step: 346, logpy: -56.117, kl: 58.433, loss: 101.646\n",
      " 35%|███▍      | 347/1000 [13:41<26:24,  2.43s/it]INFO:root:global_step: 347, logpy: -55.485, kl: 58.224, loss: 100.934\n",
      " 35%|███▍      | 348/1000 [13:43<26:19,  2.42s/it]INFO:root:global_step: 348, logpy: -54.864, kl: 57.992, loss: 100.210\n",
      " 35%|███▍      | 349/1000 [13:46<26:08,  2.41s/it]INFO:root:global_step: 349, logpy: -54.262, kl: 57.757, loss: 99.499\n",
      " 35%|███▌      | 350/1000 [13:48<25:59,  2.40s/it]INFO:root:Saved figure at: ./sim/global_step_350.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 350, logpy: -53.680, kl: 57.534, loss: 98.820\n",
      " 35%|███▌      | 351/1000 [13:52<30:45,  2.84s/it]INFO:root:global_step: 351, logpy: -53.135, kl: 57.313, loss: 98.178\n",
      " 35%|███▌      | 352/1000 [13:54<29:31,  2.73s/it]INFO:root:global_step: 352, logpy: -52.564, kl: 57.110, loss: 97.526\n",
      " 35%|███▌      | 353/1000 [13:57<28:16,  2.62s/it]INFO:root:global_step: 353, logpy: -51.995, kl: 56.908, loss: 96.876\n",
      " 35%|███▌      | 354/1000 [13:59<27:15,  2.53s/it]INFO:root:global_step: 354, logpy: -51.419, kl: 56.701, loss: 96.214\n",
      " 36%|███▌      | 355/1000 [14:01<26:43,  2.49s/it]INFO:root:global_step: 355, logpy: -50.877, kl: 56.489, loss: 95.579\n",
      " 36%|███▌      | 356/1000 [14:04<26:07,  2.43s/it]INFO:root:global_step: 356, logpy: -50.300, kl: 56.276, loss: 94.906\n",
      " 36%|███▌      | 357/1000 [14:06<25:47,  2.41s/it]INFO:root:global_step: 357, logpy: -49.712, kl: 56.055, loss: 94.214\n",
      " 36%|███▌      | 358/1000 [14:09<26:10,  2.45s/it]INFO:root:global_step: 358, logpy: -49.140, kl: 55.852, loss: 93.555\n",
      " 36%|███▌      | 359/1000 [14:11<25:58,  2.43s/it]INFO:root:global_step: 359, logpy: -48.567, kl: 55.631, loss: 92.875\n",
      " 36%|███▌      | 360/1000 [14:13<25:37,  2.40s/it]INFO:root:global_step: 360, logpy: -48.006, kl: 55.424, loss: 92.220\n",
      " 36%|███▌      | 361/1000 [14:16<25:19,  2.38s/it]INFO:root:global_step: 361, logpy: -47.481, kl: 55.227, loss: 91.611\n",
      " 36%|███▌      | 362/1000 [14:18<25:43,  2.42s/it]INFO:root:global_step: 362, logpy: -46.981, kl: 55.007, loss: 91.001\n",
      " 36%|███▋      | 363/1000 [14:21<25:28,  2.40s/it]INFO:root:global_step: 363, logpy: -46.492, kl: 54.825, loss: 90.441\n",
      " 36%|███▋      | 364/1000 [14:23<25:35,  2.41s/it]INFO:root:global_step: 364, logpy: -45.978, kl: 54.641, loss: 89.851\n",
      " 36%|███▋      | 365/1000 [14:25<25:32,  2.41s/it]INFO:root:global_step: 365, logpy: -45.434, kl: 54.446, loss: 89.220\n",
      " 37%|███▋      | 366/1000 [14:28<25:14,  2.39s/it]INFO:root:global_step: 366, logpy: -44.887, kl: 54.215, loss: 88.549\n",
      " 37%|███▋      | 367/1000 [14:30<25:05,  2.38s/it]INFO:root:global_step: 367, logpy: -44.429, kl: 54.054, loss: 88.035\n",
      " 37%|███▋      | 368/1000 [14:33<25:06,  2.38s/it]INFO:root:global_step: 368, logpy: -44.002, kl: 53.860, loss: 87.519\n",
      " 37%|███▋      | 369/1000 [14:35<25:11,  2.40s/it]INFO:root:global_step: 369, logpy: -43.629, kl: 53.641, loss: 87.030\n",
      " 37%|███▋      | 370/1000 [14:37<24:56,  2.38s/it]INFO:root:global_step: 370, logpy: -43.159, kl: 53.424, loss: 86.445\n",
      " 37%|███▋      | 371/1000 [14:40<25:03,  2.39s/it]INFO:root:global_step: 371, logpy: -42.661, kl: 53.231, loss: 85.856\n",
      " 37%|███▋      | 372/1000 [14:42<25:06,  2.40s/it]INFO:root:global_step: 372, logpy: -42.198, kl: 53.090, loss: 85.352\n",
      " 37%|███▋      | 373/1000 [14:45<25:03,  2.40s/it]INFO:root:global_step: 373, logpy: -41.797, kl: 52.907, loss: 84.868\n",
      " 37%|███▋      | 374/1000 [14:47<25:03,  2.40s/it]INFO:root:global_step: 374, logpy: -41.495, kl: 52.711, loss: 84.468\n",
      " 38%|███▊      | 375/1000 [14:49<25:14,  2.42s/it]INFO:root:global_step: 375, logpy: -41.042, kl: 52.537, loss: 83.939\n",
      " 38%|███▊      | 376/1000 [14:52<25:24,  2.44s/it]INFO:root:global_step: 376, logpy: -40.558, kl: 52.345, loss: 83.359\n",
      " 38%|███▊      | 377/1000 [14:54<25:13,  2.43s/it]INFO:root:global_step: 377, logpy: -40.172, kl: 52.172, loss: 82.895\n",
      " 38%|███▊      | 378/1000 [14:57<25:50,  2.49s/it]INFO:root:global_step: 378, logpy: -39.877, kl: 52.002, loss: 82.525\n",
      " 38%|███▊      | 379/1000 [14:59<25:45,  2.49s/it]INFO:root:global_step: 379, logpy: -39.517, kl: 51.813, loss: 82.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 380/1000 [15:02<25:33,  2.47s/it]INFO:root:global_step: 380, logpy: -39.068, kl: 51.668, loss: 81.568\n",
      " 38%|███▊      | 381/1000 [15:04<25:01,  2.43s/it]INFO:root:global_step: 381, logpy: -38.632, kl: 51.502, loss: 81.057\n",
      " 38%|███▊      | 382/1000 [15:07<25:04,  2.43s/it]INFO:root:global_step: 382, logpy: -38.310, kl: 51.337, loss: 80.661\n",
      " 38%|███▊      | 383/1000 [15:09<24:53,  2.42s/it]INFO:root:global_step: 383, logpy: -37.955, kl: 51.177, loss: 80.236\n",
      " 38%|███▊      | 384/1000 [15:11<24:52,  2.42s/it]INFO:root:global_step: 384, logpy: -37.554, kl: 51.001, loss: 79.748\n",
      " 38%|███▊      | 385/1000 [15:14<24:40,  2.41s/it]INFO:root:global_step: 385, logpy: -37.116, kl: 50.830, loss: 79.227\n",
      " 39%|███▊      | 386/1000 [15:16<24:36,  2.40s/it]INFO:root:global_step: 386, logpy: -36.669, kl: 50.686, loss: 78.723\n",
      " 39%|███▊      | 387/1000 [15:19<25:15,  2.47s/it]INFO:root:global_step: 387, logpy: -36.214, kl: 50.516, loss: 78.184\n",
      " 39%|███▉      | 388/1000 [15:21<25:01,  2.45s/it]INFO:root:global_step: 388, logpy: -35.769, kl: 50.351, loss: 77.660\n",
      " 39%|███▉      | 389/1000 [15:24<24:56,  2.45s/it]INFO:root:global_step: 389, logpy: -35.331, kl: 50.227, loss: 77.183\n",
      " 39%|███▉      | 390/1000 [15:26<24:50,  2.44s/it]INFO:root:global_step: 390, logpy: -34.902, kl: 50.074, loss: 76.684\n",
      " 39%|███▉      | 391/1000 [15:29<24:45,  2.44s/it]INFO:root:global_step: 391, logpy: -34.466, kl: 49.903, loss: 76.160\n",
      " 39%|███▉      | 392/1000 [15:31<24:42,  2.44s/it]INFO:root:global_step: 392, logpy: -34.069, kl: 49.775, loss: 75.717\n",
      " 39%|███▉      | 393/1000 [15:33<24:38,  2.44s/it]INFO:root:global_step: 393, logpy: -33.682, kl: 49.621, loss: 75.258\n",
      " 39%|███▉      | 394/1000 [15:36<24:39,  2.44s/it]INFO:root:global_step: 394, logpy: -33.310, kl: 49.460, loss: 74.805\n",
      " 40%|███▉      | 395/1000 [15:38<24:24,  2.42s/it]INFO:root:global_step: 395, logpy: -32.936, kl: 49.321, loss: 74.371\n",
      " 40%|███▉      | 396/1000 [15:41<24:13,  2.41s/it]INFO:root:global_step: 396, logpy: -32.544, kl: 49.183, loss: 73.920\n",
      " 40%|███▉      | 397/1000 [15:43<24:21,  2.42s/it]INFO:root:global_step: 397, logpy: -32.127, kl: 49.035, loss: 73.433\n",
      " 40%|███▉      | 398/1000 [15:45<24:09,  2.41s/it]INFO:root:global_step: 398, logpy: -31.694, kl: 48.870, loss: 72.913\n",
      " 40%|███▉      | 399/1000 [15:48<23:56,  2.39s/it]INFO:root:global_step: 399, logpy: -31.284, kl: 48.698, loss: 72.407\n",
      " 40%|████      | 400/1000 [15:50<23:50,  2.38s/it]INFO:root:Saved figure at: ./sim/global_step_400.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 400, logpy: -30.931, kl: 48.529, loss: 71.961\n",
      " 40%|████      | 401/1000 [15:54<28:23,  2.84s/it]INFO:root:global_step: 401, logpy: -30.653, kl: 48.401, loss: 71.630\n",
      " 40%|████      | 402/1000 [15:57<27:14,  2.73s/it]INFO:root:global_step: 402, logpy: -30.377, kl: 48.266, loss: 71.293\n",
      " 40%|████      | 403/1000 [15:59<26:14,  2.64s/it]INFO:root:global_step: 403, logpy: -30.028, kl: 48.123, loss: 70.875\n",
      " 40%|████      | 404/1000 [16:01<25:20,  2.55s/it]INFO:root:global_step: 404, logpy: -29.623, kl: 47.974, loss: 70.394\n",
      " 40%|████      | 405/1000 [16:04<25:08,  2.53s/it]INFO:root:global_step: 405, logpy: -29.245, kl: 47.831, loss: 69.945\n",
      " 41%|████      | 406/1000 [16:07<25:48,  2.61s/it]INFO:root:global_step: 406, logpy: -28.931, kl: 47.699, loss: 69.570\n",
      " 41%|████      | 407/1000 [16:09<25:47,  2.61s/it]INFO:root:global_step: 407, logpy: -28.710, kl: 47.556, loss: 69.277\n",
      " 41%|████      | 408/1000 [16:12<25:14,  2.56s/it]INFO:root:global_step: 408, logpy: -28.403, kl: 47.422, loss: 68.905\n",
      " 41%|████      | 409/1000 [16:14<24:56,  2.53s/it]INFO:root:global_step: 409, logpy: -28.012, kl: 47.253, loss: 68.415\n",
      " 41%|████      | 410/1000 [16:17<24:33,  2.50s/it]INFO:root:global_step: 410, logpy: -27.635, kl: 47.118, loss: 67.971\n",
      " 41%|████      | 411/1000 [16:19<24:08,  2.46s/it]INFO:root:global_step: 411, logpy: -27.290, kl: 46.997, loss: 67.573\n",
      " 41%|████      | 412/1000 [16:21<23:54,  2.44s/it]INFO:root:global_step: 412, logpy: -26.998, kl: 46.867, loss: 67.218\n",
      " 41%|████▏     | 413/1000 [16:24<24:05,  2.46s/it]INFO:root:global_step: 413, logpy: -26.717, kl: 46.718, loss: 66.855\n",
      " 41%|████▏     | 414/1000 [16:26<24:20,  2.49s/it]INFO:root:global_step: 414, logpy: -26.455, kl: 46.581, loss: 66.521\n",
      " 42%|████▏     | 415/1000 [16:29<23:49,  2.44s/it]INFO:root:global_step: 415, logpy: -26.158, kl: 46.470, loss: 66.178\n",
      " 42%|████▏     | 416/1000 [16:31<23:35,  2.42s/it]INFO:root:global_step: 416, logpy: -25.832, kl: 46.343, loss: 65.790\n",
      " 42%|████▏     | 417/1000 [16:33<23:30,  2.42s/it]INFO:root:global_step: 417, logpy: -25.475, kl: 46.228, loss: 65.382\n",
      " 42%|████▏     | 418/1000 [16:36<23:29,  2.42s/it]INFO:root:global_step: 418, logpy: -25.108, kl: 46.075, loss: 64.925\n",
      " 42%|████▏     | 419/1000 [16:38<23:35,  2.44s/it]INFO:root:global_step: 419, logpy: -24.744, kl: 45.930, loss: 64.479\n",
      " 42%|████▏     | 420/1000 [16:41<23:31,  2.43s/it]INFO:root:global_step: 420, logpy: -24.412, kl: 45.795, loss: 64.073\n",
      " 42%|████▏     | 421/1000 [16:43<23:31,  2.44s/it]INFO:root:global_step: 421, logpy: -24.135, kl: 45.667, loss: 63.730\n",
      " 42%|████▏     | 422/1000 [16:46<23:18,  2.42s/it]INFO:root:global_step: 422, logpy: -23.913, kl: 45.522, loss: 63.424\n",
      " 42%|████▏     | 423/1000 [16:48<23:10,  2.41s/it]INFO:root:global_step: 423, logpy: -23.691, kl: 45.409, loss: 63.148\n",
      " 42%|████▏     | 424/1000 [16:50<22:59,  2.39s/it]INFO:root:global_step: 424, logpy: -23.387, kl: 45.277, loss: 62.772\n",
      " 42%|████▎     | 425/1000 [16:53<22:58,  2.40s/it]INFO:root:global_step: 425, logpy: -23.041, kl: 45.130, loss: 62.338\n",
      " 43%|████▎     | 426/1000 [16:55<23:26,  2.45s/it]INFO:root:global_step: 426, logpy: -22.740, kl: 44.988, loss: 61.954\n",
      " 43%|████▎     | 427/1000 [16:58<23:54,  2.50s/it]INFO:root:global_step: 427, logpy: -22.541, kl: 44.869, loss: 61.693\n",
      " 43%|████▎     | 428/1000 [17:00<23:39,  2.48s/it]INFO:root:global_step: 428, logpy: -22.428, kl: 44.748, loss: 61.516\n",
      " 43%|████▎     | 429/1000 [17:03<23:29,  2.47s/it]INFO:root:global_step: 429, logpy: -22.130, kl: 44.626, loss: 61.153\n",
      " 43%|████▎     | 430/1000 [17:05<23:29,  2.47s/it]INFO:root:global_step: 430, logpy: -21.844, kl: 44.524, loss: 60.821\n",
      " 43%|████▎     | 431/1000 [17:08<23:05,  2.44s/it]INFO:root:global_step: 431, logpy: -21.694, kl: 44.402, loss: 60.605\n",
      " 43%|████▎     | 432/1000 [17:10<23:35,  2.49s/it]INFO:root:global_step: 432, logpy: -21.566, kl: 44.298, loss: 60.428\n",
      " 43%|████▎     | 433/1000 [17:13<23:26,  2.48s/it]INFO:root:global_step: 433, logpy: -21.285, kl: 44.188, loss: 60.091\n",
      " 43%|████▎     | 434/1000 [17:15<23:10,  2.46s/it]INFO:root:global_step: 434, logpy: -20.967, kl: 44.088, loss: 59.726\n",
      " 44%|████▎     | 435/1000 [17:18<23:04,  2.45s/it]INFO:root:global_step: 435, logpy: -20.721, kl: 43.943, loss: 59.389\n",
      " 44%|████▎     | 436/1000 [17:20<23:03,  2.45s/it]INFO:root:global_step: 436, logpy: -20.619, kl: 43.814, loss: 59.211\n",
      " 44%|████▎     | 437/1000 [17:22<22:47,  2.43s/it]INFO:root:global_step: 437, logpy: -20.348, kl: 43.686, loss: 58.864\n",
      " 44%|████▍     | 438/1000 [17:25<22:42,  2.43s/it]INFO:root:global_step: 438, logpy: -20.027, kl: 43.562, loss: 58.471\n",
      " 44%|████▍     | 439/1000 [17:27<22:55,  2.45s/it]INFO:root:global_step: 439, logpy: -19.756, kl: 43.436, loss: 58.124\n",
      " 44%|████▍     | 440/1000 [17:30<23:08,  2.48s/it]INFO:root:global_step: 440, logpy: -19.592, kl: 43.301, loss: 57.876\n",
      " 44%|████▍     | 441/1000 [17:32<22:50,  2.45s/it]INFO:root:global_step: 441, logpy: -19.429, kl: 43.194, loss: 57.657\n",
      " 44%|████▍     | 442/1000 [17:35<22:32,  2.42s/it]INFO:root:global_step: 442, logpy: -19.130, kl: 43.097, loss: 57.309\n",
      " 44%|████▍     | 443/1000 [17:37<22:21,  2.41s/it]INFO:root:global_step: 443, logpy: -18.866, kl: 42.986, loss: 56.985\n",
      " 44%|████▍     | 444/1000 [17:39<22:21,  2.41s/it]INFO:root:global_step: 444, logpy: -18.750, kl: 42.864, loss: 56.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 445/1000 [17:42<22:07,  2.39s/it]INFO:root:global_step: 445, logpy: -18.576, kl: 42.772, loss: 56.577\n",
      " 45%|████▍     | 446/1000 [17:44<22:20,  2.42s/it]INFO:root:global_step: 446, logpy: -18.293, kl: 42.665, loss: 56.235\n",
      " 45%|████▍     | 447/1000 [17:47<22:19,  2.42s/it]INFO:root:global_step: 447, logpy: -18.029, kl: 42.562, loss: 55.915\n",
      " 45%|████▍     | 448/1000 [17:49<22:12,  2.41s/it]INFO:root:global_step: 448, logpy: -17.886, kl: 42.442, loss: 55.699\n",
      " 45%|████▍     | 449/1000 [17:52<22:30,  2.45s/it]INFO:root:global_step: 449, logpy: -17.799, kl: 42.341, loss: 55.557\n",
      " 45%|████▌     | 450/1000 [17:54<22:30,  2.45s/it]INFO:root:Saved figure at: ./sim/global_step_450.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 450, logpy: -17.500, kl: 42.251, loss: 55.214\n",
      " 45%|████▌     | 451/1000 [17:58<25:46,  2.82s/it]INFO:root:global_step: 451, logpy: -17.283, kl: 42.148, loss: 54.939\n",
      " 45%|████▌     | 452/1000 [18:00<25:20,  2.77s/it]INFO:root:global_step: 452, logpy: -17.259, kl: 42.047, loss: 54.859\n",
      " 45%|████▌     | 453/1000 [18:03<24:42,  2.71s/it]INFO:root:global_step: 453, logpy: -17.052, kl: 41.949, loss: 54.598\n",
      " 45%|████▌     | 454/1000 [18:05<23:57,  2.63s/it]INFO:root:global_step: 454, logpy: -16.774, kl: 41.840, loss: 54.256\n",
      " 46%|████▌     | 455/1000 [18:08<23:13,  2.56s/it]INFO:root:global_step: 455, logpy: -16.699, kl: 41.748, loss: 54.132\n",
      " 46%|████▌     | 456/1000 [18:10<22:54,  2.53s/it]INFO:root:global_step: 456, logpy: -16.546, kl: 41.648, loss: 53.922\n",
      " 46%|████▌     | 457/1000 [18:13<22:27,  2.48s/it]INFO:root:global_step: 457, logpy: -16.331, kl: 41.557, loss: 53.659\n",
      " 46%|████▌     | 458/1000 [18:15<22:09,  2.45s/it]INFO:root:global_step: 458, logpy: -16.049, kl: 41.455, loss: 53.317\n",
      " 46%|████▌     | 459/1000 [18:17<21:54,  2.43s/it]INFO:root:global_step: 459, logpy: -15.849, kl: 41.377, loss: 53.081\n",
      " 46%|████▌     | 460/1000 [18:20<21:54,  2.43s/it]INFO:root:global_step: 460, logpy: -15.697, kl: 41.276, loss: 52.871\n",
      " 46%|████▌     | 461/1000 [18:22<22:04,  2.46s/it]INFO:root:global_step: 461, logpy: -15.503, kl: 41.154, loss: 52.595\n",
      " 46%|████▌     | 462/1000 [18:25<22:04,  2.46s/it]INFO:root:global_step: 462, logpy: -15.229, kl: 41.061, loss: 52.269\n",
      " 46%|████▋     | 463/1000 [18:27<21:53,  2.45s/it]INFO:root:global_step: 463, logpy: -14.980, kl: 40.950, loss: 51.948\n",
      " 46%|████▋     | 464/1000 [18:30<21:43,  2.43s/it]INFO:root:global_step: 464, logpy: -14.740, kl: 40.863, loss: 51.662\n",
      " 46%|████▋     | 465/1000 [18:32<21:34,  2.42s/it]INFO:root:global_step: 465, logpy: -14.559, kl: 40.737, loss: 51.394\n",
      " 47%|████▋     | 466/1000 [18:35<21:59,  2.47s/it]INFO:root:global_step: 466, logpy: -14.337, kl: 40.637, loss: 51.111\n",
      " 47%|████▋     | 467/1000 [18:37<21:42,  2.44s/it]INFO:root:global_step: 467, logpy: -14.108, kl: 40.558, loss: 50.842\n",
      " 47%|████▋     | 468/1000 [18:39<21:29,  2.42s/it]INFO:root:global_step: 468, logpy: -13.863, kl: 40.490, loss: 50.566\n",
      " 47%|████▋     | 469/1000 [18:42<21:14,  2.40s/it]INFO:root:global_step: 469, logpy: -13.618, kl: 40.399, loss: 50.268\n",
      " 47%|████▋     | 470/1000 [18:44<21:16,  2.41s/it]INFO:root:global_step: 470, logpy: -13.368, kl: 40.301, loss: 49.959\n",
      " 47%|████▋     | 471/1000 [18:47<21:04,  2.39s/it]INFO:root:global_step: 471, logpy: -13.160, kl: 40.218, loss: 49.704\n",
      " 47%|████▋     | 472/1000 [18:49<21:02,  2.39s/it]INFO:root:global_step: 472, logpy: -12.962, kl: 40.111, loss: 49.436\n",
      " 47%|████▋     | 473/1000 [18:51<20:54,  2.38s/it]INFO:root:global_step: 473, logpy: -12.788, kl: 40.017, loss: 49.205\n",
      " 47%|████▋     | 474/1000 [18:54<21:13,  2.42s/it]INFO:root:global_step: 474, logpy: -12.596, kl: 39.926, loss: 48.958\n",
      " 48%|████▊     | 475/1000 [18:56<21:34,  2.47s/it]INFO:root:global_step: 475, logpy: -12.382, kl: 39.827, loss: 48.679\n",
      " 48%|████▊     | 476/1000 [18:59<21:48,  2.50s/it]INFO:root:global_step: 476, logpy: -12.141, kl: 39.760, loss: 48.407\n",
      " 48%|████▊     | 477/1000 [19:01<21:28,  2.46s/it]INFO:root:global_step: 477, logpy: -11.898, kl: 39.683, loss: 48.122\n",
      " 48%|████▊     | 478/1000 [19:04<21:20,  2.45s/it]INFO:root:global_step: 478, logpy: -11.666, kl: 39.591, loss: 47.833\n",
      " 48%|████▊     | 479/1000 [19:06<21:12,  2.44s/it]INFO:root:global_step: 479, logpy: -11.456, kl: 39.505, loss: 47.570\n",
      " 48%|████▊     | 480/1000 [19:09<21:20,  2.46s/it]INFO:root:global_step: 480, logpy: -11.287, kl: 39.418, loss: 47.349\n",
      " 48%|████▊     | 481/1000 [19:11<21:27,  2.48s/it]INFO:root:global_step: 481, logpy: -11.115, kl: 39.334, loss: 47.127\n",
      " 48%|████▊     | 482/1000 [19:14<21:18,  2.47s/it]INFO:root:global_step: 482, logpy: -10.911, kl: 39.258, loss: 46.880\n",
      " 48%|████▊     | 483/1000 [19:16<21:12,  2.46s/it]INFO:root:global_step: 483, logpy: -10.688, kl: 39.175, loss: 46.607\n",
      " 48%|████▊     | 484/1000 [19:18<21:00,  2.44s/it]INFO:root:global_step: 484, logpy: -10.458, kl: 39.074, loss: 46.309\n",
      " 48%|████▊     | 485/1000 [19:21<20:55,  2.44s/it]INFO:root:global_step: 485, logpy: -10.229, kl: 39.004, loss: 46.042\n",
      " 49%|████▊     | 486/1000 [19:23<20:43,  2.42s/it]INFO:root:global_step: 486, logpy: -10.009, kl: 38.932, loss: 45.781\n",
      " 49%|████▊     | 487/1000 [19:26<20:43,  2.42s/it]INFO:root:global_step: 487, logpy: -9.775, kl: 38.839, loss: 45.487\n",
      " 49%|████▉     | 488/1000 [19:28<20:42,  2.43s/it]INFO:root:global_step: 488, logpy: -9.566, kl: 38.796, loss: 45.265\n",
      " 49%|████▉     | 489/1000 [19:30<20:29,  2.41s/it]INFO:root:global_step: 489, logpy: -9.352, kl: 38.735, loss: 45.021\n",
      " 49%|████▉     | 490/1000 [19:33<20:27,  2.41s/it]INFO:root:global_step: 490, logpy: -9.131, kl: 38.651, loss: 44.747\n",
      " 49%|████▉     | 491/1000 [19:35<20:49,  2.45s/it]INFO:root:global_step: 491, logpy: -8.918, kl: 38.575, loss: 44.489\n",
      " 49%|████▉     | 492/1000 [19:38<20:33,  2.43s/it]INFO:root:global_step: 492, logpy: -8.716, kl: 38.508, loss: 44.250\n",
      " 49%|████▉     | 493/1000 [19:40<20:37,  2.44s/it]INFO:root:global_step: 493, logpy: -8.533, kl: 38.436, loss: 44.023\n",
      " 49%|████▉     | 494/1000 [19:43<20:27,  2.43s/it]INFO:root:global_step: 494, logpy: -8.406, kl: 38.374, loss: 43.864\n",
      " 50%|████▉     | 495/1000 [19:45<20:25,  2.43s/it]INFO:root:global_step: 495, logpy: -8.302, kl: 38.281, loss: 43.697\n",
      " 50%|████▉     | 496/1000 [19:48<20:28,  2.44s/it]INFO:root:global_step: 496, logpy: -8.183, kl: 38.224, loss: 43.549\n",
      " 50%|████▉     | 497/1000 [19:50<20:13,  2.41s/it]INFO:root:global_step: 497, logpy: -8.001, kl: 38.158, loss: 43.331\n",
      " 50%|████▉     | 498/1000 [19:52<20:09,  2.41s/it]INFO:root:global_step: 498, logpy: -7.784, kl: 38.076, loss: 43.059\n",
      " 50%|████▉     | 499/1000 [19:55<20:12,  2.42s/it]INFO:root:global_step: 499, logpy: -7.587, kl: 37.995, loss: 42.809\n",
      " 50%|█████     | 500/1000 [19:57<20:13,  2.43s/it]INFO:root:Saved figure at: ./sim/global_step_500.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 500, logpy: -7.448, kl: 37.931, loss: 42.634\n",
      " 50%|█████     | 501/1000 [20:01<23:06,  2.78s/it]INFO:root:global_step: 501, logpy: -7.358, kl: 37.850, loss: 42.491\n",
      " 50%|█████     | 502/1000 [20:03<22:32,  2.72s/it]INFO:root:global_step: 502, logpy: -7.257, kl: 37.805, loss: 42.372\n",
      " 50%|█████     | 503/1000 [20:06<21:41,  2.62s/it]INFO:root:global_step: 503, logpy: -7.099, kl: 37.716, loss: 42.152\n",
      " 50%|█████     | 504/1000 [20:08<21:19,  2.58s/it]INFO:root:global_step: 504, logpy: -6.903, kl: 37.631, loss: 41.897\n",
      " 50%|█████     | 505/1000 [20:11<21:24,  2.60s/it]INFO:root:global_step: 505, logpy: -6.695, kl: 37.553, loss: 41.637\n",
      " 51%|█████     | 506/1000 [20:13<20:48,  2.53s/it]INFO:root:global_step: 506, logpy: -6.506, kl: 37.480, loss: 41.402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 507/1000 [20:16<20:26,  2.49s/it]INFO:root:global_step: 507, logpy: -6.367, kl: 37.404, loss: 41.212\n",
      " 51%|█████     | 508/1000 [20:18<20:11,  2.46s/it]INFO:root:global_step: 508, logpy: -6.284, kl: 37.333, loss: 41.085\n",
      " 51%|█████     | 509/1000 [20:20<19:55,  2.43s/it]INFO:root:global_step: 509, logpy: -6.204, kl: 37.269, loss: 40.965\n",
      " 51%|█████     | 510/1000 [20:23<19:41,  2.41s/it]INFO:root:global_step: 510, logpy: -6.045, kl: 37.205, loss: 40.768\n",
      " 51%|█████     | 511/1000 [20:25<19:35,  2.40s/it]INFO:root:global_step: 511, logpy: -5.850, kl: 37.131, loss: 40.523\n",
      " 51%|█████     | 512/1000 [20:28<19:41,  2.42s/it]INFO:root:global_step: 512, logpy: -5.688, kl: 37.048, loss: 40.303\n",
      " 51%|█████▏    | 513/1000 [20:30<19:48,  2.44s/it]INFO:root:global_step: 513, logpy: -5.608, kl: 36.980, loss: 40.179\n",
      " 51%|█████▏    | 514/1000 [20:33<20:12,  2.50s/it]INFO:root:global_step: 514, logpy: -5.515, kl: 36.914, loss: 40.045\n",
      " 52%|█████▏    | 515/1000 [20:35<19:59,  2.47s/it]INFO:root:global_step: 515, logpy: -5.373, kl: 36.854, loss: 39.866\n",
      " 52%|█████▏    | 516/1000 [20:38<19:40,  2.44s/it]INFO:root:global_step: 516, logpy: -5.171, kl: 36.760, loss: 39.593\n",
      " 52%|█████▏    | 517/1000 [20:40<19:40,  2.44s/it]INFO:root:global_step: 517, logpy: -4.983, kl: 36.707, loss: 39.376\n",
      " 52%|█████▏    | 518/1000 [20:42<19:38,  2.45s/it]INFO:root:global_step: 518, logpy: -4.806, kl: 36.666, loss: 39.182\n",
      " 52%|█████▏    | 519/1000 [20:45<19:33,  2.44s/it]INFO:root:global_step: 519, logpy: -4.665, kl: 36.624, loss: 39.021\n",
      " 52%|█████▏    | 520/1000 [20:47<19:42,  2.46s/it]INFO:root:global_step: 520, logpy: -4.540, kl: 36.556, loss: 38.851\n",
      " 52%|█████▏    | 521/1000 [20:50<20:03,  2.51s/it]INFO:root:global_step: 521, logpy: -4.422, kl: 36.482, loss: 38.682\n",
      " 52%|█████▏    | 522/1000 [20:52<19:47,  2.48s/it]INFO:root:global_step: 522, logpy: -4.282, kl: 36.412, loss: 38.494\n",
      " 52%|█████▏    | 523/1000 [20:55<20:01,  2.52s/it]INFO:root:global_step: 523, logpy: -4.111, kl: 36.341, loss: 38.274\n",
      " 52%|█████▏    | 524/1000 [20:58<20:02,  2.53s/it]INFO:root:global_step: 524, logpy: -3.930, kl: 36.259, loss: 38.033\n",
      " 52%|█████▎    | 525/1000 [21:00<19:56,  2.52s/it]INFO:root:global_step: 525, logpy: -3.744, kl: 36.199, loss: 37.808\n",
      " 53%|█████▎    | 526/1000 [21:03<19:50,  2.51s/it]INFO:root:global_step: 526, logpy: -3.570, kl: 36.145, loss: 37.600\n",
      " 53%|█████▎    | 527/1000 [21:05<19:36,  2.49s/it]INFO:root:global_step: 527, logpy: -3.432, kl: 36.085, loss: 37.425\n",
      " 53%|█████▎    | 528/1000 [21:07<19:22,  2.46s/it]INFO:root:global_step: 528, logpy: -3.362, kl: 36.037, loss: 37.327\n",
      " 53%|█████▎    | 529/1000 [21:10<19:07,  2.44s/it]INFO:root:global_step: 529, logpy: -3.290, kl: 35.967, loss: 37.206\n",
      " 53%|█████▎    | 530/1000 [21:12<19:14,  2.46s/it]INFO:root:global_step: 530, logpy: -3.163, kl: 35.906, loss: 37.039\n",
      " 53%|█████▎    | 531/1000 [21:15<19:02,  2.44s/it]INFO:root:global_step: 531, logpy: -2.988, kl: 35.840, loss: 36.818\n",
      " 53%|█████▎    | 532/1000 [21:17<18:55,  2.43s/it]INFO:root:global_step: 532, logpy: -2.800, kl: 35.753, loss: 36.563\n",
      " 53%|█████▎    | 533/1000 [21:20<19:25,  2.50s/it]INFO:root:global_step: 533, logpy: -2.679, kl: 35.698, loss: 36.407\n",
      " 53%|█████▎    | 534/1000 [21:22<19:07,  2.46s/it]INFO:root:global_step: 534, logpy: -2.628, kl: 35.631, loss: 36.309\n",
      " 54%|█████▎    | 535/1000 [21:25<19:01,  2.45s/it]INFO:root:global_step: 535, logpy: -2.601, kl: 35.587, loss: 36.258\n",
      " 54%|█████▎    | 536/1000 [21:27<19:08,  2.48s/it]INFO:root:global_step: 536, logpy: -2.469, kl: 35.520, loss: 36.078\n",
      " 54%|█████▎    | 537/1000 [21:29<18:51,  2.44s/it]INFO:root:global_step: 537, logpy: -2.295, kl: 35.451, loss: 35.853\n",
      " 54%|█████▍    | 538/1000 [21:32<18:48,  2.44s/it]INFO:root:global_step: 538, logpy: -2.130, kl: 35.387, loss: 35.643\n",
      " 54%|█████▍    | 539/1000 [21:34<18:44,  2.44s/it]INFO:root:global_step: 539, logpy: -2.017, kl: 35.309, loss: 35.472\n",
      " 54%|█████▍    | 540/1000 [21:37<18:50,  2.46s/it]INFO:root:global_step: 540, logpy: -1.953, kl: 35.236, loss: 35.353\n",
      " 54%|█████▍    | 541/1000 [21:39<18:50,  2.46s/it]INFO:root:global_step: 541, logpy: -1.851, kl: 35.191, loss: 35.225\n",
      " 54%|█████▍    | 542/1000 [21:42<18:37,  2.44s/it]INFO:root:global_step: 542, logpy: -1.690, kl: 35.142, loss: 35.033\n",
      " 54%|█████▍    | 543/1000 [21:44<18:35,  2.44s/it]INFO:root:global_step: 543, logpy: -1.517, kl: 35.075, loss: 34.810\n",
      " 54%|█████▍    | 544/1000 [21:47<18:34,  2.44s/it]INFO:root:global_step: 544, logpy: -1.363, kl: 35.008, loss: 34.607\n",
      " 55%|█████▍    | 545/1000 [21:49<18:27,  2.43s/it]INFO:root:global_step: 545, logpy: -1.257, kl: 34.958, loss: 34.469\n",
      " 55%|█████▍    | 546/1000 [21:51<18:26,  2.44s/it]INFO:root:global_step: 546, logpy: -1.166, kl: 34.894, loss: 34.332\n",
      " 55%|█████▍    | 547/1000 [21:54<18:44,  2.48s/it]INFO:root:global_step: 547, logpy: -1.049, kl: 34.825, loss: 34.162\n",
      " 55%|█████▍    | 548/1000 [21:56<18:35,  2.47s/it]INFO:root:global_step: 548, logpy: -0.898, kl: 34.759, loss: 33.962\n",
      " 55%|█████▍    | 549/1000 [21:59<18:37,  2.48s/it]INFO:root:global_step: 549, logpy: -0.729, kl: 34.705, loss: 33.757\n",
      " 55%|█████▌    | 550/1000 [22:01<18:24,  2.45s/it]INFO:root:Saved figure at: ./sim/global_step_550.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 550, logpy: -0.568, kl: 34.643, loss: 33.551\n",
      " 55%|█████▌    | 551/1000 [22:05<22:01,  2.94s/it]INFO:root:global_step: 551, logpy: -0.449, kl: 34.573, loss: 33.378\n",
      " 55%|█████▌    | 552/1000 [22:08<21:09,  2.83s/it]INFO:root:global_step: 552, logpy: -0.372, kl: 34.518, loss: 33.262\n",
      " 55%|█████▌    | 553/1000 [22:10<20:18,  2.73s/it]INFO:root:global_step: 553, logpy: -0.285, kl: 34.472, loss: 33.145\n",
      " 55%|█████▌    | 554/1000 [22:13<19:28,  2.62s/it]INFO:root:global_step: 554, logpy: -0.158, kl: 34.419, loss: 32.981\n",
      " 56%|█████▌    | 555/1000 [22:15<19:01,  2.56s/it]INFO:root:global_step: 555, logpy: -0.023, kl: 34.365, loss: 32.809\n",
      " 56%|█████▌    | 556/1000 [22:18<18:38,  2.52s/it]INFO:root:global_step: 556, logpy: 0.148, kl: 34.301, loss: 32.589\n",
      " 56%|█████▌    | 557/1000 [22:20<18:26,  2.50s/it]INFO:root:global_step: 557, logpy: 0.281, kl: 34.255, loss: 32.426\n",
      " 56%|█████▌    | 558/1000 [22:23<18:30,  2.51s/it]INFO:root:global_step: 558, logpy: 0.392, kl: 34.212, loss: 32.288\n",
      " 56%|█████▌    | 559/1000 [22:25<18:30,  2.52s/it]INFO:root:global_step: 559, logpy: 0.479, kl: 34.164, loss: 32.169\n",
      " 56%|█████▌    | 560/1000 [22:28<18:27,  2.52s/it]INFO:root:global_step: 560, logpy: 0.565, kl: 34.122, loss: 32.056\n",
      " 56%|█████▌    | 561/1000 [22:30<18:29,  2.53s/it]INFO:root:global_step: 561, logpy: 0.679, kl: 34.075, loss: 31.909\n",
      " 56%|█████▌    | 562/1000 [22:33<18:13,  2.50s/it]INFO:root:global_step: 562, logpy: 0.821, kl: 34.022, loss: 31.729\n",
      " 56%|█████▋    | 563/1000 [22:35<18:00,  2.47s/it]INFO:root:global_step: 563, logpy: 0.978, kl: 33.961, loss: 31.525\n",
      " 56%|█████▋    | 564/1000 [22:38<17:59,  2.48s/it]INFO:root:global_step: 564, logpy: 1.140, kl: 33.895, loss: 31.312\n",
      " 56%|█████▋    | 565/1000 [22:40<18:06,  2.50s/it]INFO:root:global_step: 565, logpy: 1.310, kl: 33.826, loss: 31.088\n",
      " 57%|█████▋    | 566/1000 [22:43<18:02,  2.49s/it]INFO:root:global_step: 566, logpy: 1.455, kl: 33.782, loss: 30.913\n",
      " 57%|█████▋    | 567/1000 [22:45<17:53,  2.48s/it]INFO:root:global_step: 567, logpy: 1.600, kl: 33.722, loss: 30.723\n",
      " 57%|█████▋    | 568/1000 [22:47<17:40,  2.46s/it]INFO:root:global_step: 568, logpy: 1.722, kl: 33.670, loss: 30.562\n",
      " 57%|█████▋    | 569/1000 [22:50<17:27,  2.43s/it]INFO:root:global_step: 569, logpy: 1.839, kl: 33.608, loss: 30.397\n",
      " 57%|█████▋    | 570/1000 [22:52<17:22,  2.42s/it]INFO:root:global_step: 570, logpy: 1.939, kl: 33.533, loss: 30.236\n",
      " 57%|█████▋    | 571/1000 [22:55<17:35,  2.46s/it]INFO:root:global_step: 571, logpy: 1.999, kl: 33.504, loss: 30.161\n",
      " 57%|█████▋    | 572/1000 [22:57<17:41,  2.48s/it]INFO:root:global_step: 572, logpy: 2.060, kl: 33.460, loss: 30.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 573/1000 [23:00<17:57,  2.52s/it]INFO:root:global_step: 573, logpy: 2.147, kl: 33.386, loss: 29.922\n",
      " 57%|█████▋    | 574/1000 [23:02<17:42,  2.49s/it]INFO:root:global_step: 574, logpy: 2.236, kl: 33.338, loss: 29.798\n",
      " 57%|█████▊    | 575/1000 [23:05<17:36,  2.49s/it]INFO:root:global_step: 575, logpy: 2.336, kl: 33.308, loss: 29.680\n",
      " 58%|█████▊    | 576/1000 [23:07<17:32,  2.48s/it]INFO:root:global_step: 576, logpy: 2.442, kl: 33.276, loss: 29.556\n",
      " 58%|█████▊    | 577/1000 [23:10<17:15,  2.45s/it]INFO:root:global_step: 577, logpy: 2.581, kl: 33.204, loss: 29.357\n",
      " 58%|█████▊    | 578/1000 [23:12<17:11,  2.45s/it]INFO:root:global_step: 578, logpy: 2.725, kl: 33.152, loss: 29.174\n",
      " 58%|█████▊    | 579/1000 [23:15<17:02,  2.43s/it]INFO:root:global_step: 579, logpy: 2.862, kl: 33.116, loss: 29.013\n",
      " 58%|█████▊    | 580/1000 [23:17<16:56,  2.42s/it]INFO:root:global_step: 580, logpy: 2.985, kl: 33.085, loss: 28.872\n",
      " 58%|█████▊    | 581/1000 [23:19<16:51,  2.41s/it]INFO:root:global_step: 581, logpy: 3.117, kl: 33.038, loss: 28.706\n",
      " 58%|█████▊    | 582/1000 [23:22<17:00,  2.44s/it]INFO:root:global_step: 582, logpy: 3.223, kl: 33.000, loss: 28.573\n",
      " 58%|█████▊    | 583/1000 [23:24<16:59,  2.44s/it]INFO:root:global_step: 583, logpy: 3.307, kl: 32.967, loss: 28.468\n",
      " 58%|█████▊    | 584/1000 [23:27<16:59,  2.45s/it]INFO:root:global_step: 584, logpy: 3.395, kl: 32.923, loss: 28.348\n",
      " 58%|█████▊    | 585/1000 [23:29<16:56,  2.45s/it]INFO:root:global_step: 585, logpy: 3.476, kl: 32.878, loss: 28.234\n",
      " 59%|█████▊    | 586/1000 [23:32<16:51,  2.44s/it]INFO:root:global_step: 586, logpy: 3.564, kl: 32.835, loss: 28.114\n",
      " 59%|█████▊    | 587/1000 [23:34<17:09,  2.49s/it]INFO:root:global_step: 587, logpy: 3.642, kl: 32.813, loss: 28.026\n",
      " 59%|█████▉    | 588/1000 [23:37<17:07,  2.49s/it]INFO:root:global_step: 588, logpy: 3.718, kl: 32.773, loss: 27.921\n",
      " 59%|█████▉    | 589/1000 [23:39<17:01,  2.49s/it]INFO:root:global_step: 589, logpy: 3.821, kl: 32.718, loss: 27.775\n",
      " 59%|█████▉    | 590/1000 [23:42<17:44,  2.60s/it]INFO:root:global_step: 590, logpy: 3.925, kl: 32.674, loss: 27.639\n",
      " 59%|█████▉    | 591/1000 [23:45<17:25,  2.56s/it]INFO:root:global_step: 591, logpy: 4.031, kl: 32.660, loss: 27.530\n",
      " 59%|█████▉    | 592/1000 [23:47<17:44,  2.61s/it]INFO:root:global_step: 592, logpy: 4.142, kl: 32.620, loss: 27.389\n",
      " 59%|█████▉    | 593/1000 [23:50<17:36,  2.60s/it]INFO:root:global_step: 593, logpy: 4.238, kl: 32.586, loss: 27.270\n",
      " 59%|█████▉    | 594/1000 [23:52<17:19,  2.56s/it]INFO:root:global_step: 594, logpy: 4.347, kl: 32.547, loss: 27.133\n",
      " 60%|█████▉    | 595/1000 [23:55<17:07,  2.54s/it]INFO:root:global_step: 595, logpy: 4.434, kl: 32.527, loss: 27.036\n",
      " 60%|█████▉    | 596/1000 [23:57<16:53,  2.51s/it]INFO:root:global_step: 596, logpy: 4.523, kl: 32.469, loss: 26.900\n",
      " 60%|█████▉    | 597/1000 [24:00<16:46,  2.50s/it]INFO:root:global_step: 597, logpy: 4.612, kl: 32.416, loss: 26.769\n",
      " 60%|█████▉    | 598/1000 [24:02<16:35,  2.48s/it]INFO:root:global_step: 598, logpy: 4.707, kl: 32.375, loss: 26.643\n",
      " 60%|█████▉    | 599/1000 [24:05<16:42,  2.50s/it]INFO:root:global_step: 599, logpy: 4.803, kl: 32.346, loss: 26.529\n",
      " 60%|██████    | 600/1000 [24:07<16:36,  2.49s/it]INFO:root:Saved figure at: ./sim/global_step_600.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 600, logpy: 4.910, kl: 32.321, loss: 26.407\n",
      " 60%|██████    | 601/1000 [24:11<19:18,  2.90s/it]INFO:root:global_step: 601, logpy: 5.017, kl: 32.272, loss: 26.261\n",
      " 60%|██████    | 602/1000 [24:14<18:32,  2.79s/it]INFO:root:global_step: 602, logpy: 5.141, kl: 32.224, loss: 26.098\n",
      " 60%|██████    | 603/1000 [24:16<17:47,  2.69s/it]INFO:root:global_step: 603, logpy: 5.259, kl: 32.178, loss: 25.944\n",
      " 60%|██████    | 604/1000 [24:18<17:16,  2.62s/it]INFO:root:global_step: 604, logpy: 5.380, kl: 32.129, loss: 25.785\n",
      " 60%|██████    | 605/1000 [24:21<16:53,  2.56s/it]INFO:root:global_step: 605, logpy: 5.504, kl: 32.079, loss: 25.620\n",
      " 61%|██████    | 606/1000 [24:23<16:41,  2.54s/it]INFO:root:global_step: 606, logpy: 5.599, kl: 32.051, loss: 25.506\n",
      " 61%|██████    | 607/1000 [24:26<16:26,  2.51s/it]INFO:root:global_step: 607, logpy: 5.690, kl: 32.002, loss: 25.376\n",
      " 61%|██████    | 608/1000 [24:28<16:15,  2.49s/it]INFO:root:global_step: 608, logpy: 5.756, kl: 31.944, loss: 25.261\n",
      " 61%|██████    | 609/1000 [24:31<16:03,  2.46s/it]INFO:root:global_step: 609, logpy: 5.754, kl: 31.892, loss: 25.220\n",
      " 61%|██████    | 610/1000 [24:33<16:01,  2.47s/it]INFO:root:global_step: 610, logpy: 5.744, kl: 31.840, loss: 25.187\n",
      " 61%|██████    | 611/1000 [24:36<15:49,  2.44s/it]INFO:root:global_step: 611, logpy: 5.792, kl: 31.791, loss: 25.100\n",
      " 61%|██████    | 612/1000 [24:38<15:48,  2.45s/it]INFO:root:global_step: 612, logpy: 5.887, kl: 31.769, loss: 24.992\n",
      " 61%|██████▏   | 613/1000 [24:41<16:16,  2.52s/it]INFO:root:global_step: 613, logpy: 5.993, kl: 31.738, loss: 24.863\n",
      " 61%|██████▏   | 614/1000 [24:43<16:10,  2.51s/it]INFO:root:global_step: 614, logpy: 6.070, kl: 31.709, loss: 24.766\n",
      " 62%|██████▏   | 615/1000 [24:45<15:45,  2.46s/it]INFO:root:global_step: 615, logpy: 6.117, kl: 31.667, loss: 24.686\n",
      " 62%|██████▏   | 616/1000 [24:48<15:39,  2.45s/it]INFO:root:global_step: 616, logpy: 6.135, kl: 31.629, loss: 24.639\n",
      " 62%|██████▏   | 617/1000 [24:50<15:47,  2.48s/it]INFO:root:global_step: 617, logpy: 6.158, kl: 31.602, loss: 24.598\n",
      " 62%|██████▏   | 618/1000 [24:53<16:07,  2.53s/it]INFO:root:global_step: 618, logpy: 6.227, kl: 31.573, loss: 24.508\n",
      " 62%|██████▏   | 619/1000 [24:56<16:57,  2.67s/it]INFO:root:global_step: 619, logpy: 6.329, kl: 31.544, loss: 24.385\n",
      " 62%|██████▏   | 620/1000 [24:59<16:48,  2.65s/it]INFO:root:global_step: 620, logpy: 6.435, kl: 31.517, loss: 24.260\n",
      " 62%|██████▏   | 621/1000 [25:01<16:36,  2.63s/it]INFO:root:global_step: 621, logpy: 6.525, kl: 31.481, loss: 24.142\n",
      " 62%|██████▏   | 622/1000 [25:04<16:17,  2.59s/it]INFO:root:global_step: 622, logpy: 6.563, kl: 31.438, loss: 24.069\n",
      " 62%|██████▏   | 623/1000 [25:07<16:37,  2.65s/it]INFO:root:global_step: 623, logpy: 6.573, kl: 31.390, loss: 24.019\n",
      " 62%|██████▏   | 624/1000 [25:10<17:19,  2.76s/it]INFO:root:global_step: 624, logpy: 6.620, kl: 31.348, loss: 23.939\n",
      " 62%|██████▎   | 625/1000 [25:13<17:38,  2.82s/it]INFO:root:global_step: 625, logpy: 6.706, kl: 31.298, loss: 23.811\n",
      " 63%|██████▎   | 626/1000 [25:15<17:29,  2.81s/it]INFO:root:global_step: 626, logpy: 6.812, kl: 31.262, loss: 23.677\n",
      " 63%|██████▎   | 627/1000 [25:18<16:59,  2.73s/it]INFO:root:global_step: 627, logpy: 6.901, kl: 31.234, loss: 23.566\n",
      " 63%|██████▎   | 628/1000 [25:22<18:42,  3.02s/it]INFO:root:global_step: 628, logpy: 7.011, kl: 31.187, loss: 23.417\n",
      " 63%|██████▎   | 629/1000 [25:24<18:18,  2.96s/it]INFO:root:global_step: 629, logpy: 7.096, kl: 31.154, loss: 23.307\n",
      " 63%|██████▎   | 630/1000 [25:27<17:53,  2.90s/it]INFO:root:global_step: 630, logpy: 7.159, kl: 31.109, loss: 23.207\n",
      " 63%|██████▎   | 631/1000 [25:30<17:24,  2.83s/it]INFO:root:global_step: 631, logpy: 7.173, kl: 31.087, loss: 23.178\n",
      " 63%|██████▎   | 632/1000 [25:32<16:45,  2.73s/it]INFO:root:global_step: 632, logpy: 7.177, kl: 31.058, loss: 23.152\n",
      " 63%|██████▎   | 633/1000 [25:35<16:19,  2.67s/it]INFO:root:global_step: 633, logpy: 7.225, kl: 31.015, loss: 23.069\n",
      " 63%|██████▎   | 634/1000 [25:37<16:10,  2.65s/it]INFO:root:global_step: 634, logpy: 7.304, kl: 31.000, loss: 22.982\n",
      " 64%|██████▎   | 635/1000 [25:40<16:12,  2.66s/it]INFO:root:global_step: 635, logpy: 7.402, kl: 30.960, loss: 22.851\n",
      " 64%|██████▎   | 636/1000 [25:43<16:10,  2.67s/it]INFO:root:global_step: 636, logpy: 7.488, kl: 30.921, loss: 22.733\n",
      " 64%|██████▎   | 637/1000 [25:46<16:27,  2.72s/it]INFO:root:global_step: 637, logpy: 7.539, kl: 30.885, loss: 22.653\n",
      " 64%|██████▍   | 638/1000 [25:48<16:08,  2.67s/it]INFO:root:global_step: 638, logpy: 7.578, kl: 30.845, loss: 22.581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 639/1000 [25:51<15:58,  2.65s/it]INFO:root:global_step: 639, logpy: 7.593, kl: 30.822, loss: 22.550\n",
      " 64%|██████▍   | 640/1000 [25:53<15:42,  2.62s/it]INFO:root:global_step: 640, logpy: 7.663, kl: 30.793, loss: 22.458\n",
      " 64%|██████▍   | 641/1000 [25:56<15:37,  2.61s/it]INFO:root:global_step: 641, logpy: 7.737, kl: 30.765, loss: 22.363\n",
      " 64%|██████▍   | 642/1000 [25:59<15:50,  2.65s/it]INFO:root:global_step: 642, logpy: 7.832, kl: 30.745, loss: 22.254\n",
      " 64%|██████▍   | 643/1000 [26:02<16:55,  2.84s/it]INFO:root:global_step: 643, logpy: 7.928, kl: 30.714, loss: 22.134\n",
      " 64%|██████▍   | 644/1000 [26:05<16:23,  2.76s/it]INFO:root:global_step: 644, logpy: 8.022, kl: 30.677, loss: 22.009\n",
      " 64%|██████▍   | 645/1000 [26:07<16:28,  2.78s/it]INFO:root:global_step: 645, logpy: 8.116, kl: 30.652, loss: 21.896\n",
      " 65%|██████▍   | 646/1000 [26:10<15:59,  2.71s/it]INFO:root:global_step: 646, logpy: 8.201, kl: 30.606, loss: 21.773\n",
      " 65%|██████▍   | 647/1000 [26:13<15:38,  2.66s/it]INFO:root:global_step: 647, logpy: 8.253, kl: 30.574, loss: 21.694\n",
      " 65%|██████▍   | 648/1000 [26:15<15:25,  2.63s/it]INFO:root:global_step: 648, logpy: 8.278, kl: 30.545, loss: 21.647\n",
      " 65%|██████▍   | 649/1000 [26:18<15:02,  2.57s/it]INFO:root:global_step: 649, logpy: 8.330, kl: 30.508, loss: 21.563\n",
      " 65%|██████▌   | 650/1000 [26:20<15:15,  2.62s/it]INFO:root:Saved figure at: ./sim/global_step_650.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 650, logpy: 8.412, kl: 30.480, loss: 21.460\n",
      " 65%|██████▌   | 651/1000 [26:25<18:26,  3.17s/it]INFO:root:global_step: 651, logpy: 8.502, kl: 30.448, loss: 21.344\n",
      " 65%|██████▌   | 652/1000 [26:28<17:47,  3.07s/it]INFO:root:global_step: 652, logpy: 8.598, kl: 30.424, loss: 21.230\n",
      " 65%|██████▌   | 653/1000 [26:30<17:34,  3.04s/it]INFO:root:global_step: 653, logpy: 8.668, kl: 30.417, loss: 21.159\n",
      " 65%|██████▌   | 654/1000 [26:33<16:58,  2.95s/it]INFO:root:global_step: 654, logpy: 8.721, kl: 30.390, loss: 21.085\n",
      " 66%|██████▌   | 655/1000 [26:36<16:24,  2.85s/it]INFO:root:global_step: 655, logpy: 8.773, kl: 30.353, loss: 21.002\n",
      " 66%|██████▌   | 656/1000 [26:38<15:43,  2.74s/it]INFO:root:global_step: 656, logpy: 8.794, kl: 30.337, loss: 20.971\n",
      " 66%|██████▌   | 657/1000 [26:41<15:07,  2.65s/it]INFO:root:global_step: 657, logpy: 8.826, kl: 30.296, loss: 20.903\n",
      " 66%|██████▌   | 658/1000 [26:43<14:48,  2.60s/it]INFO:root:global_step: 658, logpy: 8.906, kl: 30.263, loss: 20.796\n",
      " 66%|██████▌   | 659/1000 [26:46<14:32,  2.56s/it]INFO:root:global_step: 659, logpy: 9.007, kl: 30.222, loss: 20.659\n",
      " 66%|██████▌   | 660/1000 [26:48<14:24,  2.54s/it]INFO:root:global_step: 660, logpy: 9.093, kl: 30.194, loss: 20.551\n",
      " 66%|██████▌   | 661/1000 [26:51<14:11,  2.51s/it]INFO:root:global_step: 661, logpy: 9.149, kl: 30.171, loss: 20.478\n",
      " 66%|██████▌   | 662/1000 [26:53<14:11,  2.52s/it]INFO:root:global_step: 662, logpy: 9.161, kl: 30.141, loss: 20.441\n",
      " 66%|██████▋   | 663/1000 [26:56<14:17,  2.54s/it]INFO:root:global_step: 663, logpy: 9.178, kl: 30.101, loss: 20.390\n",
      " 66%|██████▋   | 664/1000 [26:58<14:16,  2.55s/it]INFO:root:global_step: 664, logpy: 9.238, kl: 30.058, loss: 20.292\n",
      " 66%|██████▋   | 665/1000 [27:01<14:16,  2.56s/it]INFO:root:global_step: 665, logpy: 9.302, kl: 30.031, loss: 20.207\n",
      " 67%|██████▋   | 666/1000 [27:04<14:49,  2.66s/it]INFO:root:global_step: 666, logpy: 9.398, kl: 29.995, loss: 20.079\n",
      " 67%|██████▋   | 667/1000 [27:06<14:36,  2.63s/it]INFO:root:global_step: 667, logpy: 9.489, kl: 29.960, loss: 19.959\n",
      " 67%|██████▋   | 668/1000 [27:09<14:34,  2.64s/it]INFO:root:global_step: 668, logpy: 9.578, kl: 29.938, loss: 19.853\n",
      " 67%|██████▋   | 669/1000 [27:12<14:31,  2.63s/it]INFO:root:global_step: 669, logpy: 9.666, kl: 29.912, loss: 19.744\n",
      " 67%|██████▋   | 670/1000 [27:14<14:06,  2.57s/it]INFO:root:global_step: 670, logpy: 9.744, kl: 29.910, loss: 19.669\n",
      " 67%|██████▋   | 671/1000 [27:17<14:03,  2.56s/it]INFO:root:global_step: 671, logpy: 9.826, kl: 29.880, loss: 19.562\n",
      " 67%|██████▋   | 672/1000 [27:19<13:49,  2.53s/it]INFO:root:global_step: 672, logpy: 9.861, kl: 29.854, loss: 19.505\n",
      " 67%|██████▋   | 673/1000 [27:22<13:35,  2.49s/it]INFO:root:global_step: 673, logpy: 9.885, kl: 29.810, loss: 19.443\n",
      " 67%|██████▋   | 674/1000 [27:24<13:29,  2.48s/it]INFO:root:global_step: 674, logpy: 9.901, kl: 29.794, loss: 19.415\n",
      " 68%|██████▊   | 675/1000 [27:26<13:24,  2.47s/it]INFO:root:global_step: 675, logpy: 9.968, kl: 29.754, loss: 19.314\n",
      " 68%|██████▊   | 676/1000 [27:29<13:29,  2.50s/it]INFO:root:global_step: 676, logpy: 10.056, kl: 29.713, loss: 19.190\n",
      " 68%|██████▊   | 677/1000 [27:31<13:25,  2.49s/it]INFO:root:global_step: 677, logpy: 10.141, kl: 29.687, loss: 19.082\n",
      " 68%|██████▊   | 678/1000 [27:34<13:17,  2.48s/it]INFO:root:global_step: 678, logpy: 10.234, kl: 29.662, loss: 18.969\n",
      " 68%|██████▊   | 679/1000 [27:36<13:19,  2.49s/it]INFO:root:global_step: 679, logpy: 10.331, kl: 29.632, loss: 18.847\n",
      " 68%|██████▊   | 680/1000 [27:39<13:26,  2.52s/it]INFO:root:global_step: 680, logpy: 10.405, kl: 29.605, loss: 18.750\n",
      " 68%|██████▊   | 681/1000 [27:42<13:26,  2.53s/it]INFO:root:global_step: 681, logpy: 10.467, kl: 29.577, loss: 18.665\n",
      " 68%|██████▊   | 682/1000 [27:44<13:34,  2.56s/it]INFO:root:global_step: 682, logpy: 10.529, kl: 29.526, loss: 18.556\n",
      " 68%|██████▊   | 683/1000 [27:47<13:51,  2.62s/it]INFO:root:global_step: 683, logpy: 10.577, kl: 29.504, loss: 18.491\n",
      " 68%|██████▊   | 684/1000 [27:50<13:55,  2.64s/it]INFO:root:global_step: 684, logpy: 10.619, kl: 29.489, loss: 18.438\n",
      " 68%|██████▊   | 685/1000 [27:52<13:37,  2.60s/it]INFO:root:global_step: 685, logpy: 10.679, kl: 29.452, loss: 18.345\n",
      " 69%|██████▊   | 686/1000 [27:55<13:31,  2.58s/it]INFO:root:global_step: 686, logpy: 10.751, kl: 29.422, loss: 18.248\n",
      " 69%|██████▊   | 687/1000 [27:57<13:18,  2.55s/it]INFO:root:global_step: 687, logpy: 10.830, kl: 29.391, loss: 18.142\n",
      " 69%|██████▉   | 688/1000 [28:00<13:07,  2.52s/it]INFO:root:global_step: 688, logpy: 10.911, kl: 29.366, loss: 18.041\n",
      " 69%|██████▉   | 689/1000 [28:02<13:05,  2.52s/it]INFO:root:global_step: 689, logpy: 10.992, kl: 29.335, loss: 17.932\n",
      " 69%|██████▉   | 690/1000 [28:05<13:37,  2.64s/it]INFO:root:global_step: 690, logpy: 11.074, kl: 29.306, loss: 17.825\n",
      " 69%|██████▉   | 691/1000 [28:08<13:31,  2.63s/it]INFO:root:global_step: 691, logpy: 11.156, kl: 29.273, loss: 17.715\n",
      " 69%|██████▉   | 692/1000 [28:10<13:26,  2.62s/it]INFO:root:global_step: 692, logpy: 11.203, kl: 29.256, loss: 17.655\n",
      " 69%|██████▉   | 693/1000 [28:13<13:29,  2.64s/it]INFO:root:global_step: 693, logpy: 11.259, kl: 29.229, loss: 17.576\n",
      " 69%|██████▉   | 694/1000 [28:16<13:30,  2.65s/it]INFO:root:global_step: 694, logpy: 11.272, kl: 29.199, loss: 17.536\n",
      " 70%|██████▉   | 695/1000 [28:18<13:25,  2.64s/it]INFO:root:global_step: 695, logpy: 11.273, kl: 29.176, loss: 17.517\n",
      " 70%|██████▉   | 696/1000 [28:22<14:54,  2.94s/it]INFO:root:global_step: 696, logpy: 11.306, kl: 29.153, loss: 17.464\n",
      " 70%|██████▉   | 697/1000 [28:25<14:45,  2.92s/it]INFO:root:global_step: 697, logpy: 11.369, kl: 29.126, loss: 17.378\n",
      " 70%|██████▉   | 698/1000 [28:27<14:15,  2.83s/it]INFO:root:global_step: 698, logpy: 11.442, kl: 29.101, loss: 17.284\n",
      " 70%|██████▉   | 699/1000 [28:30<13:44,  2.74s/it]INFO:root:global_step: 699, logpy: 11.520, kl: 29.070, loss: 17.178\n",
      " 70%|███████   | 700/1000 [28:32<13:17,  2.66s/it]INFO:root:Saved figure at: ./sim/global_step_700.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 700, logpy: 11.594, kl: 29.047, loss: 17.085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 701/1000 [28:36<14:45,  2.96s/it]INFO:root:global_step: 701, logpy: 11.658, kl: 29.022, loss: 17.000\n",
      " 70%|███████   | 702/1000 [28:39<14:08,  2.85s/it]INFO:root:global_step: 702, logpy: 11.727, kl: 28.995, loss: 16.907\n",
      " 70%|███████   | 703/1000 [28:41<13:34,  2.74s/it]INFO:root:global_step: 703, logpy: 11.768, kl: 28.969, loss: 16.844\n",
      " 70%|███████   | 704/1000 [28:44<13:10,  2.67s/it]INFO:root:global_step: 704, logpy: 11.812, kl: 28.940, loss: 16.775\n",
      " 70%|███████   | 705/1000 [28:46<12:56,  2.63s/it]INFO:root:global_step: 705, logpy: 11.869, kl: 28.911, loss: 16.692\n",
      " 71%|███████   | 706/1000 [28:49<12:39,  2.58s/it]INFO:root:global_step: 706, logpy: 11.943, kl: 28.880, loss: 16.591\n",
      " 71%|███████   | 707/1000 [28:51<12:24,  2.54s/it]INFO:root:global_step: 707, logpy: 12.023, kl: 28.857, loss: 16.491\n",
      " 71%|███████   | 708/1000 [28:54<12:14,  2.52s/it]INFO:root:global_step: 708, logpy: 12.085, kl: 28.837, loss: 16.412\n",
      " 71%|███████   | 709/1000 [28:56<12:10,  2.51s/it]INFO:root:global_step: 709, logpy: 12.163, kl: 28.801, loss: 16.302\n",
      " 71%|███████   | 710/1000 [28:59<12:08,  2.51s/it]INFO:root:global_step: 710, logpy: 12.237, kl: 28.771, loss: 16.201\n",
      " 71%|███████   | 711/1000 [29:01<12:07,  2.52s/it]INFO:root:global_step: 711, logpy: 12.305, kl: 28.742, loss: 16.108\n",
      " 71%|███████   | 712/1000 [29:04<12:11,  2.54s/it]INFO:root:global_step: 712, logpy: 12.360, kl: 28.725, loss: 16.039\n",
      " 71%|███████▏  | 713/1000 [29:06<11:59,  2.51s/it]INFO:root:global_step: 713, logpy: 12.436, kl: 28.676, loss: 15.917\n",
      " 71%|███████▏  | 714/1000 [29:09<11:59,  2.52s/it]INFO:root:global_step: 714, logpy: 12.501, kl: 28.645, loss: 15.825\n",
      " 72%|███████▏  | 715/1000 [29:11<12:24,  2.61s/it]INFO:root:global_step: 715, logpy: 12.563, kl: 28.641, loss: 15.762\n",
      " 72%|███████▏  | 716/1000 [29:15<12:59,  2.74s/it]INFO:root:global_step: 716, logpy: 12.642, kl: 28.604, loss: 15.649\n",
      " 72%|███████▏  | 717/1000 [29:17<12:54,  2.74s/it]INFO:root:global_step: 717, logpy: 12.712, kl: 28.582, loss: 15.560\n",
      " 72%|███████▏  | 718/1000 [29:20<12:51,  2.73s/it]INFO:root:global_step: 718, logpy: 12.779, kl: 28.545, loss: 15.458\n",
      " 72%|███████▏  | 719/1000 [29:23<12:49,  2.74s/it]INFO:root:global_step: 719, logpy: 12.820, kl: 28.511, loss: 15.386\n",
      " 72%|███████▏  | 720/1000 [29:26<12:58,  2.78s/it]INFO:root:global_step: 720, logpy: 12.845, kl: 28.491, loss: 15.346\n",
      " 72%|███████▏  | 721/1000 [29:28<12:31,  2.69s/it]INFO:root:global_step: 721, logpy: 12.855, kl: 28.468, loss: 15.316\n",
      " 72%|███████▏  | 722/1000 [29:31<12:35,  2.72s/it]INFO:root:global_step: 722, logpy: 12.863, kl: 28.443, loss: 15.286\n",
      " 72%|███████▏  | 723/1000 [29:34<12:38,  2.74s/it]INFO:root:global_step: 723, logpy: 12.881, kl: 28.415, loss: 15.242\n",
      " 72%|███████▏  | 724/1000 [29:36<12:16,  2.67s/it]INFO:root:global_step: 724, logpy: 12.932, kl: 28.381, loss: 15.161\n",
      " 72%|███████▎  | 725/1000 [29:39<12:02,  2.63s/it]INFO:root:global_step: 725, logpy: 12.988, kl: 28.349, loss: 15.075\n",
      " 73%|███████▎  | 726/1000 [29:41<11:57,  2.62s/it]INFO:root:global_step: 726, logpy: 13.059, kl: 28.324, loss: 14.981\n",
      " 73%|███████▎  | 727/1000 [29:44<11:46,  2.59s/it]INFO:root:global_step: 727, logpy: 13.126, kl: 28.293, loss: 14.886\n",
      " 73%|███████▎  | 728/1000 [29:46<11:43,  2.59s/it]INFO:root:global_step: 728, logpy: 13.189, kl: 28.270, loss: 14.803\n",
      " 73%|███████▎  | 729/1000 [29:50<12:38,  2.80s/it]INFO:root:global_step: 729, logpy: 13.251, kl: 28.243, loss: 14.717\n",
      " 73%|███████▎  | 730/1000 [29:52<12:12,  2.71s/it]INFO:root:global_step: 730, logpy: 13.302, kl: 28.219, loss: 14.645\n",
      " 73%|███████▎  | 731/1000 [29:55<12:22,  2.76s/it]INFO:root:global_step: 731, logpy: 13.346, kl: 28.191, loss: 14.575\n",
      " 73%|███████▎  | 732/1000 [29:58<12:13,  2.74s/it]INFO:root:global_step: 732, logpy: 13.376, kl: 28.178, loss: 14.535\n",
      " 73%|███████▎  | 733/1000 [30:00<11:57,  2.69s/it]INFO:root:global_step: 733, logpy: 13.416, kl: 28.142, loss: 14.463\n",
      " 73%|███████▎  | 734/1000 [30:03<11:57,  2.70s/it]INFO:root:global_step: 734, logpy: 13.472, kl: 28.111, loss: 14.379\n",
      " 74%|███████▎  | 735/1000 [30:06<11:54,  2.69s/it]INFO:root:global_step: 735, logpy: 13.507, kl: 28.107, loss: 14.341\n",
      " 74%|███████▎  | 736/1000 [30:08<11:36,  2.64s/it]INFO:root:global_step: 736, logpy: 13.565, kl: 28.080, loss: 14.259\n",
      " 74%|███████▎  | 737/1000 [30:11<11:24,  2.60s/it]INFO:root:global_step: 737, logpy: 13.637, kl: 28.053, loss: 14.163\n",
      " 74%|███████▍  | 738/1000 [30:13<11:17,  2.58s/it]INFO:root:global_step: 738, logpy: 13.699, kl: 28.021, loss: 14.071\n",
      " 74%|███████▍  | 739/1000 [30:16<11:23,  2.62s/it]INFO:root:global_step: 739, logpy: 13.747, kl: 28.010, loss: 14.014\n",
      " 74%|███████▍  | 740/1000 [30:19<11:31,  2.66s/it]INFO:root:global_step: 740, logpy: 13.793, kl: 27.982, loss: 13.943\n",
      " 74%|███████▍  | 741/1000 [30:21<11:28,  2.66s/it]INFO:root:global_step: 741, logpy: 13.846, kl: 27.972, loss: 13.882\n",
      " 74%|███████▍  | 742/1000 [30:24<11:08,  2.59s/it]INFO:root:global_step: 742, logpy: 13.889, kl: 27.951, loss: 13.821\n",
      " 74%|███████▍  | 743/1000 [30:26<11:04,  2.58s/it]INFO:root:global_step: 743, logpy: 13.940, kl: 27.933, loss: 13.753\n",
      " 74%|███████▍  | 744/1000 [30:30<12:00,  2.81s/it]INFO:root:global_step: 744, logpy: 13.975, kl: 27.916, loss: 13.704\n",
      " 74%|███████▍  | 745/1000 [30:33<11:58,  2.82s/it]INFO:root:global_step: 745, logpy: 14.000, kl: 27.894, loss: 13.660\n",
      " 75%|███████▍  | 746/1000 [30:35<11:47,  2.79s/it]INFO:root:global_step: 746, logpy: 14.009, kl: 27.864, loss: 13.623\n",
      " 75%|███████▍  | 747/1000 [30:38<11:25,  2.71s/it]INFO:root:global_step: 747, logpy: 14.045, kl: 27.829, loss: 13.555\n",
      " 75%|███████▍  | 748/1000 [30:40<11:03,  2.63s/it]INFO:root:global_step: 748, logpy: 14.065, kl: 27.808, loss: 13.517\n",
      " 75%|███████▍  | 749/1000 [30:43<10:51,  2.60s/it]INFO:root:global_step: 749, logpy: 14.103, kl: 27.771, loss: 13.443\n",
      " 75%|███████▌  | 750/1000 [30:45<10:37,  2.55s/it]INFO:root:Saved figure at: ./sim/global_step_750.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 750, logpy: 14.139, kl: 27.743, loss: 13.381\n",
      " 75%|███████▌  | 751/1000 [30:49<11:57,  2.88s/it]INFO:root:global_step: 751, logpy: 14.167, kl: 27.726, loss: 13.339\n",
      " 75%|███████▌  | 752/1000 [30:52<11:36,  2.81s/it]INFO:root:global_step: 752, logpy: 14.180, kl: 27.713, loss: 13.316\n",
      " 75%|███████▌  | 753/1000 [30:54<11:11,  2.72s/it]INFO:root:global_step: 753, logpy: 14.198, kl: 27.687, loss: 13.274\n",
      " 75%|███████▌  | 754/1000 [30:57<11:02,  2.69s/it]INFO:root:global_step: 754, logpy: 14.237, kl: 27.654, loss: 13.203\n",
      " 76%|███████▌  | 755/1000 [30:59<10:45,  2.64s/it]INFO:root:global_step: 755, logpy: 14.291, kl: 27.632, loss: 13.129\n",
      " 76%|███████▌  | 756/1000 [31:02<10:27,  2.57s/it]INFO:root:global_step: 756, logpy: 14.344, kl: 27.621, loss: 13.067\n",
      " 76%|███████▌  | 757/1000 [31:04<10:11,  2.52s/it]INFO:root:global_step: 757, logpy: 14.399, kl: 27.616, loss: 13.009\n",
      " 76%|███████▌  | 758/1000 [31:06<10:04,  2.50s/it]INFO:root:global_step: 758, logpy: 14.451, kl: 27.600, loss: 12.943\n",
      " 76%|███████▌  | 759/1000 [31:09<09:53,  2.46s/it]INFO:root:global_step: 759, logpy: 14.482, kl: 27.588, loss: 12.902\n",
      " 76%|███████▌  | 760/1000 [31:11<09:50,  2.46s/it]INFO:root:global_step: 760, logpy: 14.517, kl: 27.554, loss: 12.836\n",
      " 76%|███████▌  | 761/1000 [31:14<09:47,  2.46s/it]INFO:root:global_step: 761, logpy: 14.517, kl: 27.537, loss: 12.821\n",
      " 76%|███████▌  | 762/1000 [31:16<09:41,  2.44s/it]INFO:root:global_step: 762, logpy: 14.547, kl: 27.508, loss: 12.764\n",
      " 76%|███████▋  | 763/1000 [31:19<09:39,  2.44s/it]INFO:root:global_step: 763, logpy: 14.583, kl: 27.484, loss: 12.706\n",
      " 76%|███████▋  | 764/1000 [31:21<09:39,  2.45s/it]INFO:root:global_step: 764, logpy: 14.655, kl: 27.461, loss: 12.613\n",
      " 76%|███████▋  | 765/1000 [31:23<09:35,  2.45s/it]INFO:root:global_step: 765, logpy: 14.726, kl: 27.431, loss: 12.513\n",
      " 77%|███████▋  | 766/1000 [31:26<09:40,  2.48s/it]INFO:root:global_step: 766, logpy: 14.787, kl: 27.405, loss: 12.428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 767/1000 [31:28<09:33,  2.46s/it]INFO:root:global_step: 767, logpy: 14.814, kl: 27.370, loss: 12.369\n",
      " 77%|███████▋  | 768/1000 [31:31<09:21,  2.42s/it]INFO:root:global_step: 768, logpy: 14.796, kl: 27.352, loss: 12.370\n",
      " 77%|███████▋  | 769/1000 [31:34<09:45,  2.53s/it]INFO:root:global_step: 769, logpy: 14.790, kl: 27.336, loss: 12.363\n",
      " 77%|███████▋  | 770/1000 [31:36<09:32,  2.49s/it]INFO:root:global_step: 770, logpy: 14.837, kl: 27.304, loss: 12.285\n",
      " 77%|███████▋  | 771/1000 [31:38<09:23,  2.46s/it]INFO:root:global_step: 771, logpy: 14.891, kl: 27.288, loss: 12.217\n",
      " 77%|███████▋  | 772/1000 [31:41<09:15,  2.44s/it]INFO:root:global_step: 772, logpy: 14.945, kl: 27.265, loss: 12.142\n",
      " 77%|███████▋  | 773/1000 [31:43<09:13,  2.44s/it]INFO:root:global_step: 773, logpy: 14.982, kl: 27.234, loss: 12.076\n",
      " 77%|███████▋  | 774/1000 [31:46<09:10,  2.44s/it]INFO:root:global_step: 774, logpy: 14.992, kl: 27.216, loss: 12.049\n",
      " 78%|███████▊  | 775/1000 [31:48<09:02,  2.41s/it]INFO:root:global_step: 775, logpy: 14.980, kl: 27.207, loss: 12.053\n",
      " 78%|███████▊  | 776/1000 [31:51<09:30,  2.55s/it]INFO:root:global_step: 776, logpy: 15.006, kl: 27.179, loss: 12.002\n",
      " 78%|███████▊  | 777/1000 [31:53<09:24,  2.53s/it]INFO:root:global_step: 777, logpy: 15.042, kl: 27.154, loss: 11.943\n",
      " 78%|███████▊  | 778/1000 [31:56<09:19,  2.52s/it]INFO:root:global_step: 778, logpy: 15.099, kl: 27.135, loss: 11.868\n",
      " 78%|███████▊  | 779/1000 [31:58<09:09,  2.48s/it]INFO:root:global_step: 779, logpy: 15.156, kl: 27.103, loss: 11.781\n",
      " 78%|███████▊  | 780/1000 [32:01<09:04,  2.48s/it]INFO:root:global_step: 780, logpy: 15.219, kl: 27.091, loss: 11.707\n",
      " 78%|███████▊  | 781/1000 [32:03<09:07,  2.50s/it]INFO:root:global_step: 781, logpy: 15.264, kl: 27.094, loss: 11.668\n",
      " 78%|███████▊  | 782/1000 [32:06<09:05,  2.50s/it]INFO:root:global_step: 782, logpy: 15.307, kl: 27.084, loss: 11.616\n",
      " 78%|███████▊  | 783/1000 [32:08<09:00,  2.49s/it]INFO:root:global_step: 783, logpy: 15.328, kl: 27.067, loss: 11.580\n",
      " 78%|███████▊  | 784/1000 [32:11<09:05,  2.52s/it]INFO:root:global_step: 784, logpy: 15.341, kl: 27.055, loss: 11.556\n",
      " 78%|███████▊  | 785/1000 [32:13<09:05,  2.54s/it]INFO:root:global_step: 785, logpy: 15.362, kl: 27.046, loss: 11.528\n",
      " 79%|███████▊  | 786/1000 [32:16<08:50,  2.48s/it]INFO:root:global_step: 786, logpy: 15.411, kl: 27.032, loss: 11.466\n",
      " 79%|███████▊  | 787/1000 [32:18<08:48,  2.48s/it]INFO:root:global_step: 787, logpy: 15.458, kl: 27.019, loss: 11.407\n",
      " 79%|███████▉  | 788/1000 [32:21<08:55,  2.53s/it]INFO:root:global_step: 788, logpy: 15.534, kl: 26.986, loss: 11.300\n",
      " 79%|███████▉  | 789/1000 [32:23<09:00,  2.56s/it]INFO:root:global_step: 789, logpy: 15.601, kl: 26.967, loss: 11.216\n",
      " 79%|███████▉  | 790/1000 [32:26<08:54,  2.55s/it]INFO:root:global_step: 790, logpy: 15.660, kl: 26.963, loss: 11.154\n",
      " 79%|███████▉  | 791/1000 [32:29<09:17,  2.67s/it]INFO:root:global_step: 791, logpy: 15.726, kl: 26.946, loss: 11.072\n",
      " 79%|███████▉  | 792/1000 [32:31<08:56,  2.58s/it]INFO:root:global_step: 792, logpy: 15.775, kl: 26.921, loss: 11.000\n",
      " 79%|███████▉  | 793/1000 [32:34<08:46,  2.55s/it]INFO:root:global_step: 793, logpy: 15.780, kl: 26.900, loss: 10.976\n",
      " 79%|███████▉  | 794/1000 [32:36<08:34,  2.50s/it]INFO:root:global_step: 794, logpy: 15.780, kl: 26.881, loss: 10.957\n",
      " 80%|███████▉  | 795/1000 [32:39<08:35,  2.52s/it]INFO:root:global_step: 795, logpy: 15.767, kl: 26.863, loss: 10.954\n",
      " 80%|███████▉  | 796/1000 [32:41<08:42,  2.56s/it]INFO:root:global_step: 796, logpy: 15.794, kl: 26.841, loss: 10.907\n",
      " 80%|███████▉  | 797/1000 [32:44<08:39,  2.56s/it]INFO:root:global_step: 797, logpy: 15.850, kl: 26.828, loss: 10.839\n",
      " 80%|███████▉  | 798/1000 [32:46<08:30,  2.53s/it]INFO:root:global_step: 798, logpy: 15.908, kl: 26.811, loss: 10.765\n",
      " 80%|███████▉  | 799/1000 [32:49<08:26,  2.52s/it]INFO:root:global_step: 799, logpy: 15.963, kl: 26.786, loss: 10.686\n",
      " 80%|████████  | 800/1000 [32:51<08:22,  2.51s/it]INFO:root:Saved figure at: ./sim/global_step_800.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 800, logpy: 15.964, kl: 26.757, loss: 10.659\n",
      " 80%|████████  | 801/1000 [32:55<09:38,  2.91s/it]INFO:root:global_step: 801, logpy: 15.934, kl: 26.749, loss: 10.682\n",
      " 80%|████████  | 802/1000 [32:58<09:32,  2.89s/it]INFO:root:global_step: 802, logpy: 15.913, kl: 26.734, loss: 10.689\n",
      " 80%|████████  | 803/1000 [33:01<09:47,  2.98s/it]INFO:root:global_step: 803, logpy: 15.949, kl: 26.717, loss: 10.638\n",
      " 80%|████████  | 804/1000 [33:06<11:02,  3.38s/it]INFO:root:global_step: 804, logpy: 16.005, kl: 26.707, loss: 10.573\n",
      " 80%|████████  | 805/1000 [33:09<10:38,  3.28s/it]INFO:root:global_step: 805, logpy: 16.065, kl: 26.679, loss: 10.486\n",
      " 81%|████████  | 806/1000 [33:12<10:29,  3.24s/it]INFO:root:global_step: 806, logpy: 16.119, kl: 26.664, loss: 10.418\n",
      " 81%|████████  | 807/1000 [33:15<10:05,  3.14s/it]INFO:root:global_step: 807, logpy: 16.141, kl: 26.633, loss: 10.367\n",
      " 81%|████████  | 808/1000 [33:17<09:44,  3.05s/it]INFO:root:global_step: 808, logpy: 16.155, kl: 26.615, loss: 10.335\n",
      " 81%|████████  | 809/1000 [33:20<09:23,  2.95s/it]INFO:root:global_step: 809, logpy: 16.173, kl: 26.605, loss: 10.309\n",
      " 81%|████████  | 810/1000 [33:23<09:32,  3.01s/it]INFO:root:global_step: 810, logpy: 16.219, kl: 26.580, loss: 10.240\n",
      " 81%|████████  | 811/1000 [33:26<09:25,  2.99s/it]INFO:root:global_step: 811, logpy: 16.286, kl: 26.554, loss: 10.148\n",
      " 81%|████████  | 812/1000 [33:29<09:13,  2.95s/it]INFO:root:global_step: 812, logpy: 16.354, kl: 26.533, loss: 10.060\n",
      " 81%|████████▏ | 813/1000 [33:33<09:57,  3.19s/it]INFO:root:global_step: 813, logpy: 16.423, kl: 26.507, loss: 9.966\n",
      " 81%|████████▏ | 814/1000 [33:35<09:13,  2.98s/it]INFO:root:global_step: 814, logpy: 16.472, kl: 26.493, loss: 9.904\n",
      " 82%|████████▏ | 815/1000 [33:38<08:40,  2.81s/it]INFO:root:global_step: 815, logpy: 16.526, kl: 26.466, loss: 9.824\n",
      " 82%|████████▏ | 816/1000 [33:40<08:19,  2.71s/it]INFO:root:global_step: 816, logpy: 16.576, kl: 26.436, loss: 9.745\n",
      " 82%|████████▏ | 817/1000 [33:43<08:03,  2.64s/it]INFO:root:global_step: 817, logpy: 16.617, kl: 26.419, loss: 9.688\n",
      " 82%|████████▏ | 818/1000 [33:45<07:55,  2.61s/it]INFO:root:global_step: 818, logpy: 16.653, kl: 26.427, loss: 9.662\n",
      " 82%|████████▏ | 819/1000 [33:48<07:43,  2.56s/it]INFO:root:global_step: 819, logpy: 16.697, kl: 26.418, loss: 9.610\n",
      " 82%|████████▏ | 820/1000 [33:50<07:32,  2.51s/it]INFO:root:global_step: 820, logpy: 16.720, kl: 26.403, loss: 9.573\n",
      " 82%|████████▏ | 821/1000 [33:53<07:25,  2.49s/it]INFO:root:global_step: 821, logpy: 16.757, kl: 26.395, loss: 9.529\n",
      " 82%|████████▏ | 822/1000 [33:55<07:29,  2.53s/it]INFO:root:global_step: 822, logpy: 16.814, kl: 26.381, loss: 9.459\n",
      " 82%|████████▏ | 823/1000 [33:58<07:25,  2.52s/it]INFO:root:global_step: 823, logpy: 16.874, kl: 26.362, loss: 9.381\n",
      " 82%|████████▏ | 824/1000 [34:00<07:22,  2.51s/it]INFO:root:global_step: 824, logpy: 16.933, kl: 26.341, loss: 9.302\n",
      " 82%|████████▎ | 825/1000 [34:03<07:18,  2.50s/it]INFO:root:global_step: 825, logpy: 16.995, kl: 26.322, loss: 9.222\n",
      " 83%|████████▎ | 826/1000 [34:05<07:20,  2.53s/it]INFO:root:global_step: 826, logpy: 17.035, kl: 26.309, loss: 9.170\n",
      " 83%|████████▎ | 827/1000 [34:08<07:12,  2.50s/it]INFO:root:global_step: 827, logpy: 17.079, kl: 26.292, loss: 9.110\n",
      " 83%|████████▎ | 828/1000 [34:11<07:27,  2.60s/it]INFO:root:global_step: 828, logpy: 17.101, kl: 26.281, loss: 9.078\n",
      " 83%|████████▎ | 829/1000 [34:13<07:29,  2.63s/it]INFO:root:global_step: 829, logpy: 17.131, kl: 26.279, loss: 9.047\n",
      " 83%|████████▎ | 830/1000 [34:16<07:20,  2.59s/it]INFO:root:global_step: 830, logpy: 17.148, kl: 26.272, loss: 9.024\n",
      " 83%|████████▎ | 831/1000 [34:18<07:13,  2.56s/it]INFO:root:global_step: 831, logpy: 17.187, kl: 26.254, loss: 8.969\n",
      " 83%|████████▎ | 832/1000 [34:21<07:03,  2.52s/it]INFO:root:global_step: 832, logpy: 17.224, kl: 26.244, loss: 8.923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 833/1000 [34:23<06:56,  2.49s/it]INFO:root:global_step: 833, logpy: 17.274, kl: 26.208, loss: 8.838\n",
      " 83%|████████▎ | 834/1000 [34:26<06:53,  2.49s/it]INFO:root:global_step: 834, logpy: 17.309, kl: 26.190, loss: 8.785\n",
      " 84%|████████▎ | 835/1000 [34:28<06:53,  2.51s/it]INFO:root:global_step: 835, logpy: 17.355, kl: 26.172, loss: 8.722\n",
      " 84%|████████▎ | 836/1000 [34:31<06:54,  2.53s/it]INFO:root:global_step: 836, logpy: 17.411, kl: 26.148, loss: 8.643\n",
      " 84%|████████▎ | 837/1000 [34:33<06:50,  2.52s/it]INFO:root:global_step: 837, logpy: 17.451, kl: 26.141, loss: 8.597\n",
      " 84%|████████▍ | 838/1000 [34:36<06:46,  2.51s/it]INFO:root:global_step: 838, logpy: 17.510, kl: 26.129, loss: 8.527\n",
      " 84%|████████▍ | 839/1000 [34:38<06:44,  2.51s/it]INFO:root:global_step: 839, logpy: 17.574, kl: 26.114, loss: 8.449\n",
      " 84%|████████▍ | 840/1000 [34:41<06:41,  2.51s/it]INFO:root:global_step: 840, logpy: 17.636, kl: 26.093, loss: 8.368\n",
      " 84%|████████▍ | 841/1000 [34:43<06:43,  2.54s/it]INFO:root:global_step: 841, logpy: 17.699, kl: 26.071, loss: 8.283\n",
      " 84%|████████▍ | 842/1000 [34:46<06:43,  2.55s/it]INFO:root:global_step: 842, logpy: 17.741, kl: 26.049, loss: 8.220\n",
      " 84%|████████▍ | 843/1000 [34:49<06:47,  2.60s/it]INFO:root:global_step: 843, logpy: 17.739, kl: 26.038, loss: 8.212\n",
      " 84%|████████▍ | 844/1000 [34:51<06:41,  2.57s/it]INFO:root:global_step: 844, logpy: 17.730, kl: 26.018, loss: 8.202\n",
      " 84%|████████▍ | 845/1000 [34:54<06:34,  2.54s/it]INFO:root:global_step: 845, logpy: 17.730, kl: 25.990, loss: 8.175\n",
      " 85%|████████▍ | 846/1000 [34:56<06:33,  2.56s/it]INFO:root:global_step: 846, logpy: 17.765, kl: 25.971, loss: 8.121\n",
      " 85%|████████▍ | 847/1000 [34:59<06:31,  2.56s/it]INFO:root:global_step: 847, logpy: 17.818, kl: 25.950, loss: 8.048\n",
      " 85%|████████▍ | 848/1000 [35:01<06:26,  2.54s/it]INFO:root:global_step: 848, logpy: 17.879, kl: 25.949, loss: 7.987\n",
      " 85%|████████▍ | 849/1000 [35:04<06:20,  2.52s/it]INFO:root:global_step: 849, logpy: 17.951, kl: 25.924, loss: 7.891\n",
      " 85%|████████▌ | 850/1000 [35:06<06:17,  2.51s/it]INFO:root:Saved figure at: ./sim/global_step_850.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 850, logpy: 18.020, kl: 25.913, loss: 7.812\n",
      " 85%|████████▌ | 851/1000 [35:10<07:11,  2.89s/it]INFO:root:global_step: 851, logpy: 18.076, kl: 25.896, loss: 7.739\n",
      " 85%|████████▌ | 852/1000 [35:13<06:51,  2.78s/it]INFO:root:global_step: 852, logpy: 18.118, kl: 25.899, loss: 7.701\n",
      " 85%|████████▌ | 853/1000 [35:15<06:36,  2.70s/it]INFO:root:global_step: 853, logpy: 18.130, kl: 25.883, loss: 7.673\n",
      " 85%|████████▌ | 854/1000 [35:18<06:25,  2.64s/it]INFO:root:global_step: 854, logpy: 18.121, kl: 25.874, loss: 7.674\n",
      " 86%|████████▌ | 855/1000 [35:20<06:14,  2.59s/it]INFO:root:global_step: 855, logpy: 18.137, kl: 25.863, loss: 7.648\n",
      " 86%|████████▌ | 856/1000 [35:23<06:12,  2.59s/it]INFO:root:global_step: 856, logpy: 18.182, kl: 25.852, loss: 7.594\n",
      " 86%|████████▌ | 857/1000 [35:25<06:21,  2.67s/it]INFO:root:global_step: 857, logpy: 18.241, kl: 25.854, loss: 7.537\n",
      " 86%|████████▌ | 858/1000 [35:28<06:10,  2.61s/it]INFO:root:global_step: 858, logpy: 18.301, kl: 25.843, loss: 7.467\n",
      " 86%|████████▌ | 859/1000 [35:30<06:06,  2.60s/it]INFO:root:global_step: 859, logpy: 18.366, kl: 25.823, loss: 7.383\n",
      " 86%|████████▌ | 860/1000 [35:33<06:02,  2.59s/it]INFO:root:global_step: 860, logpy: 18.423, kl: 25.805, loss: 7.308\n",
      " 86%|████████▌ | 861/1000 [35:36<05:54,  2.55s/it]INFO:root:global_step: 861, logpy: 18.448, kl: 25.800, loss: 7.279\n",
      " 86%|████████▌ | 862/1000 [35:38<05:50,  2.54s/it]INFO:root:global_step: 862, logpy: 18.461, kl: 25.781, loss: 7.248\n",
      " 86%|████████▋ | 863/1000 [35:41<05:45,  2.53s/it]INFO:root:global_step: 863, logpy: 18.440, kl: 25.784, loss: 7.272\n",
      " 86%|████████▋ | 864/1000 [35:43<05:39,  2.50s/it]INFO:root:global_step: 864, logpy: 18.445, kl: 25.773, loss: 7.258\n",
      " 86%|████████▋ | 865/1000 [35:45<05:35,  2.48s/it]INFO:root:global_step: 865, logpy: 18.489, kl: 25.766, loss: 7.207\n",
      " 87%|████████▋ | 866/1000 [35:48<05:31,  2.47s/it]INFO:root:global_step: 866, logpy: 18.553, kl: 25.754, loss: 7.132\n",
      " 87%|████████▋ | 867/1000 [35:50<05:26,  2.45s/it]INFO:root:global_step: 867, logpy: 18.611, kl: 25.745, loss: 7.065\n",
      " 87%|████████▋ | 868/1000 [35:53<05:22,  2.45s/it]INFO:root:global_step: 868, logpy: 18.654, kl: 25.731, loss: 7.009\n",
      " 87%|████████▋ | 869/1000 [35:55<05:23,  2.47s/it]INFO:root:global_step: 869, logpy: 18.683, kl: 25.719, loss: 6.969\n",
      " 87%|████████▋ | 870/1000 [35:58<05:18,  2.45s/it]INFO:root:global_step: 870, logpy: 18.675, kl: 25.707, loss: 6.965\n",
      " 87%|████████▋ | 871/1000 [36:00<05:15,  2.45s/it]INFO:root:global_step: 871, logpy: 18.696, kl: 25.689, loss: 6.927\n",
      " 87%|████████▋ | 872/1000 [36:02<05:11,  2.44s/it]INFO:root:global_step: 872, logpy: 18.707, kl: 25.688, loss: 6.916\n",
      " 87%|████████▋ | 873/1000 [36:05<05:15,  2.48s/it]INFO:root:global_step: 873, logpy: 18.749, kl: 25.679, loss: 6.865\n",
      " 87%|████████▋ | 874/1000 [36:07<05:10,  2.46s/it]INFO:root:global_step: 874, logpy: 18.819, kl: 25.654, loss: 6.771\n",
      " 88%|████████▊ | 875/1000 [36:10<05:10,  2.48s/it]INFO:root:global_step: 875, logpy: 18.893, kl: 25.639, loss: 6.683\n",
      " 88%|████████▊ | 876/1000 [36:12<05:07,  2.48s/it]INFO:root:global_step: 876, logpy: 18.964, kl: 25.627, loss: 6.601\n",
      " 88%|████████▊ | 877/1000 [36:15<05:04,  2.48s/it]INFO:root:global_step: 877, logpy: 19.030, kl: 25.602, loss: 6.510\n",
      " 88%|████████▊ | 878/1000 [36:18<05:12,  2.56s/it]INFO:root:global_step: 878, logpy: 19.089, kl: 25.582, loss: 6.432\n",
      " 88%|████████▊ | 879/1000 [36:20<05:05,  2.53s/it]INFO:root:global_step: 879, logpy: 19.143, kl: 25.574, loss: 6.370\n",
      " 88%|████████▊ | 880/1000 [36:23<05:02,  2.52s/it]INFO:root:global_step: 880, logpy: 19.213, kl: 25.568, loss: 6.295\n",
      " 88%|████████▊ | 881/1000 [36:25<04:59,  2.51s/it]INFO:root:global_step: 881, logpy: 19.283, kl: 25.548, loss: 6.205\n",
      " 88%|████████▊ | 882/1000 [36:28<04:53,  2.49s/it]INFO:root:global_step: 882, logpy: 19.340, kl: 25.542, loss: 6.143\n",
      " 88%|████████▊ | 883/1000 [36:30<04:52,  2.50s/it]INFO:root:global_step: 883, logpy: 19.397, kl: 25.532, loss: 6.077\n",
      " 88%|████████▊ | 884/1000 [36:33<04:46,  2.47s/it]INFO:root:global_step: 884, logpy: 19.457, kl: 25.510, loss: 5.995\n",
      " 88%|████████▊ | 885/1000 [36:35<04:42,  2.46s/it]INFO:root:global_step: 885, logpy: 19.511, kl: 25.503, loss: 5.935\n",
      " 89%|████████▊ | 886/1000 [36:37<04:39,  2.45s/it]INFO:root:global_step: 886, logpy: 19.514, kl: 25.504, loss: 5.933\n",
      " 89%|████████▊ | 887/1000 [36:40<04:37,  2.45s/it]INFO:root:global_step: 887, logpy: 19.513, kl: 25.493, loss: 5.924\n",
      " 89%|████████▉ | 888/1000 [36:42<04:36,  2.47s/it]INFO:root:global_step: 888, logpy: 19.518, kl: 25.487, loss: 5.913\n",
      " 89%|████████▉ | 889/1000 [36:45<04:36,  2.49s/it]INFO:root:global_step: 889, logpy: 19.556, kl: 25.471, loss: 5.860\n",
      " 89%|████████▉ | 890/1000 [36:47<04:33,  2.49s/it]INFO:root:global_step: 890, logpy: 19.606, kl: 25.469, loss: 5.809\n",
      " 89%|████████▉ | 891/1000 [36:50<04:30,  2.48s/it]INFO:root:global_step: 891, logpy: 19.681, kl: 25.460, loss: 5.724\n",
      " 89%|████████▉ | 892/1000 [36:52<04:26,  2.47s/it]INFO:root:global_step: 892, logpy: 19.749, kl: 25.436, loss: 5.634\n",
      " 89%|████████▉ | 893/1000 [36:55<04:30,  2.53s/it]INFO:root:global_step: 893, logpy: 19.768, kl: 25.440, loss: 5.619\n",
      " 89%|████████▉ | 894/1000 [36:58<04:29,  2.55s/it]INFO:root:global_step: 894, logpy: 19.792, kl: 25.423, loss: 5.579\n",
      " 90%|████████▉ | 895/1000 [37:00<04:25,  2.53s/it]INFO:root:global_step: 895, logpy: 19.811, kl: 25.416, loss: 5.553\n",
      " 90%|████████▉ | 896/1000 [37:02<04:20,  2.51s/it]INFO:root:global_step: 896, logpy: 19.867, kl: 25.398, loss: 5.479\n",
      " 90%|████████▉ | 897/1000 [37:06<04:34,  2.67s/it]INFO:root:global_step: 897, logpy: 19.927, kl: 25.380, loss: 5.402\n",
      " 90%|████████▉ | 898/1000 [37:08<04:35,  2.70s/it]INFO:root:global_step: 898, logpy: 19.996, kl: 25.376, loss: 5.330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 899/1000 [37:11<04:40,  2.77s/it]INFO:root:global_step: 899, logpy: 20.059, kl: 25.365, loss: 5.256\n",
      " 90%|█████████ | 900/1000 [37:14<04:37,  2.78s/it]INFO:root:Saved figure at: ./sim/global_step_900.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 900, logpy: 20.075, kl: 25.366, loss: 5.242\n",
      " 90%|█████████ | 901/1000 [37:18<05:06,  3.10s/it]INFO:root:global_step: 901, logpy: 20.073, kl: 25.356, loss: 5.235\n",
      " 90%|█████████ | 902/1000 [37:21<04:51,  2.98s/it]INFO:root:global_step: 902, logpy: 20.048, kl: 25.346, loss: 5.249\n",
      " 90%|█████████ | 903/1000 [37:23<04:33,  2.82s/it]INFO:root:global_step: 903, logpy: 20.078, kl: 25.341, loss: 5.215\n",
      " 90%|█████████ | 904/1000 [37:26<04:20,  2.72s/it]INFO:root:global_step: 904, logpy: 20.118, kl: 25.335, loss: 5.169\n",
      " 90%|█████████ | 905/1000 [37:28<04:09,  2.63s/it]INFO:root:global_step: 905, logpy: 20.183, kl: 25.323, loss: 5.093\n",
      " 91%|█████████ | 906/1000 [37:31<04:07,  2.63s/it]INFO:root:global_step: 906, logpy: 20.261, kl: 25.308, loss: 5.000\n",
      " 91%|█████████ | 907/1000 [37:33<03:58,  2.56s/it]INFO:root:global_step: 907, logpy: 20.325, kl: 25.290, loss: 4.919\n",
      " 91%|█████████ | 908/1000 [37:35<03:52,  2.53s/it]INFO:root:global_step: 908, logpy: 20.397, kl: 25.275, loss: 4.833\n",
      " 91%|█████████ | 909/1000 [37:38<03:51,  2.55s/it]INFO:root:global_step: 909, logpy: 20.452, kl: 25.275, loss: 4.778\n",
      " 91%|█████████ | 910/1000 [37:40<03:47,  2.53s/it]INFO:root:global_step: 910, logpy: 20.506, kl: 25.266, loss: 4.715\n",
      " 91%|█████████ | 911/1000 [37:43<03:43,  2.51s/it]INFO:root:global_step: 911, logpy: 20.535, kl: 25.254, loss: 4.675\n",
      " 91%|█████████ | 912/1000 [37:46<03:41,  2.52s/it]INFO:root:global_step: 912, logpy: 20.570, kl: 25.235, loss: 4.621\n",
      " 91%|█████████▏| 913/1000 [37:48<03:38,  2.51s/it]INFO:root:global_step: 913, logpy: 20.572, kl: 25.226, loss: 4.611\n",
      " 91%|█████████▏| 914/1000 [37:51<03:38,  2.54s/it]INFO:root:global_step: 914, logpy: 20.604, kl: 25.231, loss: 4.584\n",
      " 92%|█████████▏| 915/1000 [37:53<03:33,  2.51s/it]INFO:root:global_step: 915, logpy: 20.651, kl: 25.214, loss: 4.521\n",
      " 92%|█████████▏| 916/1000 [37:56<03:29,  2.50s/it]INFO:root:global_step: 916, logpy: 20.720, kl: 25.211, loss: 4.448\n",
      " 92%|█████████▏| 917/1000 [37:58<03:27,  2.50s/it]INFO:root:global_step: 917, logpy: 20.776, kl: 25.203, loss: 4.386\n",
      " 92%|█████████▏| 918/1000 [38:01<03:25,  2.51s/it]INFO:root:global_step: 918, logpy: 20.844, kl: 25.193, loss: 4.307\n",
      " 92%|█████████▏| 919/1000 [38:03<03:21,  2.49s/it]INFO:root:global_step: 919, logpy: 20.912, kl: 25.182, loss: 4.229\n",
      " 92%|█████████▏| 920/1000 [38:05<03:18,  2.49s/it]INFO:root:global_step: 920, logpy: 20.978, kl: 25.178, loss: 4.160\n",
      " 92%|█████████▏| 921/1000 [38:08<03:16,  2.48s/it]INFO:root:global_step: 921, logpy: 21.038, kl: 25.167, loss: 4.089\n",
      " 92%|█████████▏| 922/1000 [38:10<03:13,  2.48s/it]INFO:root:global_step: 922, logpy: 21.084, kl: 25.155, loss: 4.032\n",
      " 92%|█████████▏| 923/1000 [38:13<03:10,  2.47s/it]INFO:root:global_step: 923, logpy: 21.144, kl: 25.137, loss: 3.954\n",
      " 92%|█████████▏| 924/1000 [38:15<03:07,  2.47s/it]INFO:root:global_step: 924, logpy: 21.194, kl: 25.131, loss: 3.899\n",
      " 92%|█████████▎| 925/1000 [38:18<03:05,  2.47s/it]INFO:root:global_step: 925, logpy: 21.258, kl: 25.116, loss: 3.820\n",
      " 93%|█████████▎| 926/1000 [38:20<03:05,  2.51s/it]INFO:root:global_step: 926, logpy: 21.302, kl: 25.108, loss: 3.768\n",
      " 93%|█████████▎| 927/1000 [38:23<03:01,  2.49s/it]INFO:root:global_step: 927, logpy: 21.340, kl: 25.106, loss: 3.728\n",
      " 93%|█████████▎| 928/1000 [38:25<02:57,  2.47s/it]INFO:root:global_step: 928, logpy: 21.367, kl: 25.092, loss: 3.688\n",
      " 93%|█████████▎| 929/1000 [38:28<02:54,  2.46s/it]INFO:root:global_step: 929, logpy: 21.386, kl: 25.072, loss: 3.650\n",
      " 93%|█████████▎| 930/1000 [38:30<02:52,  2.46s/it]INFO:root:global_step: 930, logpy: 21.390, kl: 25.062, loss: 3.636\n",
      " 93%|█████████▎| 931/1000 [38:33<02:49,  2.46s/it]INFO:root:global_step: 931, logpy: 21.416, kl: 25.053, loss: 3.600\n",
      " 93%|█████████▎| 932/1000 [38:35<02:46,  2.45s/it]INFO:root:global_step: 932, logpy: 21.460, kl: 25.039, loss: 3.544\n",
      " 93%|█████████▎| 933/1000 [38:38<02:44,  2.46s/it]INFO:root:global_step: 933, logpy: 21.529, kl: 25.022, loss: 3.458\n",
      " 93%|█████████▎| 934/1000 [38:40<02:45,  2.50s/it]INFO:root:global_step: 934, logpy: 21.592, kl: 25.014, loss: 3.387\n",
      " 94%|█████████▎| 935/1000 [38:43<02:40,  2.47s/it]INFO:root:global_step: 935, logpy: 21.662, kl: 25.003, loss: 3.307\n",
      " 94%|█████████▎| 936/1000 [38:45<02:38,  2.47s/it]INFO:root:global_step: 936, logpy: 21.726, kl: 24.998, loss: 3.237\n",
      " 94%|█████████▎| 937/1000 [38:47<02:34,  2.46s/it]INFO:root:global_step: 937, logpy: 21.773, kl: 25.004, loss: 3.197\n",
      " 94%|█████████▍| 938/1000 [38:50<02:31,  2.44s/it]INFO:root:global_step: 938, logpy: 21.831, kl: 24.999, loss: 3.134\n",
      " 94%|█████████▍| 939/1000 [38:52<02:28,  2.44s/it]INFO:root:global_step: 939, logpy: 21.857, kl: 25.001, loss: 3.111\n",
      " 94%|█████████▍| 940/1000 [38:55<02:28,  2.47s/it]INFO:root:global_step: 940, logpy: 21.894, kl: 24.996, loss: 3.070\n",
      " 94%|█████████▍| 941/1000 [38:58<02:31,  2.57s/it]INFO:root:global_step: 941, logpy: 21.932, kl: 24.989, loss: 3.024\n",
      " 94%|█████████▍| 942/1000 [39:00<02:28,  2.56s/it]INFO:root:global_step: 942, logpy: 21.987, kl: 24.992, loss: 2.972\n",
      " 94%|█████████▍| 943/1000 [39:03<02:24,  2.54s/it]INFO:root:global_step: 943, logpy: 22.019, kl: 24.985, loss: 2.934\n",
      " 94%|█████████▍| 944/1000 [39:05<02:21,  2.52s/it]INFO:root:global_step: 944, logpy: 22.067, kl: 24.978, loss: 2.879\n",
      " 94%|█████████▍| 945/1000 [39:08<02:18,  2.52s/it]INFO:root:global_step: 945, logpy: 22.109, kl: 24.964, loss: 2.824\n",
      " 95%|█████████▍| 946/1000 [39:10<02:15,  2.51s/it]INFO:root:global_step: 946, logpy: 22.160, kl: 24.954, loss: 2.763\n",
      " 95%|█████████▍| 947/1000 [39:13<02:12,  2.50s/it]INFO:root:global_step: 947, logpy: 22.218, kl: 24.952, loss: 2.702\n",
      " 95%|█████████▍| 948/1000 [39:15<02:09,  2.49s/it]INFO:root:global_step: 948, logpy: 22.269, kl: 24.947, loss: 2.648\n",
      " 95%|█████████▍| 949/1000 [39:18<02:06,  2.49s/it]INFO:root:global_step: 949, logpy: 22.323, kl: 24.936, loss: 2.583\n",
      " 95%|█████████▌| 950/1000 [39:20<02:03,  2.47s/it]INFO:root:Saved figure at: ./sim/global_step_950.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 950, logpy: 22.361, kl: 24.926, loss: 2.536\n",
      " 95%|█████████▌| 951/1000 [39:24<02:19,  2.84s/it]INFO:root:global_step: 951, logpy: 22.400, kl: 24.916, loss: 2.486\n",
      " 95%|█████████▌| 952/1000 [39:26<02:15,  2.82s/it]INFO:root:global_step: 952, logpy: 22.381, kl: 24.918, loss: 2.507\n",
      " 95%|█████████▌| 953/1000 [39:29<02:07,  2.72s/it]INFO:root:global_step: 953, logpy: 22.368, kl: 24.910, loss: 2.513\n",
      " 95%|█████████▌| 954/1000 [39:31<02:01,  2.64s/it]INFO:root:global_step: 954, logpy: 22.382, kl: 24.894, loss: 2.484\n",
      " 96%|█████████▌| 955/1000 [39:34<01:57,  2.62s/it]INFO:root:global_step: 955, logpy: 22.413, kl: 24.890, loss: 2.448\n",
      " 96%|█████████▌| 956/1000 [39:37<01:55,  2.63s/it]INFO:root:global_step: 956, logpy: 22.459, kl: 24.880, loss: 2.393\n",
      " 96%|█████████▌| 957/1000 [39:40<01:57,  2.72s/it]INFO:root:global_step: 957, logpy: 22.489, kl: 24.879, loss: 2.362\n",
      " 96%|█████████▌| 958/1000 [39:42<01:50,  2.63s/it]INFO:root:global_step: 958, logpy: 22.533, kl: 24.879, loss: 2.318\n",
      " 96%|█████████▌| 959/1000 [39:44<01:45,  2.58s/it]INFO:root:global_step: 959, logpy: 22.589, kl: 24.865, loss: 2.249\n",
      " 96%|█████████▌| 960/1000 [39:47<01:40,  2.52s/it]INFO:root:global_step: 960, logpy: 22.646, kl: 24.852, loss: 2.179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 961/1000 [39:49<01:37,  2.50s/it]INFO:root:global_step: 961, logpy: 22.678, kl: 24.858, loss: 2.153\n",
      " 96%|█████████▌| 962/1000 [39:52<01:34,  2.49s/it]INFO:root:global_step: 962, logpy: 22.708, kl: 24.849, loss: 2.114\n",
      " 96%|█████████▋| 963/1000 [39:54<01:33,  2.53s/it]INFO:root:global_step: 963, logpy: 22.747, kl: 24.840, loss: 2.067\n",
      " 96%|█████████▋| 964/1000 [39:57<01:36,  2.68s/it]INFO:root:global_step: 964, logpy: 22.778, kl: 24.845, loss: 2.040\n",
      " 96%|█████████▋| 965/1000 [40:00<01:33,  2.67s/it]INFO:root:global_step: 965, logpy: 22.828, kl: 24.843, loss: 1.989\n",
      " 97%|█████████▋| 966/1000 [40:03<01:33,  2.75s/it]INFO:root:global_step: 966, logpy: 22.864, kl: 24.865, loss: 1.976\n",
      " 97%|█████████▋| 967/1000 [40:06<01:29,  2.72s/it]INFO:root:global_step: 967, logpy: 22.927, kl: 24.856, loss: 1.903\n",
      " 97%|█████████▋| 968/1000 [40:08<01:24,  2.65s/it]INFO:root:global_step: 968, logpy: 22.987, kl: 24.848, loss: 1.836\n",
      " 97%|█████████▋| 969/1000 [40:11<01:22,  2.65s/it]INFO:root:global_step: 969, logpy: 23.035, kl: 24.843, loss: 1.783\n",
      " 97%|█████████▋| 970/1000 [40:13<01:18,  2.60s/it]INFO:root:global_step: 970, logpy: 23.066, kl: 24.840, loss: 1.750\n",
      " 97%|█████████▋| 971/1000 [40:16<01:14,  2.59s/it]INFO:root:global_step: 971, logpy: 23.086, kl: 24.833, loss: 1.722\n",
      " 97%|█████████▋| 972/1000 [40:18<01:10,  2.53s/it]INFO:root:global_step: 972, logpy: 23.069, kl: 24.839, loss: 1.746\n",
      " 97%|█████████▋| 973/1000 [40:21<01:07,  2.49s/it]INFO:root:global_step: 973, logpy: 23.069, kl: 24.850, loss: 1.757\n",
      " 97%|█████████▋| 974/1000 [40:23<01:05,  2.53s/it]INFO:root:global_step: 974, logpy: 23.111, kl: 24.844, loss: 1.710\n",
      " 98%|█████████▊| 975/1000 [40:26<01:02,  2.50s/it]INFO:root:global_step: 975, logpy: 23.156, kl: 24.854, loss: 1.675\n",
      " 98%|█████████▊| 976/1000 [40:29<01:03,  2.64s/it]INFO:root:global_step: 976, logpy: 23.225, kl: 24.847, loss: 1.599\n",
      " 98%|█████████▊| 977/1000 [40:31<01:01,  2.68s/it]INFO:root:global_step: 977, logpy: 23.289, kl: 24.831, loss: 1.519\n",
      " 98%|█████████▊| 978/1000 [40:34<00:59,  2.69s/it]INFO:root:global_step: 978, logpy: 23.356, kl: 24.828, loss: 1.450\n",
      " 98%|█████████▊| 979/1000 [40:37<00:56,  2.69s/it]INFO:root:global_step: 979, logpy: 23.421, kl: 24.821, loss: 1.378\n",
      " 98%|█████████▊| 980/1000 [40:39<00:53,  2.68s/it]INFO:root:global_step: 980, logpy: 23.471, kl: 24.825, loss: 1.332\n",
      " 98%|█████████▊| 981/1000 [40:42<00:49,  2.61s/it]INFO:root:global_step: 981, logpy: 23.534, kl: 24.815, loss: 1.259\n",
      " 98%|█████████▊| 982/1000 [40:44<00:45,  2.56s/it]INFO:root:global_step: 982, logpy: 23.587, kl: 24.815, loss: 1.207\n",
      " 98%|█████████▊| 983/1000 [40:47<00:42,  2.52s/it]INFO:root:global_step: 983, logpy: 23.645, kl: 24.802, loss: 1.135\n",
      " 98%|█████████▊| 984/1000 [40:49<00:40,  2.51s/it]INFO:root:global_step: 984, logpy: 23.693, kl: 24.798, loss: 1.084\n",
      " 98%|█████████▊| 985/1000 [40:52<00:37,  2.51s/it]INFO:root:global_step: 985, logpy: 23.722, kl: 24.793, loss: 1.050\n",
      " 99%|█████████▊| 986/1000 [40:54<00:35,  2.51s/it]INFO:root:global_step: 986, logpy: 23.706, kl: 24.797, loss: 1.070\n",
      " 99%|█████████▊| 987/1000 [40:57<00:34,  2.62s/it]INFO:root:global_step: 987, logpy: 23.672, kl: 24.791, loss: 1.098\n",
      " 99%|█████████▉| 988/1000 [41:00<00:32,  2.68s/it]INFO:root:global_step: 988, logpy: 23.650, kl: 24.795, loss: 1.125\n",
      " 99%|█████████▉| 989/1000 [41:02<00:28,  2.58s/it]INFO:root:global_step: 989, logpy: 23.668, kl: 24.777, loss: 1.089\n",
      " 99%|█████████▉| 990/1000 [41:05<00:25,  2.54s/it]INFO:root:global_step: 990, logpy: 23.731, kl: 24.763, loss: 1.012\n",
      " 99%|█████████▉| 991/1000 [41:07<00:22,  2.51s/it]INFO:root:global_step: 991, logpy: 23.801, kl: 24.750, loss: 0.929\n",
      " 99%|█████████▉| 992/1000 [41:10<00:20,  2.54s/it]INFO:root:global_step: 992, logpy: 23.869, kl: 24.756, loss: 0.868\n",
      " 99%|█████████▉| 993/1000 [41:12<00:17,  2.52s/it]INFO:root:global_step: 993, logpy: 23.903, kl: 24.757, loss: 0.835\n",
      " 99%|█████████▉| 994/1000 [41:15<00:15,  2.53s/it]INFO:root:global_step: 994, logpy: 23.891, kl: 24.750, loss: 0.840\n",
      "100%|█████████▉| 995/1000 [41:17<00:12,  2.49s/it]INFO:root:global_step: 995, logpy: 23.823, kl: 24.759, loss: 0.916\n",
      "100%|█████████▉| 996/1000 [41:20<00:09,  2.46s/it]INFO:root:global_step: 996, logpy: 23.795, kl: 24.749, loss: 0.935\n",
      "100%|█████████▉| 997/1000 [41:22<00:07,  2.43s/it]INFO:root:global_step: 997, logpy: 23.837, kl: 24.741, loss: 0.886\n",
      "100%|█████████▉| 998/1000 [41:24<00:04,  2.42s/it]INFO:root:global_step: 998, logpy: 23.911, kl: 24.745, loss: 0.816\n",
      "100%|█████████▉| 999/1000 [41:27<00:02,  2.40s/it]INFO:root:global_step: 999, logpy: 23.984, kl: 24.737, loss: 0.735\n",
      "100%|██████████| 1000/1000 [41:29<00:00,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "manual_seed(args['seed'])\n",
    "\n",
    "if args['debug']:\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "ckpt_dir = os.path.join('./sim/', 'ckpts')\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "sdeint_fn = torchsde.sdeint_adjoint if args['adjoint'] else torchsde.sdeint\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356ad31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712260f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
