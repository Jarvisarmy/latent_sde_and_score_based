{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4dd7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from typing import Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import distributions, nn, optim\n",
    "\n",
    "import torchsde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0aeebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the gpu is available or not, if yes, use gpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2902c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the tuple for data, \n",
    "Data = namedtuple('Data', ['ts_', 'ts_ext_', 'ts_vis_', 'ts', 'ts_ext', 'ts_vis', 'ys', 'ys_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9198a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"train_iters\": 1000,\n",
    "    \"pause_iters\": 50,\n",
    "    \"hide_ticks\": False,\n",
    "    \"save_ckpt\":True,\n",
    "    \"likelihood\":\"laplace\",\n",
    "    \"scale\": 0.001,\n",
    "    \"adjoint\": True,\n",
    "    \"debug\": True,\n",
    "    \"seed\": 42,\n",
    "    'data':'segmented_cosine',\n",
    "    \"dt\": 1e-2,\n",
    "    \"batch_size\": 256,\n",
    "    \"method\": 'euler',\n",
    "    \"adaptive\": 'False',\n",
    "    \"rtol\": 1e-3,\n",
    "    \"atol\": 1e-3\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "40420fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output values from 0 to maxval with iters steps\n",
    "class LinearScheduler(object):\n",
    "    def __init__(self, iters, maxval=1.0):\n",
    "        self._iters = max(1,iters)\n",
    "        self._val = maxval/self._iters\n",
    "        self._maxval = maxval\n",
    "    \n",
    "    def step(self):\n",
    "        self._val = min(self._maxval, self._val+self._maxval/self._iters)\n",
    "        \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "93c09c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMAMetric(object):\n",
    "    def __init__(self, gamma: Optional[float]=0.99):\n",
    "        super(EMAMetric, self).__init__()\n",
    "        self._val=0\n",
    "        self._gamma = gamma\n",
    "    def step(self, x:Union[torch.Tensor, np.ndarray]):\n",
    "        x = x.detach().cpu().numpy() if torch.is_tensor(x) else x\n",
    "        self._val = self._gamma * self._val + (1-self._gamma)*x\n",
    "        return self._val\n",
    "    \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a4ec86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "def manual_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "98a2f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the dvision is stable\n",
    "def _stable_division(a,b,epsilon=1e-7):\n",
    "    b = torch.where(b.abs().detach() > epsilon, b, torch.full_like(b, fill_value=epsilon)*b.sign())\n",
    "    return a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "522d3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentSDE(torchsde.SDEIto): # sde with ito calculus\n",
    "    def __init__(self, theta=1.0, mu=0.0, sigma=0.01):\n",
    "        super(LatentSDE, self).__init__(noise_type=\"diagonal\")\n",
    "        logvar = math.log(sigma ** 2/(2.*theta))\n",
    "        \n",
    "        # prior drift\n",
    "        self.register_buffer(\"theta\",torch.tensor([[theta]])) # prior parameters, register 成buffer, 参数不会进行更新\n",
    "        self.register_buffer(\"mu\", torch.tensor([[mu]]))\n",
    "        self.register_buffer(\"sigma\",torch.tensor([[sigma]]))\n",
    "        \n",
    "        # p(y0)\n",
    "        self.register_buffer(\"py0_mean\", torch.tensor([[mu]]))\n",
    "        self.register_buffer(\"py0_logvar\", torch.tensor([[logvar]]))\n",
    "        \n",
    "        # approximate posterior drift: Takes in 2 positional encodings and the state\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3,200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200,200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200,1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Initialization the parameters\n",
    "        self.net[-1].weight.data.fill_(0.) # 初始化最后一层的参数\n",
    "        self.net[-1].bias.data.fill_(0.)\n",
    "            \n",
    "        # q(y0)\n",
    "        self.qy0_mean = nn.Parameter(torch.tensor([[mu]]), requires_grad=True) # 创建parameters\n",
    "        self.qy0_logvar = nn.Parameter(torch.tensor([[logvar]]), requires_grad=True) # 创建parameters\n",
    "        #self.theta = nn.Parameter(torch.tensor([[theta]]),requires_grad=True)\n",
    "        #self.sigma = nn.Parameter(torch.tensor([[sigma]]),requires_grad=True)\n",
    "        \n",
    "        #self.theta = nn.Parameter(torch.tensor([[theta]]),requires_grad=True)\n",
    "            \n",
    "    def f(self, t, y):  # Approximate posterior drift.\n",
    "        if t.dim() == 0:\n",
    "            t = torch.full_like(y, fill_value=t) # create a tensor of t\n",
    "        # Positional encoding in transformers for time-inhomogeneous posterior.\n",
    "        return self.net(torch.cat((torch.sin(t), torch.cos(t), y), dim=-1))\n",
    "\n",
    "    def g(self, t, y):  # Shared diffusion.\n",
    "        return self.sigma.repeat(y.size(0), 1) # 重复复制, 创建一个size为[y.size[0],1]\n",
    "\n",
    "    def h(self, t, y):  # Prior drift.\n",
    "        return self.theta * (self.mu - y)\n",
    "\n",
    "    def f_aug(self, t, y):  # Drift for augmented dynamics with logqp term.\n",
    "        y = y[:, 0:1] # 提取第一列，保持列的形态\n",
    "        f, g, h = self.f(t, y), self.g(t, y), self.h(t, y)\n",
    "        u = _stable_division(f - h, g) # 计算u(z,t)\n",
    "        f_logqp = .5 * (u ** 2).sum(dim=1, keepdim=True) # 计算integral\n",
    "        return torch.cat([f, f_logqp], dim=1)\n",
    "\n",
    "    def g_aug(self, t, y):  # Diffusion for augmented dynamics with logqp term.\n",
    "        y = y[:, 0:1]\n",
    "        g = self.g(t, y)\n",
    "        g_logqp = torch.zeros_like(y)\n",
    "        return torch.cat([g, g_logqp], dim=1)\n",
    "\n",
    "    def forward(self, ts, batch_size, eps=None):\n",
    "        eps = torch.randn(batch_size, 1).to(self.qy0_std) if eps is None else eps\n",
    "        y0 = self.qy0_mean + eps * self.qy0_std # the latent variable\n",
    "        qy0 = distributions.Normal(loc=self.qy0_mean, scale=self.qy0_std) # approximate posterior distribution\n",
    "        py0 = distributions.Normal(loc=self.py0_mean, scale=self.py0_std) # prior distribution\n",
    "        logqp0 = distributions.kl_divergence(qy0, py0).sum(dim=1)  # KL(t=0). calculate the kl divergence\n",
    "        #print(y0.size()) # (256, 1)\n",
    "        aug_y0 = torch.cat([y0, torch.zeros(batch_size, 1).to(y0)], dim=1) # create the augmented initial value\n",
    "        #print(aug_y0.size()) # [256, 2]\n",
    "        aug_ys = sdeint_fn(\n",
    "            sde=self,\n",
    "            y0=aug_y0,\n",
    "            ts=ts,\n",
    "            method=args['method'],\n",
    "            dt=args['dt'],\n",
    "            adaptive=args['adaptive'],\n",
    "            rtol=args['rtol'],\n",
    "            atol=args['atol'],\n",
    "            names={'drift': 'f_aug', 'diffusion': 'g_aug'}\n",
    "        ) # call the sde solver to \n",
    "        # print(aug_ys.size()) # [22, 256, 2]\n",
    "        ys, logqp_path = aug_ys[:, :, 0:1], aug_ys[-1, :, 1] # get the integral of the u(z,t) at the last time\n",
    "        \n",
    "        logqp = (logqp0 + logqp_path).mean(dim=0)  # KL(t=0) + KL(path).\n",
    "        return ys, logqp\n",
    "\n",
    "    def sample_p(self, ts, batch_size, eps=None, bm=None):\n",
    "        eps = torch.randn(batch_size, 1).to(self.py0_mean) if eps is None else eps\n",
    "        y0 = self.py0_mean + eps * self.py0_std\n",
    "        return sdeint_fn(self, y0, ts, bm=bm, method=args['method'], dt=args['dt'], names={'drift': 'h'}) # prior sde\n",
    "\n",
    "    def sample_q(self, ts, batch_size, eps=None, bm=None):\n",
    "        eps = torch.randn(batch_size, 1).to(self.qy0_mean) if eps is None else eps\n",
    "        y0 = self.qy0_mean + eps * self.qy0_std\n",
    "        return sdeint_fn(self, y0, ts, bm=bm, method=args['method'], dt=args['dt']) # posterior sde\n",
    "\n",
    "    @property\n",
    "    def py0_std(self):\n",
    "        return torch.exp(.5 * self.py0_logvar)\n",
    "\n",
    "    @property\n",
    "    def qy0_std(self):\n",
    "        return torch.exp(.5 * self.qy0_logvar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80736fff",
   "metadata": {},
   "source": [
    "## 5. Simulation of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cd15aaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = 120\n",
    "#beta = 0.1694\n",
    "beta=1\n",
    "phi = 0.9\n",
    "sigma_v = 0.003342\n",
    "u_over_v = 1\n",
    "rho = -0.856\n",
    "mu = 0\n",
    "def make_time_series():\n",
    "    \n",
    "    \n",
    "    noise_mu = [0, 0]\n",
    "    sigma_u = u_over_v * sigma_v\n",
    "    cov_uv = rho*sigma_u*sigma_v\n",
    "    cov = [[sigma_u**2, cov_uv], [cov_uv, sigma_v**2]]\n",
    "    \n",
    "    shocks = np.random.multivariate_normal(noise_mu, cov, T) # 1st column is u; 2nd columne is v\n",
    "    z0 = np.random.normal(mu, sigma_u ** 2/(1-phi**2), 1)\n",
    "    r0 = shocks[0][0]\n",
    "\n",
    "    z = np.zeros(T)\n",
    "    r = np.zeros(T)\n",
    "    z[0] = z0\n",
    "    r[0] = r0\n",
    "\n",
    "    for idx_t in range(T-1):\n",
    "        z[idx_t+1] = phi*(z[idx_t]-mu) + shocks[idx_t+1][1] + mu\n",
    "        r[idx_t+1] = beta*z[idx_t] + shocks[idx_t+1][0]\n",
    "    return r, z\n",
    "r,z = make_time_series()\n",
    "plt.figure(figsize=(15,5))\n",
    "xvalues = np.array(range(T))\n",
    "plt.plot(xvalues, r, linestyle='-', color='g', label=\"observed $r_t$\")\n",
    "plt.plot(xvalues, z, linestyle=\"--\", color=\"r\", label=r\"hidden variable $\\beta * z_t$\", linewidth=3.0)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('%')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Simulated $r_t$ and hidden $\\beta * z_t$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0cf9fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    ts_ = np.linspace(0.1,3.0,120)\n",
    "    ts_ext_ = np.array([0.] + list(ts_) + [3.1])\n",
    "    ts_vis_ = np.linspace(0.1, 3.1, 121)\n",
    "    ys_ = r[:,None]\n",
    "    ts = torch.tensor(ts_).float()\n",
    "    ts_ext = torch.tensor(ts_ext_).float()\n",
    "    ts_vis = torch.tensor(ts_vis_).float()\n",
    "    ys = torch.tensor(ys_).float().to(device)\n",
    "    return Data(ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "99a1fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Dataset\n",
    "    ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_ = make_data()\n",
    "    mu = torch.mean(ys)\n",
    "    sigma = torch.std(ys)\n",
    "    # plotting parameters\n",
    "    vis_batch_size = 1024\n",
    "    ylims = (-0.03, 0.03)\n",
    "    alphas = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55]\n",
    "    percentiles = [0.999, 0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    vis_idx = np.random.permutation(vis_batch_size)\n",
    "    sample_colors = ('#8c96c6', '#8c6bb1', '#810f7c')\n",
    "    fill_color = '#9ebcda'\n",
    "    mean_color = '#4d004b'\n",
    "    num_samples = len(sample_colors)\n",
    "    \n",
    "    eps = torch.randn(vis_batch_size, 1).to(device)\n",
    "    bm = torchsde.BrownianInterval(\n",
    "        t0=ts_vis[0],\n",
    "        t1=ts_vis[-1],\n",
    "        size=(vis_batch_size,1),\n",
    "        device=device,\n",
    "        levy_area_approximation=\"space-time\")\n",
    "    \n",
    "    # Model\n",
    "    model = LatentSDE(mu=mu,sigma=sigma).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    kl_scheduler = LinearScheduler(iters=500)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    logpy_metric = EMAMetric()\n",
    "    kl_metric = EMAMetric()\n",
    "    loss_metric = EMAMetric()\n",
    "    \n",
    "    \n",
    "    # show prior\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        zs = model.sample_p(ts=ts_vis, batch_size = vis_batch_size, eps = eps, bm=bm).squeeze()\n",
    "       \n",
    "        ts_vis_, zs_ = ts_vis.cpu().numpy(), zs.cpu().numpy()\n",
    "        #plt.scatter(ts_vis_, ys_, color='r', label=\"prior\")\n",
    "        zs_ = np.sort(zs_,axis=1)\n",
    "        img_dir = os.path.join('./img_generation/','prior.png')\n",
    "        plt.subplot(frameon=False)\n",
    "        for alpha, percentile in zip(alphas, percentiles):\n",
    "            idx = int((1 - percentile) / 2. * vis_batch_size)\n",
    "            zs_bot_ = zs_[:, idx]\n",
    "            zs_top_ = zs_[:, -idx]\n",
    "            plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
    "\n",
    "        # `zorder` determines who's on top; the larger the more at the top.\n",
    "        \n",
    "        plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # Data.\n",
    "        plt.plot(ts_, ys_, marker='x', zorder=3, color='k',label=\"observed $r_t$\")\n",
    "        plt.plot(ts_, z[:,None], color='g', linewidth=3.0, label=r\"hidden variable $z_t$\")\n",
    "        plt.ylim(ylims)\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$Y_t$')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.savefig(img_dir, dpi=300)\n",
    "        plt.close()\n",
    "        logging.info(f'Saved prior figure at: {img_dir}')\n",
    "    \n",
    "    \n",
    "    for global_step in tqdm.tqdm(range(args['train_iters'])):\n",
    "        \n",
    "        # Plot and save.\n",
    "        if global_step % args['pause_iters'] == 0:\n",
    "            img_path = os.path.join(\"./img_generation/\", f'global_step_{global_step}.png')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                zs = model.sample_q(ts=ts_vis, batch_size=vis_batch_size, eps=eps, bm=bm).squeeze()\n",
    "                samples = zs[:, vis_idx]\n",
    "                ts_vis_, zs_, samples_ = ts_vis.cpu().numpy(), zs.cpu().numpy(), samples.cpu().numpy()\n",
    "                zs_ = np.sort(zs_, axis=1)\n",
    "                plt.subplot(frameon=False)\n",
    "\n",
    "                if True: # args.show_percentiles:\n",
    "                    for alpha, percentile in zip(alphas, percentiles):\n",
    "                        idx = int((1 - percentile) / 2. * vis_batch_size)\n",
    "                        zs_bot_, zs_top_ = zs_[:, idx], zs_[:, -idx]\n",
    "                        plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
    "\n",
    "                if False: #args.show_mean:\n",
    "                    plt.plot(ts_vis_, zs_.mean(axis=1), color=mean_color)\n",
    "\n",
    "                if True: #args.show_samples:\n",
    "                    #for j in range(num_samples):\n",
    "                    #    plt.plot(ts_vis_, samples_[:, j], color=sample_colors[j], linewidth=1.0)\n",
    "                    # plt.plot(ts_, z[:,None], color='g', linewidth=3.0, label=r\"hidden variable $z_t$\")\n",
    "                    plt.plot(ts_vis_, samples_.mean(axis=1), marker='o', color='r',label=r'mean of latent variables')\n",
    "\n",
    "                if True: #args.show_arrows:\n",
    "                    num, dt = 3, 0.12\n",
    "                    t, y = torch.meshgrid(\n",
    "                        [torch.linspace(0, 3, num).to(device), torch.linspace(-0.3, 0.3, num).to(device)]\n",
    "                    )\n",
    "                    t, y = t.reshape(-1, 1), y.reshape(-1, 1)\n",
    "                    fty = model.f(t=t, y=y).reshape(num, num)\n",
    "                    dt = torch.zeros(num, num).fill_(dt).to(device)\n",
    "                    dy = fty * dt\n",
    "                    dt_, dy_, t_, y_ = dt.cpu().numpy(), dy.cpu().numpy(), t.cpu().numpy(), y.cpu().numpy()\n",
    "                    plt.quiver(t_, y_, dt_, dy_, alpha=0.3, edgecolors='k', width=0.0035, scale=50)\n",
    "\n",
    "                if False: #args.hide_ticks:\n",
    "                    plt.xticks([], [])\n",
    "                    plt.yticks([], [])\n",
    "\n",
    "                #plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # Data.\n",
    "                plt.plot(ts_, ys_, linestyle=\"-\",color='g', zorder=3, label=\"observed $r_t$ \") # new added\n",
    "                \n",
    "                for j in range(num_samples):\n",
    "                    zs = samples[:,j:j+1]\n",
    "                    likelihood_constructor = {\"laplace\": distributions.Laplace, \"normal\": distributions.Normal}[args['likelihood']]\n",
    "                    likelihood = likelihood_constructor(loc=zs, scale=args['scale'])\n",
    "                    reconstruct = likelihood.sample()\n",
    "                    print(reconstruct.size())\n",
    "                    plt.plot(ts_vis_, reconstruct, color=sample_colors[j], linewidth=1.0,label=r'estimated observations')\n",
    "                \n",
    "\n",
    "                \n",
    "                plt.ylim(ylims)\n",
    "                plt.xlabel('$t$')\n",
    "                plt.ylabel('$Y_t$')\n",
    "                plt.tight_layout()\n",
    "                plt.legend()\n",
    "                plt.savefig(img_path, dpi=300)\n",
    "                plt.close()\n",
    "                logging.info(f'Saved figure at: {img_path}')\n",
    "\n",
    "                \n",
    "        \n",
    "        # Train.\n",
    "        optimizer.zero_grad() # zero the gradient\n",
    "        zs, kl = model(ts=ts_ext, batch_size=args['batch_size']) # pass through the model, ys, logqp\n",
    "        zs = zs.squeeze() # remove the dimensions of input of size 1\n",
    "        zs = zs[1:-1]  # Drop first and last which are only used to penalize out-of-data region and spread uncertainty.\n",
    "        likelihood_constructor = {\"laplace\": distributions.Laplace, \"normal\": distributions.Normal}[args['likelihood']]\n",
    "        likelihood = likelihood_constructor(loc=zs, scale=args['scale']) # create the laplace distribution\n",
    "        logpy = likelihood.log_prob(ys).sum(dim=0).mean(dim=0)\n",
    "        loss = -logpy + kl * kl_scheduler.val\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        kl_scheduler.step()\n",
    "\n",
    "        logpy_metric.step(logpy)\n",
    "        kl_metric.step(kl)\n",
    "        loss_metric.step(loss)\n",
    "\n",
    "        logging.info(\n",
    "            f'global_step: {global_step}, '\n",
    "            f'logpy: {logpy_metric.val:.3f}, '\n",
    "            f'kl: {kl_metric.val:.3f}, '\n",
    "            f'loss: {loss_metric.val:.3f}'\n",
    "        )\n",
    "    torch.save(\n",
    "        {'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'kl_scheduler': kl_scheduler},\n",
    "        os.path.join('./sim/', f'global_step_{global_step}.ckpt')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "74b55c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved prior figure at: ./img_generation/prior.png\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_0.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 0, logpy: -18.315, kl: 0.030, loss: 18.315\n",
      "  0%|          | 1/1000 [00:02<47:56,  2.88s/it]INFO:root:global_step: 1, logpy: -585.703, kl: 22.858, loss: 585.795\n",
      "  0%|          | 2/1000 [00:04<40:47,  2.45s/it]INFO:root:global_step: 2, logpy: -1171.891, kl: 50.262, loss: 1172.147\n",
      "  0%|          | 3/1000 [00:05<35:54,  2.16s/it]INFO:root:global_step: 3, logpy: -1357.197, kl: 52.687, loss: 1357.474\n",
      "  0%|          | 4/1000 [00:07<32:24,  1.95s/it]INFO:root:global_step: 4, logpy: -1396.050, kl: 52.503, loss: 1396.328\n",
      "  0%|          | 5/1000 [00:08<30:34,  1.84s/it]INFO:root:global_step: 5, logpy: -1679.736, kl: 59.525, loss: 1680.102\n",
      "  1%|          | 6/1000 [00:10<29:10,  1.76s/it]INFO:root:global_step: 6, logpy: -1693.857, kl: 59.009, loss: 1694.220\n",
      "  1%|          | 7/1000 [00:11<27:59,  1.69s/it]INFO:root:global_step: 7, logpy: -1700.471, kl: 58.572, loss: 1700.833\n",
      "  1%|          | 8/1000 [00:13<26:52,  1.63s/it]INFO:root:global_step: 8, logpy: -1741.059, kl: 58.223, loss: 1741.421\n",
      "  1%|          | 9/1000 [00:14<26:17,  1.59s/it]INFO:root:global_step: 9, logpy: -1753.613, kl: 57.740, loss: 1753.974\n",
      "  1%|          | 10/1000 [00:16<25:10,  1.53s/it]INFO:root:global_step: 10, logpy: -1760.657, kl: 57.291, loss: 1761.017\n",
      "  1%|          | 11/1000 [00:17<24:32,  1.49s/it]INFO:root:global_step: 11, logpy: -1752.690, kl: 56.778, loss: 1753.048\n",
      "  1%|          | 12/1000 [00:19<23:47,  1.44s/it]INFO:root:global_step: 12, logpy: -1759.942, kl: 56.339, loss: 1760.300\n",
      "  1%|▏         | 13/1000 [00:20<23:32,  1.43s/it]INFO:root:global_step: 13, logpy: -1759.934, kl: 55.859, loss: 1760.290\n",
      "  1%|▏         | 14/1000 [00:21<23:50,  1.45s/it]INFO:root:global_step: 14, logpy: -1760.473, kl: 55.381, loss: 1760.829\n",
      "  2%|▏         | 15/1000 [00:23<24:09,  1.47s/it]INFO:root:global_step: 15, logpy: -1754.727, kl: 54.907, loss: 1755.081\n",
      "  2%|▏         | 16/1000 [00:24<24:11,  1.47s/it]INFO:root:global_step: 16, logpy: -1743.816, kl: 54.425, loss: 1744.169\n",
      "  2%|▏         | 17/1000 [00:26<24:13,  1.48s/it]INFO:root:global_step: 17, logpy: -1742.164, kl: 53.995, loss: 1742.518\n",
      "  2%|▏         | 18/1000 [00:27<24:18,  1.49s/it]INFO:root:global_step: 18, logpy: -1731.306, kl: 53.521, loss: 1731.659\n",
      "  2%|▏         | 19/1000 [00:29<24:25,  1.49s/it]INFO:root:global_step: 19, logpy: -1729.755, kl: 53.064, loss: 1730.107\n",
      "  2%|▏         | 20/1000 [00:31<25:03,  1.53s/it]INFO:root:global_step: 20, logpy: -1725.127, kl: 52.600, loss: 1725.478\n",
      "  2%|▏         | 21/1000 [00:32<26:29,  1.62s/it]INFO:root:global_step: 21, logpy: -1720.357, kl: 52.111, loss: 1720.707\n",
      "  2%|▏         | 22/1000 [00:34<26:47,  1.64s/it]INFO:root:global_step: 22, logpy: -1709.881, kl: 51.624, loss: 1710.228\n",
      "  2%|▏         | 23/1000 [00:36<26:54,  1.65s/it]INFO:root:global_step: 23, logpy: -1705.763, kl: 51.184, loss: 1706.111\n",
      "  2%|▏         | 24/1000 [00:37<26:35,  1.64s/it]INFO:root:global_step: 24, logpy: -1699.013, kl: 50.764, loss: 1699.362\n",
      "  2%|▎         | 25/1000 [00:39<26:07,  1.61s/it]INFO:root:global_step: 25, logpy: -1692.735, kl: 50.350, loss: 1693.085\n",
      "  3%|▎         | 26/1000 [00:41<26:20,  1.62s/it]INFO:root:global_step: 26, logpy: -1686.478, kl: 49.911, loss: 1686.828\n",
      "  3%|▎         | 27/1000 [00:42<26:52,  1.66s/it]INFO:root:global_step: 27, logpy: -1674.131, kl: 49.450, loss: 1674.480\n",
      "  3%|▎         | 28/1000 [00:44<27:23,  1.69s/it]INFO:root:global_step: 28, logpy: -1666.859, kl: 49.009, loss: 1667.208\n",
      "  3%|▎         | 29/1000 [00:46<27:48,  1.72s/it]INFO:root:global_step: 29, logpy: -1652.438, kl: 48.561, loss: 1652.786\n",
      "  3%|▎         | 30/1000 [00:48<27:50,  1.72s/it]INFO:root:global_step: 30, logpy: -1644.560, kl: 48.148, loss: 1644.908\n",
      "  3%|▎         | 31/1000 [00:49<27:53,  1.73s/it]INFO:root:global_step: 31, logpy: -1634.108, kl: 47.749, loss: 1634.458\n",
      "  3%|▎         | 32/1000 [00:51<27:50,  1.73s/it]INFO:root:global_step: 32, logpy: -1623.257, kl: 47.338, loss: 1623.609\n",
      "  3%|▎         | 33/1000 [00:53<27:39,  1.72s/it]INFO:root:global_step: 33, logpy: -1611.988, kl: 46.915, loss: 1612.339\n",
      "  3%|▎         | 34/1000 [00:55<27:58,  1.74s/it]INFO:root:global_step: 34, logpy: -1600.586, kl: 46.489, loss: 1600.937\n",
      "  4%|▎         | 35/1000 [00:56<28:26,  1.77s/it]INFO:root:global_step: 35, logpy: -1589.443, kl: 46.071, loss: 1589.794\n",
      "  4%|▎         | 36/1000 [00:58<28:43,  1.79s/it]INFO:root:global_step: 36, logpy: -1576.756, kl: 45.669, loss: 1577.107\n",
      "  4%|▎         | 37/1000 [01:00<28:55,  1.80s/it]INFO:root:global_step: 37, logpy: -1565.161, kl: 45.284, loss: 1565.515\n",
      "  4%|▍         | 38/1000 [01:02<28:42,  1.79s/it]INFO:root:global_step: 38, logpy: -1552.537, kl: 44.908, loss: 1552.893\n",
      "  4%|▍         | 39/1000 [01:04<28:33,  1.78s/it]INFO:root:global_step: 39, logpy: -1540.233, kl: 44.526, loss: 1540.590\n",
      "  4%|▍         | 40/1000 [01:06<29:21,  1.84s/it]INFO:root:global_step: 40, logpy: -1527.575, kl: 44.137, loss: 1527.934\n",
      "  4%|▍         | 41/1000 [01:07<29:35,  1.85s/it]INFO:root:global_step: 41, logpy: -1515.446, kl: 43.753, loss: 1515.806\n",
      "  4%|▍         | 42/1000 [01:09<29:54,  1.87s/it]INFO:root:global_step: 42, logpy: -1501.808, kl: 43.376, loss: 1502.170\n",
      "  4%|▍         | 43/1000 [01:11<29:51,  1.87s/it]INFO:root:global_step: 43, logpy: -1489.420, kl: 43.014, loss: 1489.784\n",
      "  4%|▍         | 44/1000 [01:13<29:25,  1.85s/it]INFO:root:global_step: 44, logpy: -1476.109, kl: 42.654, loss: 1476.476\n",
      "  4%|▍         | 45/1000 [01:15<29:23,  1.85s/it]INFO:root:global_step: 45, logpy: -1463.395, kl: 42.288, loss: 1463.764\n",
      "  5%|▍         | 46/1000 [01:17<29:28,  1.85s/it]INFO:root:global_step: 46, logpy: -1450.003, kl: 41.921, loss: 1450.374\n",
      "  5%|▍         | 47/1000 [01:19<29:39,  1.87s/it]INFO:root:global_step: 47, logpy: -1437.437, kl: 41.556, loss: 1437.808\n",
      "  5%|▍         | 48/1000 [01:21<29:52,  1.88s/it]INFO:root:global_step: 48, logpy: -1424.192, kl: 41.199, loss: 1424.566\n",
      "  5%|▍         | 49/1000 [01:22<29:54,  1.89s/it]INFO:root:global_step: 49, logpy: -1411.649, kl: 40.852, loss: 1412.026\n",
      "  5%|▌         | 50/1000 [01:24<29:51,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_50.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 50, logpy: -1398.633, kl: 40.509, loss: 1399.013\n",
      "  5%|▌         | 51/1000 [01:27<35:37,  2.25s/it]INFO:root:global_step: 51, logpy: -1385.997, kl: 40.168, loss: 1386.380\n",
      "  5%|▌         | 52/1000 [01:29<34:03,  2.16s/it]INFO:root:global_step: 52, logpy: -1373.271, kl: 39.826, loss: 1373.656\n",
      "  5%|▌         | 53/1000 [01:31<33:47,  2.14s/it]INFO:root:global_step: 53, logpy: -1360.767, kl: 39.487, loss: 1361.155\n",
      "  5%|▌         | 54/1000 [01:34<33:24,  2.12s/it]INFO:root:global_step: 54, logpy: -1348.292, kl: 39.155, loss: 1348.682\n",
      "  6%|▌         | 55/1000 [01:36<32:38,  2.07s/it]INFO:root:global_step: 55, logpy: -1335.994, kl: 38.835, loss: 1336.388\n",
      "  6%|▌         | 56/1000 [01:37<31:41,  2.01s/it]INFO:root:global_step: 56, logpy: -1323.813, kl: 38.515, loss: 1324.211\n",
      "  6%|▌         | 57/1000 [01:39<31:23,  2.00s/it]INFO:root:global_step: 57, logpy: -1311.634, kl: 38.196, loss: 1312.036\n",
      "  6%|▌         | 58/1000 [01:41<31:08,  1.98s/it]INFO:root:global_step: 58, logpy: -1299.597, kl: 37.874, loss: 1300.002\n",
      "  6%|▌         | 59/1000 [01:43<31:46,  2.03s/it]INFO:root:global_step: 59, logpy: -1287.423, kl: 37.555, loss: 1287.832\n",
      "  6%|▌         | 60/1000 [01:46<32:21,  2.07s/it]INFO:root:global_step: 60, logpy: -1275.611, kl: 37.245, loss: 1276.023\n",
      "  6%|▌         | 61/1000 [01:48<32:16,  2.06s/it]INFO:root:global_step: 61, logpy: -1263.805, kl: 36.941, loss: 1264.221\n",
      "  6%|▌         | 62/1000 [01:50<31:35,  2.02s/it]INFO:root:global_step: 62, logpy: -1252.358, kl: 36.639, loss: 1252.779\n",
      "  6%|▋         | 63/1000 [01:52<31:25,  2.01s/it]INFO:root:global_step: 63, logpy: -1240.625, kl: 36.335, loss: 1241.049\n",
      "  6%|▋         | 64/1000 [01:54<31:40,  2.03s/it]INFO:root:global_step: 64, logpy: -1229.287, kl: 36.030, loss: 1229.715\n",
      "  6%|▋         | 65/1000 [01:56<31:39,  2.03s/it]INFO:root:global_step: 65, logpy: -1217.863, kl: 35.731, loss: 1218.295\n",
      "  7%|▋         | 66/1000 [01:58<31:48,  2.04s/it]INFO:root:global_step: 66, logpy: -1206.610, kl: 35.441, loss: 1207.047\n",
      "  7%|▋         | 67/1000 [02:00<31:37,  2.03s/it]INFO:root:global_step: 67, logpy: -1195.301, kl: 35.151, loss: 1195.742\n",
      "  7%|▋         | 68/1000 [02:02<31:26,  2.02s/it]INFO:root:global_step: 68, logpy: -1184.207, kl: 34.866, loss: 1184.653\n",
      "  7%|▋         | 69/1000 [02:04<31:24,  2.02s/it]INFO:root:global_step: 69, logpy: -1173.210, kl: 34.581, loss: 1173.660\n",
      "  7%|▋         | 70/1000 [02:06<32:03,  2.07s/it]INFO:root:global_step: 70, logpy: -1162.381, kl: 34.301, loss: 1162.836\n",
      "  7%|▋         | 71/1000 [02:08<32:14,  2.08s/it]INFO:root:global_step: 71, logpy: -1151.840, kl: 34.023, loss: 1152.300\n",
      "  7%|▋         | 72/1000 [02:10<32:22,  2.09s/it]INFO:root:global_step: 72, logpy: -1140.953, kl: 33.750, loss: 1141.418\n",
      "  7%|▋         | 73/1000 [02:12<32:08,  2.08s/it]INFO:root:global_step: 73, logpy: -1130.571, kl: 33.482, loss: 1131.041\n",
      "  7%|▋         | 74/1000 [02:14<32:08,  2.08s/it]INFO:root:global_step: 74, logpy: -1119.831, kl: 33.213, loss: 1120.307\n",
      "  8%|▊         | 75/1000 [02:16<32:05,  2.08s/it]INFO:root:global_step: 75, logpy: -1109.355, kl: 32.943, loss: 1109.836\n",
      "  8%|▊         | 76/1000 [02:18<31:44,  2.06s/it]INFO:root:global_step: 76, logpy: -1098.892, kl: 32.679, loss: 1099.378\n",
      "  8%|▊         | 77/1000 [02:20<31:42,  2.06s/it]INFO:root:global_step: 77, logpy: -1088.659, kl: 32.419, loss: 1089.150\n",
      "  8%|▊         | 78/1000 [02:23<31:35,  2.06s/it]INFO:root:global_step: 78, logpy: -1078.491, kl: 32.164, loss: 1078.988\n",
      "  8%|▊         | 79/1000 [02:25<31:19,  2.04s/it]INFO:root:global_step: 79, logpy: -1068.310, kl: 31.912, loss: 1068.813\n",
      "  8%|▊         | 80/1000 [02:27<31:20,  2.04s/it]INFO:root:global_step: 80, logpy: -1058.213, kl: 31.655, loss: 1058.722\n",
      "  8%|▊         | 81/1000 [02:29<31:22,  2.05s/it]INFO:root:global_step: 81, logpy: -1048.140, kl: 31.403, loss: 1048.654\n",
      "  8%|▊         | 82/1000 [02:31<31:17,  2.04s/it]INFO:root:global_step: 82, logpy: -1038.365, kl: 31.157, loss: 1038.885\n",
      "  8%|▊         | 83/1000 [02:33<31:21,  2.05s/it]INFO:root:global_step: 83, logpy: -1028.455, kl: 30.913, loss: 1028.982\n",
      "  8%|▊         | 84/1000 [02:35<31:12,  2.04s/it]INFO:root:global_step: 84, logpy: -1018.753, kl: 30.674, loss: 1019.286\n",
      "  8%|▊         | 85/1000 [02:37<31:28,  2.06s/it]INFO:root:global_step: 85, logpy: -1008.935, kl: 30.432, loss: 1009.474\n",
      "  9%|▊         | 86/1000 [02:39<31:25,  2.06s/it]INFO:root:global_step: 86, logpy: -999.411, kl: 30.195, loss: 999.956\n",
      "  9%|▊         | 87/1000 [02:41<31:26,  2.07s/it]INFO:root:global_step: 87, logpy: -989.863, kl: 29.960, loss: 990.414\n",
      "  9%|▉         | 88/1000 [02:43<31:20,  2.06s/it]INFO:root:global_step: 88, logpy: -980.405, kl: 29.731, loss: 980.964\n",
      "  9%|▉         | 89/1000 [02:45<31:03,  2.05s/it]INFO:root:global_step: 89, logpy: -971.044, kl: 29.503, loss: 971.609\n",
      "  9%|▉         | 90/1000 [02:47<30:59,  2.04s/it]INFO:root:global_step: 90, logpy: -961.650, kl: 29.278, loss: 962.222\n",
      "  9%|▉         | 91/1000 [02:49<31:03,  2.05s/it]INFO:root:global_step: 91, logpy: -952.422, kl: 29.051, loss: 953.001\n",
      "  9%|▉         | 92/1000 [02:51<31:07,  2.06s/it]INFO:root:global_step: 92, logpy: -943.251, kl: 28.829, loss: 943.837\n",
      "  9%|▉         | 93/1000 [02:53<31:08,  2.06s/it]INFO:root:global_step: 93, logpy: -934.159, kl: 28.608, loss: 934.751\n",
      "  9%|▉         | 94/1000 [02:55<31:18,  2.07s/it]INFO:root:global_step: 94, logpy: -925.044, kl: 28.389, loss: 925.643\n",
      " 10%|▉         | 95/1000 [02:58<31:22,  2.08s/it]INFO:root:global_step: 95, logpy: -916.096, kl: 28.171, loss: 916.702\n",
      " 10%|▉         | 96/1000 [03:00<31:23,  2.08s/it]INFO:root:global_step: 96, logpy: -907.286, kl: 27.954, loss: 907.898\n",
      " 10%|▉         | 97/1000 [03:02<31:33,  2.10s/it]INFO:root:global_step: 97, logpy: -898.433, kl: 27.743, loss: 899.053\n",
      " 10%|▉         | 98/1000 [03:04<31:36,  2.10s/it]INFO:root:global_step: 98, logpy: -889.655, kl: 27.536, loss: 890.283\n",
      " 10%|▉         | 99/1000 [03:06<31:47,  2.12s/it]INFO:root:global_step: 99, logpy: -880.992, kl: 27.333, loss: 881.628\n",
      " 10%|█         | 100/1000 [03:08<31:44,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_100.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 100, logpy: -872.371, kl: 27.126, loss: 873.014\n",
      " 10%|█         | 101/1000 [03:11<37:21,  2.49s/it]INFO:root:global_step: 101, logpy: -863.803, kl: 26.919, loss: 864.452\n",
      " 10%|█         | 102/1000 [03:14<35:51,  2.40s/it]INFO:root:global_step: 102, logpy: -855.393, kl: 26.720, loss: 856.050\n",
      " 10%|█         | 103/1000 [03:16<34:43,  2.32s/it]INFO:root:global_step: 103, logpy: -847.124, kl: 26.523, loss: 847.790\n",
      " 10%|█         | 104/1000 [03:18<33:54,  2.27s/it]INFO:root:global_step: 104, logpy: -838.889, kl: 26.327, loss: 839.563\n",
      " 10%|█         | 105/1000 [03:20<33:17,  2.23s/it]INFO:root:global_step: 105, logpy: -830.771, kl: 26.133, loss: 831.452\n",
      " 11%|█         | 106/1000 [03:22<32:47,  2.20s/it]INFO:root:global_step: 106, logpy: -822.571, kl: 25.937, loss: 823.259\n",
      " 11%|█         | 107/1000 [03:24<32:32,  2.19s/it]INFO:root:global_step: 107, logpy: -814.546, kl: 25.745, loss: 815.242\n",
      " 11%|█         | 108/1000 [03:27<32:25,  2.18s/it]INFO:root:global_step: 108, logpy: -806.623, kl: 25.558, loss: 807.327\n",
      " 11%|█         | 109/1000 [03:29<32:23,  2.18s/it]INFO:root:global_step: 109, logpy: -798.766, kl: 25.369, loss: 799.478\n",
      " 11%|█         | 110/1000 [03:31<32:21,  2.18s/it]INFO:root:global_step: 110, logpy: -790.913, kl: 25.180, loss: 791.633\n",
      " 11%|█         | 111/1000 [03:33<32:53,  2.22s/it]INFO:root:global_step: 111, logpy: -783.207, kl: 24.994, loss: 783.934\n",
      " 11%|█         | 112/1000 [03:35<32:49,  2.22s/it]INFO:root:global_step: 112, logpy: -775.486, kl: 24.814, loss: 776.222\n",
      " 11%|█▏        | 113/1000 [03:38<32:38,  2.21s/it]INFO:root:global_step: 113, logpy: -767.872, kl: 24.638, loss: 768.617\n",
      " 11%|█▏        | 114/1000 [03:40<32:31,  2.20s/it]INFO:root:global_step: 114, logpy: -760.272, kl: 24.460, loss: 761.025\n",
      " 12%|█▏        | 115/1000 [03:42<32:51,  2.23s/it]INFO:root:global_step: 115, logpy: -752.864, kl: 24.281, loss: 753.624\n",
      " 12%|█▏        | 116/1000 [03:44<32:57,  2.24s/it]INFO:root:global_step: 116, logpy: -745.432, kl: 24.105, loss: 746.201\n",
      " 12%|█▏        | 117/1000 [03:47<32:52,  2.23s/it]INFO:root:global_step: 117, logpy: -738.036, kl: 23.934, loss: 738.813\n",
      " 12%|█▏        | 118/1000 [03:49<32:44,  2.23s/it]INFO:root:global_step: 118, logpy: -730.777, kl: 23.769, loss: 731.565\n",
      " 12%|█▏        | 119/1000 [03:51<32:32,  2.22s/it]INFO:root:global_step: 119, logpy: -723.477, kl: 23.603, loss: 724.273\n",
      " 12%|█▏        | 120/1000 [03:53<32:35,  2.22s/it]INFO:root:global_step: 120, logpy: -716.234, kl: 23.437, loss: 717.040\n",
      " 12%|█▏        | 121/1000 [03:56<32:54,  2.25s/it]INFO:root:global_step: 121, logpy: -709.067, kl: 23.272, loss: 709.881\n",
      " 12%|█▏        | 122/1000 [03:58<33:22,  2.28s/it]INFO:root:global_step: 122, logpy: -701.968, kl: 23.114, loss: 702.792\n",
      " 12%|█▏        | 123/1000 [04:00<33:24,  2.29s/it]INFO:root:global_step: 123, logpy: -695.028, kl: 22.956, loss: 695.863\n",
      " 12%|█▏        | 124/1000 [04:02<33:26,  2.29s/it]INFO:root:global_step: 124, logpy: -687.988, kl: 22.796, loss: 688.832\n",
      " 12%|█▎        | 125/1000 [04:05<33:17,  2.28s/it]INFO:root:global_step: 125, logpy: -681.081, kl: 22.637, loss: 681.933\n",
      " 13%|█▎        | 126/1000 [04:07<33:09,  2.28s/it]INFO:root:global_step: 126, logpy: -674.234, kl: 22.479, loss: 675.096\n",
      " 13%|█▎        | 127/1000 [04:09<33:03,  2.27s/it]INFO:root:global_step: 127, logpy: -667.437, kl: 22.325, loss: 668.308\n",
      " 13%|█▎        | 128/1000 [04:12<33:12,  2.29s/it]INFO:root:global_step: 128, logpy: -660.721, kl: 22.172, loss: 661.602\n",
      " 13%|█▎        | 129/1000 [04:14<33:15,  2.29s/it]INFO:root:global_step: 129, logpy: -654.007, kl: 22.020, loss: 654.897\n",
      " 13%|█▎        | 130/1000 [04:16<33:28,  2.31s/it]INFO:root:global_step: 130, logpy: -647.335, kl: 21.868, loss: 648.234\n",
      " 13%|█▎        | 131/1000 [04:19<33:35,  2.32s/it]INFO:root:global_step: 131, logpy: -640.736, kl: 21.717, loss: 641.644\n",
      " 13%|█▎        | 132/1000 [04:21<33:26,  2.31s/it]INFO:root:global_step: 132, logpy: -634.214, kl: 21.569, loss: 635.131\n",
      " 13%|█▎        | 133/1000 [04:23<33:21,  2.31s/it]INFO:root:global_step: 133, logpy: -627.828, kl: 21.424, loss: 628.755\n",
      " 13%|█▎        | 134/1000 [04:25<33:08,  2.30s/it]INFO:root:global_step: 134, logpy: -621.400, kl: 21.277, loss: 622.336\n",
      " 14%|█▎        | 135/1000 [04:28<33:16,  2.31s/it]INFO:root:global_step: 135, logpy: -614.967, kl: 21.136, loss: 615.913\n",
      " 14%|█▎        | 136/1000 [04:30<33:32,  2.33s/it]INFO:root:global_step: 136, logpy: -608.669, kl: 20.997, loss: 609.625\n",
      " 14%|█▎        | 137/1000 [04:32<33:19,  2.32s/it]INFO:root:global_step: 137, logpy: -602.398, kl: 20.857, loss: 603.364\n",
      " 14%|█▍        | 138/1000 [04:35<33:19,  2.32s/it]INFO:root:global_step: 138, logpy: -596.236, kl: 20.723, loss: 597.213\n",
      " 14%|█▍        | 139/1000 [04:37<33:19,  2.32s/it]INFO:root:global_step: 139, logpy: -590.075, kl: 20.587, loss: 591.062\n",
      " 14%|█▍        | 140/1000 [04:39<33:28,  2.34s/it]INFO:root:global_step: 140, logpy: -584.072, kl: 20.452, loss: 585.070\n",
      " 14%|█▍        | 141/1000 [04:42<33:26,  2.34s/it]INFO:root:global_step: 141, logpy: -577.958, kl: 20.317, loss: 578.965\n",
      " 14%|█▍        | 142/1000 [04:44<33:22,  2.33s/it]INFO:root:global_step: 142, logpy: -571.985, kl: 20.187, loss: 573.003\n",
      " 14%|█▍        | 143/1000 [04:46<33:15,  2.33s/it]INFO:root:global_step: 143, logpy: -566.048, kl: 20.057, loss: 567.077\n",
      " 14%|█▍        | 144/1000 [04:49<33:17,  2.33s/it]INFO:root:global_step: 144, logpy: -560.116, kl: 19.928, loss: 561.155\n",
      " 14%|█▍        | 145/1000 [04:51<33:25,  2.35s/it]INFO:root:global_step: 145, logpy: -554.309, kl: 19.800, loss: 555.358\n",
      " 15%|█▍        | 146/1000 [04:54<33:27,  2.35s/it]INFO:root:global_step: 146, logpy: -548.498, kl: 19.673, loss: 549.558\n",
      " 15%|█▍        | 147/1000 [04:56<34:00,  2.39s/it]INFO:root:global_step: 147, logpy: -542.813, kl: 19.549, loss: 543.883\n",
      " 15%|█▍        | 148/1000 [04:58<33:43,  2.38s/it]INFO:root:global_step: 148, logpy: -537.159, kl: 19.425, loss: 538.240\n",
      " 15%|█▍        | 149/1000 [05:01<33:32,  2.37s/it]INFO:root:global_step: 149, logpy: -531.509, kl: 19.303, loss: 532.601\n",
      " 15%|█▌        | 150/1000 [05:03<33:31,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_150.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 150, logpy: -525.957, kl: 19.181, loss: 527.060\n",
      " 15%|█▌        | 151/1000 [05:07<38:48,  2.74s/it]INFO:root:global_step: 151, logpy: -520.451, kl: 19.064, loss: 521.566\n",
      " 15%|█▌        | 152/1000 [05:09<37:40,  2.67s/it]INFO:root:global_step: 152, logpy: -514.930, kl: 18.947, loss: 516.056\n",
      " 15%|█▌        | 153/1000 [05:12<36:27,  2.58s/it]INFO:root:global_step: 153, logpy: -509.459, kl: 18.831, loss: 510.596\n",
      " 15%|█▌        | 154/1000 [05:14<35:30,  2.52s/it]INFO:root:global_step: 154, logpy: -504.124, kl: 18.718, loss: 505.273\n",
      " 16%|█▌        | 155/1000 [05:16<35:09,  2.50s/it]INFO:root:global_step: 155, logpy: -498.752, kl: 18.604, loss: 499.912\n",
      " 16%|█▌        | 156/1000 [05:19<35:11,  2.50s/it]INFO:root:global_step: 156, logpy: -493.482, kl: 18.491, loss: 494.654\n",
      " 16%|█▌        | 157/1000 [05:22<35:41,  2.54s/it]INFO:root:global_step: 157, logpy: -488.183, kl: 18.379, loss: 489.365\n",
      " 16%|█▌        | 158/1000 [05:24<35:15,  2.51s/it]INFO:root:global_step: 158, logpy: -483.014, kl: 18.271, loss: 484.209\n",
      " 16%|█▌        | 159/1000 [05:26<34:38,  2.47s/it]INFO:root:global_step: 159, logpy: -477.830, kl: 18.161, loss: 479.036\n",
      " 16%|█▌        | 160/1000 [05:29<34:15,  2.45s/it]INFO:root:global_step: 160, logpy: -472.732, kl: 18.051, loss: 473.950\n",
      " 16%|█▌        | 161/1000 [05:31<34:17,  2.45s/it]INFO:root:global_step: 161, logpy: -467.684, kl: 17.941, loss: 468.913\n",
      " 16%|█▌        | 162/1000 [05:34<34:09,  2.45s/it]INFO:root:global_step: 162, logpy: -462.698, kl: 17.836, loss: 463.938\n",
      " 16%|█▋        | 163/1000 [05:36<34:32,  2.48s/it]INFO:root:global_step: 163, logpy: -457.772, kl: 17.735, loss: 459.025\n",
      " 16%|█▋        | 164/1000 [05:39<34:07,  2.45s/it]INFO:root:global_step: 164, logpy: -452.872, kl: 17.634, loss: 454.138\n",
      " 16%|█▋        | 165/1000 [05:41<33:54,  2.44s/it]INFO:root:global_step: 165, logpy: -448.005, kl: 17.533, loss: 449.283\n",
      " 17%|█▋        | 166/1000 [05:44<34:28,  2.48s/it]INFO:root:global_step: 166, logpy: -443.214, kl: 17.435, loss: 444.505\n",
      " 17%|█▋        | 167/1000 [05:46<35:02,  2.52s/it]INFO:root:global_step: 167, logpy: -438.372, kl: 17.336, loss: 439.676\n",
      " 17%|█▋        | 168/1000 [05:49<35:26,  2.56s/it]INFO:root:global_step: 168, logpy: -433.645, kl: 17.239, loss: 434.961\n",
      " 17%|█▋        | 169/1000 [05:51<34:47,  2.51s/it]INFO:root:global_step: 169, logpy: -428.972, kl: 17.144, loss: 430.302\n",
      " 17%|█▋        | 170/1000 [05:54<34:23,  2.49s/it]INFO:root:global_step: 170, logpy: -424.360, kl: 17.046, loss: 425.701\n",
      " 17%|█▋        | 171/1000 [05:56<34:10,  2.47s/it]INFO:root:global_step: 171, logpy: -419.648, kl: 16.950, loss: 421.002\n",
      " 17%|█▋        | 172/1000 [05:59<34:21,  2.49s/it]INFO:root:global_step: 172, logpy: -415.020, kl: 16.856, loss: 416.386\n",
      " 17%|█▋        | 173/1000 [06:01<34:07,  2.48s/it]INFO:root:global_step: 173, logpy: -410.520, kl: 16.766, loss: 411.899\n",
      " 17%|█▋        | 174/1000 [06:03<33:48,  2.46s/it]INFO:root:global_step: 174, logpy: -405.936, kl: 16.673, loss: 407.328\n",
      " 18%|█▊        | 175/1000 [06:06<33:49,  2.46s/it]INFO:root:global_step: 175, logpy: -401.477, kl: 16.580, loss: 402.882\n",
      " 18%|█▊        | 176/1000 [06:08<33:55,  2.47s/it]INFO:root:global_step: 176, logpy: -397.045, kl: 16.488, loss: 398.462\n",
      " 18%|█▊        | 177/1000 [06:11<33:58,  2.48s/it]INFO:root:global_step: 177, logpy: -392.635, kl: 16.400, loss: 394.064\n",
      " 18%|█▊        | 178/1000 [06:13<33:52,  2.47s/it]INFO:root:global_step: 178, logpy: -388.254, kl: 16.314, loss: 389.697\n",
      " 18%|█▊        | 179/1000 [06:16<34:12,  2.50s/it]INFO:root:global_step: 179, logpy: -383.922, kl: 16.229, loss: 385.378\n",
      " 18%|█▊        | 180/1000 [06:18<33:59,  2.49s/it]INFO:root:global_step: 180, logpy: -379.629, kl: 16.144, loss: 381.099\n",
      " 18%|█▊        | 181/1000 [06:21<33:50,  2.48s/it]INFO:root:global_step: 181, logpy: -375.422, kl: 16.060, loss: 376.906\n",
      " 18%|█▊        | 182/1000 [06:23<33:55,  2.49s/it]INFO:root:global_step: 182, logpy: -371.188, kl: 15.977, loss: 372.685\n",
      " 18%|█▊        | 183/1000 [06:26<33:48,  2.48s/it]INFO:root:global_step: 183, logpy: -367.003, kl: 15.895, loss: 368.514\n",
      " 18%|█▊        | 184/1000 [06:28<33:41,  2.48s/it]INFO:root:global_step: 184, logpy: -362.920, kl: 15.815, loss: 364.445\n",
      " 18%|█▊        | 185/1000 [06:31<33:34,  2.47s/it]INFO:root:global_step: 185, logpy: -358.851, kl: 15.736, loss: 360.390\n",
      " 19%|█▊        | 186/1000 [06:33<33:37,  2.48s/it]INFO:root:global_step: 186, logpy: -354.773, kl: 15.658, loss: 356.327\n",
      " 19%|█▊        | 187/1000 [06:36<33:35,  2.48s/it]INFO:root:global_step: 187, logpy: -350.804, kl: 15.581, loss: 352.372\n",
      " 19%|█▉        | 188/1000 [06:38<33:30,  2.48s/it]INFO:root:global_step: 188, logpy: -346.774, kl: 15.504, loss: 348.356\n",
      " 19%|█▉        | 189/1000 [06:41<33:32,  2.48s/it]INFO:root:global_step: 189, logpy: -342.822, kl: 15.429, loss: 344.419\n",
      " 19%|█▉        | 190/1000 [06:43<33:50,  2.51s/it]INFO:root:global_step: 190, logpy: -338.956, kl: 15.355, loss: 340.568\n",
      " 19%|█▉        | 191/1000 [06:46<33:51,  2.51s/it]INFO:root:global_step: 191, logpy: -334.960, kl: 15.282, loss: 336.586\n",
      " 19%|█▉        | 192/1000 [06:48<33:47,  2.51s/it]INFO:root:global_step: 192, logpy: -331.051, kl: 15.210, loss: 332.692\n",
      " 19%|█▉        | 193/1000 [06:51<33:43,  2.51s/it]INFO:root:global_step: 193, logpy: -327.227, kl: 15.137, loss: 328.883\n",
      " 19%|█▉        | 194/1000 [06:53<33:46,  2.51s/it]INFO:root:global_step: 194, logpy: -323.404, kl: 15.065, loss: 325.074\n",
      " 20%|█▉        | 195/1000 [06:56<33:45,  2.52s/it]INFO:root:global_step: 195, logpy: -319.658, kl: 14.994, loss: 321.343\n",
      " 20%|█▉        | 196/1000 [06:58<33:37,  2.51s/it]INFO:root:global_step: 196, logpy: -315.981, kl: 14.928, loss: 317.682\n",
      " 20%|█▉        | 197/1000 [07:01<33:42,  2.52s/it]INFO:root:global_step: 197, logpy: -312.213, kl: 14.861, loss: 313.930\n",
      " 20%|█▉        | 198/1000 [07:03<33:32,  2.51s/it]INFO:root:global_step: 198, logpy: -308.558, kl: 14.794, loss: 310.290\n",
      " 20%|█▉        | 199/1000 [07:06<33:58,  2.54s/it]INFO:root:global_step: 199, logpy: -305.012, kl: 14.727, loss: 306.758\n",
      " 20%|██        | 200/1000 [07:09<33:52,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_200.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 200, logpy: -301.454, kl: 14.662, loss: 303.217\n",
      " 20%|██        | 201/1000 [07:12<39:23,  2.96s/it]INFO:root:global_step: 201, logpy: -297.845, kl: 14.597, loss: 299.623\n",
      " 20%|██        | 202/1000 [07:15<37:45,  2.84s/it]INFO:root:global_step: 202, logpy: -294.333, kl: 14.533, loss: 296.126\n",
      " 20%|██        | 203/1000 [07:18<36:32,  2.75s/it]INFO:root:global_step: 203, logpy: -290.791, kl: 14.469, loss: 292.599\n",
      " 20%|██        | 204/1000 [07:20<35:48,  2.70s/it]INFO:root:global_step: 204, logpy: -287.305, kl: 14.407, loss: 289.130\n",
      " 20%|██        | 205/1000 [07:23<34:58,  2.64s/it]INFO:root:global_step: 205, logpy: -283.846, kl: 14.347, loss: 285.686\n",
      " 21%|██        | 206/1000 [07:25<34:34,  2.61s/it]INFO:root:global_step: 206, logpy: -280.428, kl: 14.285, loss: 282.284\n",
      " 21%|██        | 207/1000 [07:28<34:28,  2.61s/it]INFO:root:global_step: 207, logpy: -277.036, kl: 14.225, loss: 278.908\n",
      " 21%|██        | 208/1000 [07:30<34:31,  2.62s/it]INFO:root:global_step: 208, logpy: -273.730, kl: 14.166, loss: 275.617\n",
      " 21%|██        | 209/1000 [07:33<34:43,  2.63s/it]INFO:root:global_step: 209, logpy: -270.367, kl: 14.106, loss: 272.270\n",
      " 21%|██        | 210/1000 [07:36<34:30,  2.62s/it]INFO:root:global_step: 210, logpy: -267.054, kl: 14.050, loss: 268.974\n",
      " 21%|██        | 211/1000 [07:38<34:29,  2.62s/it]INFO:root:global_step: 211, logpy: -263.718, kl: 13.996, loss: 265.655\n",
      " 21%|██        | 212/1000 [07:41<34:08,  2.60s/it]INFO:root:global_step: 212, logpy: -260.475, kl: 13.939, loss: 262.429\n",
      " 21%|██▏       | 213/1000 [07:43<34:11,  2.61s/it]INFO:root:global_step: 213, logpy: -257.258, kl: 13.886, loss: 259.229\n",
      " 21%|██▏       | 214/1000 [07:46<34:01,  2.60s/it]INFO:root:global_step: 214, logpy: -254.030, kl: 13.835, loss: 256.019\n",
      " 22%|██▏       | 215/1000 [07:49<34:00,  2.60s/it]INFO:root:global_step: 215, logpy: -250.866, kl: 13.783, loss: 252.872\n",
      " 22%|██▏       | 216/1000 [07:51<34:07,  2.61s/it]INFO:root:global_step: 216, logpy: -247.731, kl: 13.731, loss: 249.754\n",
      " 22%|██▏       | 217/1000 [07:54<33:57,  2.60s/it]INFO:root:global_step: 217, logpy: -244.689, kl: 13.681, loss: 246.731\n",
      " 22%|██▏       | 218/1000 [07:56<33:52,  2.60s/it]INFO:root:global_step: 218, logpy: -241.609, kl: 13.630, loss: 243.667\n",
      " 22%|██▏       | 219/1000 [07:59<33:42,  2.59s/it]INFO:root:global_step: 219, logpy: -238.633, kl: 13.584, loss: 240.710\n",
      " 22%|██▏       | 220/1000 [08:02<33:29,  2.58s/it]INFO:root:global_step: 220, logpy: -235.611, kl: 13.537, loss: 237.707\n",
      " 22%|██▏       | 221/1000 [08:04<33:18,  2.57s/it]INFO:root:global_step: 221, logpy: -232.613, kl: 13.489, loss: 234.727\n",
      " 22%|██▏       | 222/1000 [08:07<33:47,  2.61s/it]INFO:root:global_step: 222, logpy: -229.652, kl: 13.440, loss: 231.783\n",
      " 22%|██▏       | 223/1000 [08:10<33:59,  2.63s/it]INFO:root:global_step: 223, logpy: -226.668, kl: 13.392, loss: 228.817\n",
      " 22%|██▏       | 224/1000 [08:12<33:51,  2.62s/it]INFO:root:global_step: 224, logpy: -223.796, kl: 13.349, loss: 225.964\n",
      " 22%|██▎       | 225/1000 [08:15<33:47,  2.62s/it]INFO:root:global_step: 225, logpy: -220.878, kl: 13.304, loss: 223.064\n",
      " 23%|██▎       | 226/1000 [08:17<33:39,  2.61s/it]INFO:root:global_step: 226, logpy: -218.004, kl: 13.260, loss: 220.209\n",
      " 23%|██▎       | 227/1000 [08:20<33:35,  2.61s/it]INFO:root:global_step: 227, logpy: -215.167, kl: 13.217, loss: 217.391\n",
      " 23%|██▎       | 228/1000 [08:23<33:33,  2.61s/it]INFO:root:global_step: 228, logpy: -212.366, kl: 13.175, loss: 214.609\n",
      " 23%|██▎       | 229/1000 [08:25<33:23,  2.60s/it]INFO:root:global_step: 229, logpy: -209.569, kl: 13.133, loss: 211.830\n",
      " 23%|██▎       | 230/1000 [08:28<33:16,  2.59s/it]INFO:root:global_step: 230, logpy: -206.794, kl: 13.090, loss: 209.074\n",
      " 23%|██▎       | 231/1000 [08:30<33:14,  2.59s/it]INFO:root:global_step: 231, logpy: -204.002, kl: 13.048, loss: 206.300\n",
      " 23%|██▎       | 232/1000 [08:33<33:25,  2.61s/it]INFO:root:global_step: 232, logpy: -201.302, kl: 13.009, loss: 203.620\n",
      " 23%|██▎       | 233/1000 [08:36<33:37,  2.63s/it]INFO:root:global_step: 233, logpy: -198.685, kl: 12.970, loss: 201.023\n",
      " 23%|██▎       | 234/1000 [08:38<33:31,  2.63s/it]INFO:root:global_step: 234, logpy: -195.989, kl: 12.934, loss: 198.348\n",
      " 24%|██▎       | 235/1000 [08:41<33:30,  2.63s/it]INFO:root:global_step: 235, logpy: -193.298, kl: 12.894, loss: 195.674\n",
      " 24%|██▎       | 236/1000 [08:43<33:10,  2.61s/it]INFO:root:global_step: 236, logpy: -190.683, kl: 12.856, loss: 193.079\n",
      " 24%|██▎       | 237/1000 [08:46<33:07,  2.60s/it]INFO:root:global_step: 237, logpy: -188.004, kl: 12.821, loss: 190.421\n",
      " 24%|██▍       | 238/1000 [08:49<32:57,  2.60s/it]INFO:root:global_step: 238, logpy: -185.460, kl: 12.789, loss: 187.899\n",
      " 24%|██▍       | 239/1000 [08:51<32:47,  2.58s/it]INFO:root:global_step: 239, logpy: -182.853, kl: 12.752, loss: 185.310\n",
      " 24%|██▍       | 240/1000 [08:54<32:42,  2.58s/it]INFO:root:global_step: 240, logpy: -180.287, kl: 12.716, loss: 182.764\n",
      " 24%|██▍       | 241/1000 [08:56<32:46,  2.59s/it]INFO:root:global_step: 241, logpy: -177.812, kl: 12.683, loss: 180.310\n",
      " 24%|██▍       | 242/1000 [08:59<32:47,  2.60s/it]INFO:root:global_step: 242, logpy: -175.277, kl: 12.651, loss: 177.796\n",
      " 24%|██▍       | 243/1000 [09:02<32:48,  2.60s/it]INFO:root:global_step: 243, logpy: -172.825, kl: 12.618, loss: 175.365\n",
      " 24%|██▍       | 244/1000 [09:04<33:02,  2.62s/it]INFO:root:global_step: 244, logpy: -170.345, kl: 12.584, loss: 172.904\n",
      " 24%|██▍       | 245/1000 [09:07<33:11,  2.64s/it]INFO:root:global_step: 245, logpy: -167.861, kl: 12.551, loss: 170.441\n",
      " 25%|██▍       | 246/1000 [09:10<33:08,  2.64s/it]INFO:root:global_step: 246, logpy: -165.522, kl: 12.519, loss: 168.121\n",
      " 25%|██▍       | 247/1000 [09:12<33:13,  2.65s/it]INFO:root:global_step: 247, logpy: -163.195, kl: 12.489, loss: 165.816\n",
      " 25%|██▍       | 248/1000 [09:15<33:13,  2.65s/it]INFO:root:global_step: 248, logpy: -160.808, kl: 12.458, loss: 163.450\n",
      " 25%|██▍       | 249/1000 [09:17<33:09,  2.65s/it]INFO:root:global_step: 249, logpy: -158.551, kl: 12.429, loss: 161.214\n",
      " 25%|██▌       | 250/1000 [09:20<33:20,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_250.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 250, logpy: -156.313, kl: 12.401, loss: 158.998\n",
      " 25%|██▌       | 251/1000 [09:24<37:48,  3.03s/it]INFO:root:global_step: 251, logpy: -153.947, kl: 12.372, loss: 156.653\n",
      " 25%|██▌       | 252/1000 [09:27<36:41,  2.94s/it]INFO:root:global_step: 252, logpy: -151.788, kl: 12.345, loss: 154.515\n",
      " 25%|██▌       | 253/1000 [09:30<35:40,  2.87s/it]INFO:root:global_step: 253, logpy: -149.580, kl: 12.317, loss: 152.329\n",
      " 25%|██▌       | 254/1000 [09:32<35:45,  2.88s/it]INFO:root:global_step: 254, logpy: -147.270, kl: 12.290, loss: 150.040\n",
      " 26%|██▌       | 255/1000 [09:35<35:18,  2.84s/it]INFO:root:global_step: 255, logpy: -145.143, kl: 12.263, loss: 147.935\n",
      " 26%|██▌       | 256/1000 [09:38<34:38,  2.79s/it]INFO:root:global_step: 256, logpy: -142.955, kl: 12.237, loss: 145.768\n",
      " 26%|██▌       | 257/1000 [09:41<34:16,  2.77s/it]INFO:root:global_step: 257, logpy: -140.729, kl: 12.210, loss: 143.564\n",
      " 26%|██▌       | 258/1000 [09:43<34:23,  2.78s/it]INFO:root:global_step: 258, logpy: -138.533, kl: 12.188, loss: 141.391\n",
      " 26%|██▌       | 259/1000 [09:46<34:30,  2.79s/it]INFO:root:global_step: 259, logpy: -136.359, kl: 12.164, loss: 139.240\n",
      " 26%|██▌       | 260/1000 [09:49<34:06,  2.77s/it]INFO:root:global_step: 260, logpy: -134.176, kl: 12.135, loss: 137.076\n",
      " 26%|██▌       | 261/1000 [09:52<33:46,  2.74s/it]INFO:root:global_step: 261, logpy: -132.041, kl: 12.111, loss: 134.963\n",
      " 26%|██▌       | 262/1000 [09:54<33:33,  2.73s/it]INFO:root:global_step: 262, logpy: -129.944, kl: 12.090, loss: 132.889\n",
      " 26%|██▋       | 263/1000 [09:57<33:18,  2.71s/it]INFO:root:global_step: 263, logpy: -127.831, kl: 12.065, loss: 130.798\n",
      " 26%|██▋       | 264/1000 [10:00<33:17,  2.71s/it]INFO:root:global_step: 264, logpy: -125.767, kl: 12.044, loss: 128.756\n",
      " 26%|██▋       | 265/1000 [10:03<33:41,  2.75s/it]INFO:root:global_step: 265, logpy: -123.692, kl: 12.022, loss: 126.704\n",
      " 27%|██▋       | 266/1000 [10:05<34:03,  2.78s/it]INFO:root:global_step: 266, logpy: -121.582, kl: 12.001, loss: 124.617\n",
      " 27%|██▋       | 267/1000 [10:08<33:56,  2.78s/it]INFO:root:global_step: 267, logpy: -119.528, kl: 11.981, loss: 122.586\n",
      " 27%|██▋       | 268/1000 [10:11<33:40,  2.76s/it]INFO:root:global_step: 268, logpy: -117.547, kl: 11.962, loss: 120.629\n",
      " 27%|██▋       | 269/1000 [10:14<33:26,  2.74s/it]INFO:root:global_step: 269, logpy: -115.576, kl: 11.946, loss: 118.683\n",
      " 27%|██▋       | 270/1000 [10:16<33:07,  2.72s/it]INFO:root:global_step: 270, logpy: -113.553, kl: 11.925, loss: 116.682\n",
      " 27%|██▋       | 271/1000 [10:19<33:10,  2.73s/it]INFO:root:global_step: 271, logpy: -111.580, kl: 11.906, loss: 114.733\n",
      " 27%|██▋       | 272/1000 [10:22<33:06,  2.73s/it]INFO:root:global_step: 272, logpy: -109.648, kl: 11.888, loss: 112.825\n",
      " 27%|██▋       | 273/1000 [10:24<32:55,  2.72s/it]INFO:root:global_step: 273, logpy: -107.687, kl: 11.874, loss: 110.889\n",
      " 27%|██▋       | 274/1000 [10:27<32:47,  2.71s/it]INFO:root:global_step: 274, logpy: -105.772, kl: 11.856, loss: 108.997\n",
      " 28%|██▊       | 275/1000 [10:30<32:44,  2.71s/it]INFO:root:global_step: 275, logpy: -103.881, kl: 11.838, loss: 107.129\n",
      " 28%|██▊       | 276/1000 [10:33<33:02,  2.74s/it]INFO:root:global_step: 276, logpy: -102.005, kl: 11.820, loss: 105.277\n",
      " 28%|██▊       | 277/1000 [10:35<32:44,  2.72s/it]INFO:root:global_step: 277, logpy: -100.160, kl: 11.808, loss: 103.458\n",
      " 28%|██▊       | 278/1000 [10:38<32:42,  2.72s/it]INFO:root:global_step: 278, logpy: -98.364, kl: 11.792, loss: 101.686\n",
      " 28%|██▊       | 279/1000 [10:41<32:34,  2.71s/it]INFO:root:global_step: 279, logpy: -96.480, kl: 11.778, loss: 99.827\n",
      " 28%|██▊       | 280/1000 [10:43<32:33,  2.71s/it]INFO:root:global_step: 280, logpy: -94.657, kl: 11.766, loss: 98.030\n",
      " 28%|██▊       | 281/1000 [10:46<32:40,  2.73s/it]INFO:root:global_step: 281, logpy: -92.876, kl: 11.751, loss: 96.274\n",
      " 28%|██▊       | 282/1000 [10:49<32:26,  2.71s/it]INFO:root:global_step: 282, logpy: -91.115, kl: 11.741, loss: 94.539\n",
      " 28%|██▊       | 283/1000 [10:52<32:21,  2.71s/it]INFO:root:global_step: 283, logpy: -89.340, kl: 11.728, loss: 92.789\n",
      " 28%|██▊       | 284/1000 [10:54<32:29,  2.72s/it]INFO:root:global_step: 284, logpy: -87.596, kl: 11.715, loss: 91.070\n",
      " 28%|██▊       | 285/1000 [10:57<32:25,  2.72s/it]INFO:root:global_step: 285, logpy: -85.814, kl: 11.703, loss: 89.314\n",
      " 29%|██▊       | 286/1000 [11:00<32:26,  2.73s/it]INFO:root:global_step: 286, logpy: -84.135, kl: 11.690, loss: 87.659\n",
      " 29%|██▊       | 287/1000 [11:03<32:39,  2.75s/it]INFO:root:global_step: 287, logpy: -82.465, kl: 11.679, loss: 86.015\n",
      " 29%|██▉       | 288/1000 [11:05<32:30,  2.74s/it]INFO:root:global_step: 288, logpy: -80.738, kl: 11.667, loss: 84.313\n",
      " 29%|██▉       | 289/1000 [11:08<32:25,  2.74s/it]INFO:root:global_step: 289, logpy: -79.107, kl: 11.656, loss: 82.707\n",
      " 29%|██▉       | 290/1000 [11:11<32:25,  2.74s/it]INFO:root:global_step: 290, logpy: -77.473, kl: 11.642, loss: 81.098\n",
      " 29%|██▉       | 291/1000 [11:14<32:26,  2.75s/it]INFO:root:global_step: 291, logpy: -75.796, kl: 11.633, loss: 79.447\n",
      " 29%|██▉       | 292/1000 [11:16<32:20,  2.74s/it]INFO:root:global_step: 292, logpy: -74.221, kl: 11.623, loss: 77.898\n",
      " 29%|██▉       | 293/1000 [11:19<32:14,  2.74s/it]INFO:root:global_step: 293, logpy: -72.592, kl: 11.612, loss: 76.294\n",
      " 29%|██▉       | 294/1000 [11:22<32:11,  2.74s/it]INFO:root:global_step: 294, logpy: -71.009, kl: 11.605, loss: 74.738\n",
      " 30%|██▉       | 295/1000 [11:24<32:08,  2.74s/it]INFO:root:global_step: 295, logpy: -69.563, kl: 11.602, loss: 73.321\n",
      " 30%|██▉       | 296/1000 [11:27<32:01,  2.73s/it]INFO:root:global_step: 296, logpy: -68.007, kl: 11.595, loss: 71.793\n",
      " 30%|██▉       | 297/1000 [11:30<32:13,  2.75s/it]INFO:root:global_step: 297, logpy: -66.409, kl: 11.586, loss: 70.221\n",
      " 30%|██▉       | 298/1000 [11:33<32:21,  2.77s/it]INFO:root:global_step: 298, logpy: -64.916, kl: 11.584, loss: 68.757\n",
      " 30%|██▉       | 299/1000 [11:36<32:15,  2.76s/it]INFO:root:global_step: 299, logpy: -63.432, kl: 11.577, loss: 67.300\n",
      " 30%|███       | 300/1000 [11:38<32:10,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_300.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 300, logpy: -61.894, kl: 11.571, loss: 65.790\n",
      " 30%|███       | 301/1000 [11:42<36:52,  3.16s/it]INFO:root:global_step: 301, logpy: -60.500, kl: 11.568, loss: 64.425\n",
      " 30%|███       | 302/1000 [11:45<35:23,  3.04s/it]INFO:root:global_step: 302, logpy: -58.991, kl: 11.563, loss: 62.944\n",
      " 30%|███       | 303/1000 [11:48<35:03,  3.02s/it]INFO:root:global_step: 303, logpy: -57.553, kl: 11.557, loss: 61.533\n",
      " 30%|███       | 304/1000 [11:51<34:39,  2.99s/it]INFO:root:global_step: 304, logpy: -56.113, kl: 11.554, loss: 60.122\n",
      " 30%|███       | 305/1000 [11:54<33:44,  2.91s/it]INFO:root:global_step: 305, logpy: -54.621, kl: 11.549, loss: 58.657\n",
      " 31%|███       | 306/1000 [11:57<33:12,  2.87s/it]INFO:root:global_step: 306, logpy: -53.175, kl: 11.543, loss: 57.239\n",
      " 31%|███       | 307/1000 [11:59<32:52,  2.85s/it]INFO:root:global_step: 307, logpy: -51.742, kl: 11.544, loss: 55.836\n",
      " 31%|███       | 308/1000 [12:02<32:23,  2.81s/it]INFO:root:global_step: 308, logpy: -50.285, kl: 11.539, loss: 54.407\n",
      " 31%|███       | 309/1000 [12:05<32:24,  2.81s/it]INFO:root:global_step: 309, logpy: -48.871, kl: 11.535, loss: 53.021\n",
      " 31%|███       | 310/1000 [12:08<32:19,  2.81s/it]INFO:root:global_step: 310, logpy: -47.426, kl: 11.534, loss: 51.605\n",
      " 31%|███       | 311/1000 [12:10<32:23,  2.82s/it]INFO:root:global_step: 311, logpy: -46.107, kl: 11.535, loss: 50.317\n",
      " 31%|███       | 312/1000 [12:13<32:44,  2.86s/it]INFO:root:global_step: 312, logpy: -44.845, kl: 11.536, loss: 49.085\n",
      " 31%|███▏      | 313/1000 [12:16<32:47,  2.86s/it]INFO:root:global_step: 313, logpy: -43.447, kl: 11.533, loss: 47.716\n",
      " 31%|███▏      | 314/1000 [12:19<32:42,  2.86s/it]INFO:root:global_step: 314, logpy: -42.139, kl: 11.532, loss: 46.437\n",
      " 32%|███▏      | 315/1000 [12:22<32:28,  2.84s/it]INFO:root:global_step: 315, logpy: -40.842, kl: 11.530, loss: 45.169\n",
      " 32%|███▏      | 316/1000 [12:25<32:08,  2.82s/it]INFO:root:global_step: 316, logpy: -39.501, kl: 11.532, loss: 43.859\n",
      " 32%|███▏      | 317/1000 [12:27<31:48,  2.79s/it]INFO:root:global_step: 317, logpy: -38.202, kl: 11.535, loss: 42.591\n",
      " 32%|███▏      | 318/1000 [12:30<31:55,  2.81s/it]INFO:root:global_step: 318, logpy: -36.909, kl: 11.535, loss: 41.328\n",
      " 32%|███▏      | 319/1000 [12:33<32:10,  2.84s/it]INFO:root:global_step: 319, logpy: -35.600, kl: 11.536, loss: 40.049\n",
      " 32%|███▏      | 320/1000 [12:36<32:14,  2.84s/it]INFO:root:global_step: 320, logpy: -34.298, kl: 11.534, loss: 38.776\n",
      " 32%|███▏      | 321/1000 [12:39<32:12,  2.85s/it]INFO:root:global_step: 321, logpy: -32.998, kl: 11.533, loss: 37.505\n",
      " 32%|███▏      | 322/1000 [12:42<31:51,  2.82s/it]INFO:root:global_step: 322, logpy: -31.679, kl: 11.537, loss: 36.218\n",
      " 32%|███▏      | 323/1000 [12:44<31:30,  2.79s/it]INFO:root:global_step: 323, logpy: -30.431, kl: 11.536, loss: 34.999\n",
      " 32%|███▏      | 324/1000 [12:47<31:30,  2.80s/it]INFO:root:global_step: 324, logpy: -29.179, kl: 11.536, loss: 33.776\n",
      " 32%|███▎      | 325/1000 [12:50<31:24,  2.79s/it]INFO:root:global_step: 325, logpy: -27.983, kl: 11.540, loss: 32.611\n",
      " 33%|███▎      | 326/1000 [12:53<31:13,  2.78s/it]INFO:root:global_step: 326, logpy: -26.769, kl: 11.539, loss: 31.426\n",
      " 33%|███▎      | 327/1000 [12:56<31:16,  2.79s/it]INFO:root:global_step: 327, logpy: -25.529, kl: 11.540, loss: 30.216\n",
      " 33%|███▎      | 328/1000 [12:58<31:08,  2.78s/it]INFO:root:global_step: 328, logpy: -24.352, kl: 11.542, loss: 29.069\n",
      " 33%|███▎      | 329/1000 [13:01<31:13,  2.79s/it]INFO:root:global_step: 329, logpy: -23.199, kl: 11.547, loss: 27.949\n",
      " 33%|███▎      | 330/1000 [13:04<31:04,  2.78s/it]INFO:root:global_step: 330, logpy: -22.033, kl: 11.548, loss: 26.812\n",
      " 33%|███▎      | 331/1000 [13:07<31:09,  2.79s/it]INFO:root:global_step: 331, logpy: -20.832, kl: 11.554, loss: 25.644\n",
      " 33%|███▎      | 332/1000 [13:10<31:15,  2.81s/it]INFO:root:global_step: 332, logpy: -19.646, kl: 11.556, loss: 24.488\n",
      " 33%|███▎      | 333/1000 [13:12<31:22,  2.82s/it]INFO:root:global_step: 333, logpy: -18.456, kl: 11.556, loss: 23.327\n",
      " 33%|███▎      | 334/1000 [13:15<31:12,  2.81s/it]INFO:root:global_step: 334, logpy: -17.322, kl: 11.563, loss: 22.226\n",
      " 34%|███▎      | 335/1000 [13:18<31:09,  2.81s/it]INFO:root:global_step: 335, logpy: -16.125, kl: 11.567, loss: 21.060\n",
      " 34%|███▎      | 336/1000 [13:21<31:03,  2.81s/it]INFO:root:global_step: 336, logpy: -14.933, kl: 11.569, loss: 19.899\n",
      " 34%|███▎      | 337/1000 [13:24<31:01,  2.81s/it]INFO:root:global_step: 337, logpy: -13.763, kl: 11.573, loss: 18.760\n",
      " 34%|███▍      | 338/1000 [13:26<31:08,  2.82s/it]INFO:root:global_step: 338, logpy: -12.637, kl: 11.579, loss: 17.666\n",
      " 34%|███▍      | 339/1000 [13:29<30:58,  2.81s/it]INFO:root:global_step: 339, logpy: -11.550, kl: 11.584, loss: 16.611\n",
      " 34%|███▍      | 340/1000 [13:32<30:55,  2.81s/it]INFO:root:global_step: 340, logpy: -10.401, kl: 11.588, loss: 15.494\n",
      " 34%|███▍      | 341/1000 [13:35<30:57,  2.82s/it]INFO:root:global_step: 341, logpy: -9.300, kl: 11.592, loss: 14.423\n",
      " 34%|███▍      | 342/1000 [13:38<30:45,  2.80s/it]INFO:root:global_step: 342, logpy: -8.230, kl: 11.599, loss: 13.387\n",
      " 34%|███▍      | 343/1000 [13:41<31:09,  2.85s/it]INFO:root:global_step: 343, logpy: -7.141, kl: 11.607, loss: 12.332\n",
      " 34%|███▍      | 344/1000 [13:44<31:24,  2.87s/it]INFO:root:global_step: 344, logpy: -6.056, kl: 11.610, loss: 11.276\n",
      " 34%|███▍      | 345/1000 [13:47<31:48,  2.91s/it]INFO:root:global_step: 345, logpy: -4.978, kl: 11.618, loss: 10.233\n",
      " 35%|███▍      | 346/1000 [13:49<31:46,  2.92s/it]INFO:root:global_step: 346, logpy: -3.922, kl: 11.627, loss: 9.211\n",
      " 35%|███▍      | 347/1000 [13:52<31:19,  2.88s/it]INFO:root:global_step: 347, logpy: -2.852, kl: 11.633, loss: 8.173\n",
      " 35%|███▍      | 348/1000 [13:55<31:14,  2.87s/it]INFO:root:global_step: 348, logpy: -1.804, kl: 11.638, loss: 7.157\n",
      " 35%|███▍      | 349/1000 [13:58<31:12,  2.88s/it]INFO:root:global_step: 349, logpy: -0.771, kl: 11.645, loss: 6.157\n",
      " 35%|███▌      | 350/1000 [14:01<31:06,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_350.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 350, logpy: 0.267, kl: 11.652, loss: 5.151\n",
      " 35%|███▌      | 351/1000 [14:05<35:14,  3.26s/it]INFO:root:global_step: 351, logpy: 1.297, kl: 11.657, loss: 4.153\n",
      " 35%|███▌      | 352/1000 [14:08<34:17,  3.18s/it]INFO:root:global_step: 352, logpy: 2.296, kl: 11.662, loss: 3.185\n",
      " 35%|███▌      | 353/1000 [14:11<33:43,  3.13s/it]INFO:root:global_step: 353, logpy: 3.272, kl: 11.669, loss: 2.241\n",
      " 35%|███▌      | 354/1000 [14:14<32:51,  3.05s/it]INFO:root:global_step: 354, logpy: 4.193, kl: 11.677, loss: 1.354\n",
      " 36%|███▌      | 355/1000 [14:17<32:18,  3.01s/it]INFO:root:global_step: 355, logpy: 5.124, kl: 11.687, loss: 0.457\n",
      " 36%|███▌      | 356/1000 [14:20<32:08,  2.99s/it]INFO:root:global_step: 356, logpy: 6.062, kl: 11.696, loss: -0.446\n",
      " 36%|███▌      | 357/1000 [14:23<31:39,  2.95s/it]INFO:root:global_step: 357, logpy: 6.950, kl: 11.707, loss: -1.299\n",
      " 36%|███▌      | 358/1000 [14:26<31:24,  2.93s/it]INFO:root:global_step: 358, logpy: 7.901, kl: 11.714, loss: -2.217\n",
      " 36%|███▌      | 359/1000 [14:28<31:16,  2.93s/it]INFO:root:global_step: 359, logpy: 8.827, kl: 11.721, loss: -3.111\n",
      " 36%|███▌      | 360/1000 [14:31<31:05,  2.92s/it]INFO:root:global_step: 360, logpy: 9.713, kl: 11.732, loss: -3.962\n",
      " 36%|███▌      | 361/1000 [14:34<30:54,  2.90s/it]INFO:root:global_step: 361, logpy: 10.641, kl: 11.741, loss: -4.856\n",
      " 36%|███▌      | 362/1000 [14:37<30:44,  2.89s/it]INFO:root:global_step: 362, logpy: 11.548, kl: 11.750, loss: -5.729\n",
      " 36%|███▋      | 363/1000 [14:40<30:54,  2.91s/it]INFO:root:global_step: 363, logpy: 12.409, kl: 11.759, loss: -6.556\n",
      " 36%|███▋      | 364/1000 [14:43<30:39,  2.89s/it]INFO:root:global_step: 364, logpy: 13.331, kl: 11.769, loss: -7.443\n",
      " 36%|███▋      | 365/1000 [14:46<30:28,  2.88s/it]INFO:root:global_step: 365, logpy: 14.226, kl: 11.779, loss: -8.304\n",
      " 37%|███▋      | 366/1000 [14:49<30:11,  2.86s/it]INFO:root:global_step: 366, logpy: 15.136, kl: 11.791, loss: -9.178\n",
      " 37%|███▋      | 367/1000 [14:51<30:12,  2.86s/it]INFO:root:global_step: 367, logpy: 16.049, kl: 11.801, loss: -10.056\n",
      " 37%|███▋      | 368/1000 [14:54<30:15,  2.87s/it]INFO:root:global_step: 368, logpy: 16.921, kl: 11.815, loss: -10.890\n",
      " 37%|███▋      | 369/1000 [14:57<30:16,  2.88s/it]INFO:root:global_step: 369, logpy: 17.829, kl: 11.824, loss: -11.765\n",
      " 37%|███▋      | 370/1000 [15:00<30:22,  2.89s/it]INFO:root:global_step: 370, logpy: 18.738, kl: 11.837, loss: -12.636\n",
      " 37%|███▋      | 371/1000 [15:03<30:19,  2.89s/it]INFO:root:global_step: 371, logpy: 19.595, kl: 11.849, loss: -13.457\n",
      " 37%|███▋      | 372/1000 [15:06<30:11,  2.88s/it]INFO:root:global_step: 372, logpy: 20.487, kl: 11.862, loss: -14.313\n",
      " 37%|███▋      | 373/1000 [15:09<30:05,  2.88s/it]INFO:root:global_step: 373, logpy: 21.296, kl: 11.876, loss: -15.085\n",
      " 37%|███▋      | 374/1000 [15:12<30:25,  2.92s/it]INFO:root:global_step: 374, logpy: 22.164, kl: 11.887, loss: -15.917\n",
      " 38%|███▊      | 375/1000 [15:15<30:14,  2.90s/it]INFO:root:global_step: 375, logpy: 23.044, kl: 11.901, loss: -16.760\n",
      " 38%|███▊      | 376/1000 [15:17<30:07,  2.90s/it]INFO:root:global_step: 376, logpy: 23.791, kl: 11.913, loss: -17.471\n",
      " 38%|███▊      | 377/1000 [15:20<30:04,  2.90s/it]INFO:root:global_step: 377, logpy: 24.609, kl: 11.926, loss: -18.253\n",
      " 38%|███▊      | 378/1000 [15:23<30:03,  2.90s/it]INFO:root:global_step: 378, logpy: 25.404, kl: 11.937, loss: -19.012\n",
      " 38%|███▊      | 379/1000 [15:26<30:03,  2.90s/it]INFO:root:global_step: 379, logpy: 26.173, kl: 11.948, loss: -19.746\n",
      " 38%|███▊      | 380/1000 [15:29<29:58,  2.90s/it]INFO:root:global_step: 380, logpy: 27.066, kl: 11.958, loss: -20.605\n",
      " 38%|███▊      | 381/1000 [15:32<29:55,  2.90s/it]INFO:root:global_step: 381, logpy: 27.859, kl: 11.969, loss: -21.362\n",
      " 38%|███▊      | 382/1000 [15:35<29:47,  2.89s/it]INFO:root:global_step: 382, logpy: 28.659, kl: 11.980, loss: -22.127\n",
      " 38%|███▊      | 383/1000 [15:38<29:37,  2.88s/it]INFO:root:global_step: 383, logpy: 29.480, kl: 11.990, loss: -22.913\n",
      " 38%|███▊      | 384/1000 [15:41<29:39,  2.89s/it]INFO:root:global_step: 384, logpy: 30.296, kl: 12.004, loss: -23.693\n",
      " 38%|███▊      | 385/1000 [15:44<30:14,  2.95s/it]INFO:root:global_step: 385, logpy: 31.105, kl: 12.018, loss: -24.464\n",
      " 39%|███▊      | 386/1000 [15:47<30:18,  2.96s/it]INFO:root:global_step: 386, logpy: 31.875, kl: 12.032, loss: -25.197\n",
      " 39%|███▊      | 387/1000 [15:50<30:31,  2.99s/it]INFO:root:global_step: 387, logpy: 32.638, kl: 12.048, loss: -25.921\n",
      " 39%|███▉      | 388/1000 [15:53<30:12,  2.96s/it]INFO:root:global_step: 388, logpy: 33.397, kl: 12.063, loss: -26.641\n",
      " 39%|███▉      | 389/1000 [15:56<30:34,  3.00s/it]INFO:root:global_step: 389, logpy: 34.166, kl: 12.074, loss: -27.376\n",
      " 39%|███▉      | 390/1000 [15:59<30:43,  3.02s/it]INFO:root:global_step: 390, logpy: 34.936, kl: 12.087, loss: -28.108\n",
      " 39%|███▉      | 391/1000 [16:02<30:32,  3.01s/it]INFO:root:global_step: 391, logpy: 35.706, kl: 12.098, loss: -28.843\n",
      " 39%|███▉      | 392/1000 [16:05<31:24,  3.10s/it]INFO:root:global_step: 392, logpy: 36.443, kl: 12.110, loss: -29.544\n",
      " 39%|███▉      | 393/1000 [16:08<30:50,  3.05s/it]INFO:root:global_step: 393, logpy: 37.217, kl: 12.124, loss: -30.282\n",
      " 39%|███▉      | 394/1000 [16:11<30:53,  3.06s/it]INFO:root:global_step: 394, logpy: 37.977, kl: 12.134, loss: -31.007\n",
      " 40%|███▉      | 395/1000 [16:14<30:52,  3.06s/it]INFO:root:global_step: 395, logpy: 38.717, kl: 12.150, loss: -31.708\n",
      " 40%|███▉      | 396/1000 [16:17<30:43,  3.05s/it]INFO:root:global_step: 396, logpy: 39.449, kl: 12.158, loss: -32.407\n",
      " 40%|███▉      | 397/1000 [16:20<30:19,  3.02s/it]INFO:root:global_step: 397, logpy: 40.182, kl: 12.174, loss: -33.101\n",
      " 40%|███▉      | 398/1000 [16:23<30:04,  3.00s/it]INFO:root:global_step: 398, logpy: 40.898, kl: 12.190, loss: -33.778\n",
      " 40%|███▉      | 399/1000 [16:26<29:55,  2.99s/it]INFO:root:global_step: 399, logpy: 41.597, kl: 12.204, loss: -34.439\n",
      " 40%|████      | 400/1000 [16:29<29:42,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_400.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 400, logpy: 42.362, kl: 12.215, loss: -35.169\n",
      " 40%|████      | 401/1000 [16:33<33:13,  3.33s/it]INFO:root:global_step: 401, logpy: 43.057, kl: 12.230, loss: -35.825\n",
      " 40%|████      | 402/1000 [16:36<32:30,  3.26s/it]INFO:root:global_step: 402, logpy: 43.758, kl: 12.244, loss: -36.489\n",
      " 40%|████      | 403/1000 [16:39<31:43,  3.19s/it]INFO:root:global_step: 403, logpy: 44.469, kl: 12.257, loss: -37.164\n",
      " 40%|████      | 404/1000 [16:42<30:51,  3.11s/it]INFO:root:global_step: 404, logpy: 45.227, kl: 12.270, loss: -37.884\n",
      " 40%|████      | 405/1000 [16:45<30:08,  3.04s/it]INFO:root:global_step: 405, logpy: 45.890, kl: 12.285, loss: -38.510\n",
      " 41%|████      | 406/1000 [16:48<30:07,  3.04s/it]INFO:root:global_step: 406, logpy: 46.595, kl: 12.296, loss: -39.179\n",
      " 41%|████      | 407/1000 [16:51<29:50,  3.02s/it]INFO:root:global_step: 407, logpy: 47.213, kl: 12.311, loss: -39.759\n",
      " 41%|████      | 408/1000 [16:54<29:35,  3.00s/it]INFO:root:global_step: 408, logpy: 47.863, kl: 12.327, loss: -40.369\n",
      " 41%|████      | 409/1000 [16:57<29:17,  2.97s/it]INFO:root:global_step: 409, logpy: 48.494, kl: 12.347, loss: -40.958\n",
      " 41%|████      | 410/1000 [17:00<29:03,  2.95s/it]INFO:root:global_step: 410, logpy: 49.159, kl: 12.362, loss: -41.585\n",
      " 41%|████      | 411/1000 [17:03<28:55,  2.95s/it]INFO:root:global_step: 411, logpy: 49.765, kl: 12.375, loss: -42.153\n",
      " 41%|████      | 412/1000 [17:06<29:01,  2.96s/it]INFO:root:global_step: 412, logpy: 50.381, kl: 12.391, loss: -42.730\n",
      " 41%|████▏     | 413/1000 [17:09<28:54,  2.95s/it]INFO:root:global_step: 413, logpy: 51.006, kl: 12.404, loss: -43.319\n",
      " 41%|████▏     | 414/1000 [17:12<29:00,  2.97s/it]INFO:root:global_step: 414, logpy: 51.670, kl: 12.419, loss: -43.944\n",
      " 42%|████▏     | 415/1000 [17:15<28:52,  2.96s/it]INFO:root:global_step: 415, logpy: 52.305, kl: 12.436, loss: -44.539\n",
      " 42%|████▏     | 416/1000 [17:18<28:42,  2.95s/it]INFO:root:global_step: 416, logpy: 52.958, kl: 12.452, loss: -45.152\n",
      " 42%|████▏     | 417/1000 [17:21<28:56,  2.98s/it]INFO:root:global_step: 417, logpy: 53.626, kl: 12.466, loss: -45.783\n",
      " 42%|████▏     | 418/1000 [17:24<28:39,  2.95s/it]INFO:root:global_step: 418, logpy: 54.225, kl: 12.481, loss: -46.343\n",
      " 42%|████▏     | 419/1000 [17:27<28:35,  2.95s/it]INFO:root:global_step: 419, logpy: 54.872, kl: 12.498, loss: -46.950\n",
      " 42%|████▏     | 420/1000 [17:29<28:31,  2.95s/it]INFO:root:global_step: 420, logpy: 55.479, kl: 12.510, loss: -47.520\n",
      " 42%|████▏     | 421/1000 [17:33<28:49,  2.99s/it]INFO:root:global_step: 421, logpy: 56.078, kl: 12.527, loss: -48.079\n",
      " 42%|████▏     | 422/1000 [17:36<28:43,  2.98s/it]INFO:root:global_step: 422, logpy: 56.619, kl: 12.545, loss: -48.580\n",
      " 42%|████▏     | 423/1000 [17:38<28:32,  2.97s/it]INFO:root:global_step: 423, logpy: 57.222, kl: 12.559, loss: -49.144\n",
      " 42%|████▏     | 424/1000 [17:41<28:34,  2.98s/it]INFO:root:global_step: 424, logpy: 57.853, kl: 12.571, loss: -49.740\n",
      " 42%|████▎     | 425/1000 [17:45<28:51,  3.01s/it]INFO:root:global_step: 425, logpy: 58.465, kl: 12.583, loss: -50.315\n",
      " 43%|████▎     | 426/1000 [17:48<28:45,  3.01s/it]INFO:root:global_step: 426, logpy: 59.095, kl: 12.597, loss: -50.908\n",
      " 43%|████▎     | 427/1000 [17:50<28:27,  2.98s/it]INFO:root:global_step: 427, logpy: 59.669, kl: 12.612, loss: -51.442\n",
      " 43%|████▎     | 428/1000 [17:53<28:34,  3.00s/it]INFO:root:global_step: 428, logpy: 60.267, kl: 12.626, loss: -52.002\n",
      " 43%|████▎     | 429/1000 [17:56<28:25,  2.99s/it]INFO:root:global_step: 429, logpy: 60.846, kl: 12.646, loss: -52.538\n",
      " 43%|████▎     | 430/1000 [17:59<28:10,  2.97s/it]INFO:root:global_step: 430, logpy: 61.426, kl: 12.662, loss: -53.079\n",
      " 43%|████▎     | 431/1000 [18:02<28:02,  2.96s/it]INFO:root:global_step: 431, logpy: 62.000, kl: 12.677, loss: -53.614\n",
      " 43%|████▎     | 432/1000 [18:05<28:03,  2.96s/it]INFO:root:global_step: 432, logpy: 62.536, kl: 12.694, loss: -54.109\n",
      " 43%|████▎     | 433/1000 [18:08<28:01,  2.97s/it]INFO:root:global_step: 433, logpy: 63.101, kl: 12.712, loss: -54.632\n",
      " 43%|████▎     | 434/1000 [18:11<28:01,  2.97s/it]INFO:root:global_step: 434, logpy: 63.636, kl: 12.729, loss: -55.126\n",
      " 44%|████▎     | 435/1000 [18:14<27:48,  2.95s/it]INFO:root:global_step: 435, logpy: 64.172, kl: 12.748, loss: -55.620\n",
      " 44%|████▎     | 436/1000 [18:17<27:46,  2.95s/it]INFO:root:global_step: 436, logpy: 64.678, kl: 12.768, loss: -56.083\n",
      " 44%|████▎     | 437/1000 [18:20<27:43,  2.95s/it]INFO:root:global_step: 437, logpy: 65.161, kl: 12.785, loss: -56.525\n",
      " 44%|████▍     | 438/1000 [18:23<27:44,  2.96s/it]INFO:root:global_step: 438, logpy: 65.715, kl: 12.802, loss: -57.038\n",
      " 44%|████▍     | 439/1000 [18:26<27:55,  2.99s/it]INFO:root:global_step: 439, logpy: 66.207, kl: 12.816, loss: -57.492\n",
      " 44%|████▍     | 440/1000 [18:29<27:45,  2.97s/it]INFO:root:global_step: 440, logpy: 66.712, kl: 12.832, loss: -57.958\n",
      " 44%|████▍     | 441/1000 [18:32<28:42,  3.08s/it]INFO:root:global_step: 441, logpy: 67.215, kl: 12.847, loss: -58.421\n",
      " 44%|████▍     | 442/1000 [18:35<28:49,  3.10s/it]INFO:root:global_step: 442, logpy: 67.751, kl: 12.862, loss: -58.918\n",
      " 44%|████▍     | 443/1000 [18:39<28:39,  3.09s/it]INFO:root:global_step: 443, logpy: 68.272, kl: 12.877, loss: -59.399\n",
      " 44%|████▍     | 444/1000 [18:42<28:22,  3.06s/it]INFO:root:global_step: 444, logpy: 68.812, kl: 12.893, loss: -59.900\n",
      " 44%|████▍     | 445/1000 [18:45<28:12,  3.05s/it]INFO:root:global_step: 445, logpy: 69.334, kl: 12.910, loss: -60.380\n",
      " 45%|████▍     | 446/1000 [18:48<27:57,  3.03s/it]INFO:root:global_step: 446, logpy: 69.865, kl: 12.928, loss: -60.870\n",
      " 45%|████▍     | 447/1000 [18:51<27:44,  3.01s/it]INFO:root:global_step: 447, logpy: 70.376, kl: 12.947, loss: -61.337\n",
      " 45%|████▍     | 448/1000 [18:54<27:37,  3.00s/it]INFO:root:global_step: 448, logpy: 70.860, kl: 12.967, loss: -61.778\n",
      " 45%|████▍     | 449/1000 [18:56<27:27,  2.99s/it]INFO:root:global_step: 449, logpy: 71.338, kl: 12.983, loss: -62.216\n",
      " 45%|████▌     | 450/1000 [19:00<27:45,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_450.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 450, logpy: 71.799, kl: 13.000, loss: -62.635\n",
      " 45%|████▌     | 451/1000 [19:04<31:03,  3.39s/it]INFO:root:global_step: 451, logpy: 72.196, kl: 13.017, loss: -62.991\n",
      " 45%|████▌     | 452/1000 [19:07<30:08,  3.30s/it]INFO:root:global_step: 452, logpy: 72.672, kl: 13.034, loss: -63.426\n",
      " 45%|████▌     | 453/1000 [19:10<29:51,  3.27s/it]INFO:root:global_step: 453, logpy: 73.166, kl: 13.050, loss: -63.879\n",
      " 45%|████▌     | 454/1000 [19:13<29:13,  3.21s/it]INFO:root:global_step: 454, logpy: 73.632, kl: 13.064, loss: -64.306\n",
      " 46%|████▌     | 455/1000 [19:16<28:35,  3.15s/it]INFO:root:global_step: 455, logpy: 74.096, kl: 13.080, loss: -64.730\n",
      " 46%|████▌     | 456/1000 [19:19<28:02,  3.09s/it]INFO:root:global_step: 456, logpy: 74.544, kl: 13.100, loss: -65.134\n",
      " 46%|████▌     | 457/1000 [19:22<27:43,  3.06s/it]INFO:root:global_step: 457, logpy: 75.039, kl: 13.115, loss: -65.589\n",
      " 46%|████▌     | 458/1000 [19:25<27:28,  3.04s/it]INFO:root:global_step: 458, logpy: 75.444, kl: 13.135, loss: -65.950\n",
      " 46%|████▌     | 459/1000 [19:28<27:22,  3.04s/it]INFO:root:global_step: 459, logpy: 75.816, kl: 13.150, loss: -66.283\n",
      " 46%|████▌     | 460/1000 [19:31<27:30,  3.06s/it]INFO:root:global_step: 460, logpy: 76.233, kl: 13.169, loss: -66.656\n",
      " 46%|████▌     | 461/1000 [19:34<27:11,  3.03s/it]INFO:root:global_step: 461, logpy: 76.658, kl: 13.187, loss: -67.039\n",
      " 46%|████▌     | 462/1000 [19:37<27:02,  3.02s/it]INFO:root:global_step: 462, logpy: 77.060, kl: 13.206, loss: -67.397\n",
      " 46%|████▋     | 463/1000 [19:40<26:56,  3.01s/it]INFO:root:global_step: 463, logpy: 77.492, kl: 13.227, loss: -67.783\n",
      " 46%|████▋     | 464/1000 [19:43<27:03,  3.03s/it]INFO:root:global_step: 464, logpy: 77.972, kl: 13.243, loss: -68.223\n",
      " 46%|████▋     | 465/1000 [19:46<27:23,  3.07s/it]INFO:root:global_step: 465, logpy: 78.435, kl: 13.258, loss: -68.646\n",
      " 47%|████▋     | 466/1000 [19:50<27:22,  3.08s/it]INFO:root:global_step: 466, logpy: 78.898, kl: 13.273, loss: -69.069\n",
      " 47%|████▋     | 467/1000 [19:53<27:08,  3.06s/it]INFO:root:global_step: 467, logpy: 79.322, kl: 13.289, loss: -69.452\n",
      " 47%|████▋     | 468/1000 [19:56<27:28,  3.10s/it]INFO:root:global_step: 468, logpy: 79.760, kl: 13.307, loss: -69.847\n",
      " 47%|████▋     | 469/1000 [19:59<27:46,  3.14s/it]INFO:root:global_step: 469, logpy: 80.165, kl: 13.322, loss: -70.213\n",
      " 47%|████▋     | 470/1000 [20:02<28:14,  3.20s/it]INFO:root:global_step: 470, logpy: 80.596, kl: 13.338, loss: -70.601\n",
      " 47%|████▋     | 471/1000 [20:05<28:03,  3.18s/it]INFO:root:global_step: 471, logpy: 81.007, kl: 13.353, loss: -70.973\n",
      " 47%|████▋     | 472/1000 [20:09<29:49,  3.39s/it]INFO:root:global_step: 472, logpy: 81.469, kl: 13.370, loss: -71.392\n",
      " 47%|████▋     | 473/1000 [20:13<29:27,  3.35s/it]INFO:root:global_step: 473, logpy: 81.864, kl: 13.388, loss: -71.745\n",
      " 47%|████▋     | 474/1000 [20:16<28:37,  3.26s/it]INFO:root:global_step: 474, logpy: 82.243, kl: 13.411, loss: -72.076\n",
      " 48%|████▊     | 475/1000 [20:19<28:24,  3.25s/it]INFO:root:global_step: 475, logpy: 82.654, kl: 13.432, loss: -72.441\n",
      " 48%|████▊     | 476/1000 [20:22<28:40,  3.28s/it]INFO:root:global_step: 476, logpy: 83.076, kl: 13.447, loss: -72.822\n",
      " 48%|████▊     | 477/1000 [20:26<28:45,  3.30s/it]INFO:root:global_step: 477, logpy: 83.488, kl: 13.467, loss: -73.189\n",
      " 48%|████▊     | 478/1000 [20:29<28:50,  3.32s/it]INFO:root:global_step: 478, logpy: 83.863, kl: 13.486, loss: -73.519\n",
      " 48%|████▊     | 479/1000 [20:32<28:18,  3.26s/it]INFO:root:global_step: 479, logpy: 84.291, kl: 13.508, loss: -73.900\n",
      " 48%|████▊     | 480/1000 [20:35<28:13,  3.26s/it]INFO:root:global_step: 480, logpy: 84.697, kl: 13.527, loss: -74.263\n",
      " 48%|████▊     | 481/1000 [20:38<27:54,  3.23s/it]INFO:root:global_step: 481, logpy: 85.113, kl: 13.545, loss: -74.635\n",
      " 48%|████▊     | 482/1000 [20:42<28:15,  3.27s/it]INFO:root:global_step: 482, logpy: 85.512, kl: 13.563, loss: -74.991\n",
      " 48%|████▊     | 483/1000 [20:45<28:14,  3.28s/it]INFO:root:global_step: 483, logpy: 85.931, kl: 13.584, loss: -75.363\n",
      " 48%|████▊     | 484/1000 [20:48<27:40,  3.22s/it]INFO:root:global_step: 484, logpy: 86.282, kl: 13.603, loss: -75.670\n",
      " 48%|████▊     | 485/1000 [20:51<27:23,  3.19s/it]INFO:root:global_step: 485, logpy: 86.629, kl: 13.620, loss: -75.975\n",
      " 49%|████▊     | 486/1000 [20:54<26:57,  3.15s/it]INFO:root:global_step: 486, logpy: 87.018, kl: 13.637, loss: -76.321\n",
      " 49%|████▊     | 487/1000 [20:57<26:36,  3.11s/it]INFO:root:global_step: 487, logpy: 87.361, kl: 13.658, loss: -76.617\n",
      " 49%|████▉     | 488/1000 [21:00<26:13,  3.07s/it]INFO:root:global_step: 488, logpy: 87.747, kl: 13.684, loss: -76.952\n",
      " 49%|████▉     | 489/1000 [21:03<25:52,  3.04s/it]INFO:root:global_step: 489, logpy: 88.112, kl: 13.701, loss: -77.274\n",
      " 49%|████▉     | 490/1000 [21:06<25:47,  3.03s/it]INFO:root:global_step: 490, logpy: 88.489, kl: 13.713, loss: -77.612\n",
      " 49%|████▉     | 491/1000 [21:09<25:51,  3.05s/it]INFO:root:global_step: 491, logpy: 88.830, kl: 13.732, loss: -77.908\n",
      " 49%|████▉     | 492/1000 [21:12<25:42,  3.04s/it]INFO:root:global_step: 492, logpy: 89.220, kl: 13.750, loss: -78.254\n",
      " 49%|████▉     | 493/1000 [21:16<25:50,  3.06s/it]INFO:root:global_step: 493, logpy: 89.561, kl: 13.769, loss: -78.551\n",
      " 49%|████▉     | 494/1000 [21:19<25:38,  3.04s/it]INFO:root:global_step: 494, logpy: 89.910, kl: 13.785, loss: -78.858\n",
      " 50%|████▉     | 495/1000 [21:22<25:43,  3.06s/it]INFO:root:global_step: 495, logpy: 90.255, kl: 13.807, loss: -79.155\n",
      " 50%|████▉     | 496/1000 [21:25<25:39,  3.05s/it]INFO:root:global_step: 496, logpy: 90.549, kl: 13.828, loss: -79.402\n",
      " 50%|████▉     | 497/1000 [21:28<25:40,  3.06s/it]INFO:root:global_step: 497, logpy: 90.903, kl: 13.849, loss: -79.708\n",
      " 50%|████▉     | 498/1000 [21:31<26:10,  3.13s/it]INFO:root:global_step: 498, logpy: 91.185, kl: 13.866, loss: -79.948\n",
      " 50%|████▉     | 499/1000 [21:34<26:03,  3.12s/it]INFO:root:global_step: 499, logpy: 91.518, kl: 13.885, loss: -80.236\n",
      " 50%|█████     | 500/1000 [21:38<26:35,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_500.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 500, logpy: 91.844, kl: 13.905, loss: -80.515\n",
      " 50%|█████     | 501/1000 [21:42<30:22,  3.65s/it]INFO:root:global_step: 501, logpy: 92.173, kl: 13.924, loss: -80.799\n",
      " 50%|█████     | 502/1000 [21:46<29:16,  3.53s/it]INFO:root:global_step: 502, logpy: 92.537, kl: 13.943, loss: -81.119\n",
      " 50%|█████     | 503/1000 [21:49<28:29,  3.44s/it]INFO:root:global_step: 503, logpy: 92.824, kl: 13.963, loss: -81.360\n",
      " 50%|█████     | 504/1000 [21:52<27:56,  3.38s/it]INFO:root:global_step: 504, logpy: 93.162, kl: 13.979, loss: -81.658\n",
      " 50%|█████     | 505/1000 [21:55<27:19,  3.31s/it]INFO:root:global_step: 505, logpy: 93.508, kl: 13.997, loss: -81.961\n",
      " 51%|█████     | 506/1000 [21:58<26:39,  3.24s/it]INFO:root:global_step: 506, logpy: 93.831, kl: 14.011, loss: -82.244\n",
      " 51%|█████     | 507/1000 [22:01<26:06,  3.18s/it]INFO:root:global_step: 507, logpy: 94.161, kl: 14.028, loss: -82.533\n",
      " 51%|█████     | 508/1000 [22:04<25:52,  3.16s/it]INFO:root:global_step: 508, logpy: 94.469, kl: 14.049, loss: -82.796\n",
      " 51%|█████     | 509/1000 [22:07<25:31,  3.12s/it]INFO:root:global_step: 509, logpy: 94.777, kl: 14.063, loss: -83.068\n",
      " 51%|█████     | 510/1000 [22:11<26:08,  3.20s/it]INFO:root:global_step: 510, logpy: 95.110, kl: 14.081, loss: -83.358\n",
      " 51%|█████     | 511/1000 [22:14<26:12,  3.22s/it]INFO:root:global_step: 511, logpy: 95.433, kl: 14.100, loss: -83.639\n",
      " 51%|█████     | 512/1000 [22:17<26:04,  3.21s/it]INFO:root:global_step: 512, logpy: 95.785, kl: 14.119, loss: -83.950\n",
      " 51%|█████▏    | 513/1000 [22:21<26:29,  3.26s/it]INFO:root:global_step: 513, logpy: 96.109, kl: 14.134, loss: -84.235\n",
      " 51%|█████▏    | 514/1000 [22:24<27:07,  3.35s/it]INFO:root:global_step: 514, logpy: 96.397, kl: 14.154, loss: -84.480\n",
      " 52%|█████▏    | 515/1000 [22:28<27:10,  3.36s/it]INFO:root:global_step: 515, logpy: 96.730, kl: 14.172, loss: -84.773\n",
      " 52%|█████▏    | 516/1000 [22:31<27:00,  3.35s/it]INFO:root:global_step: 516, logpy: 97.059, kl: 14.185, loss: -85.067\n",
      " 52%|█████▏    | 517/1000 [22:34<26:25,  3.28s/it]INFO:root:global_step: 517, logpy: 97.374, kl: 14.207, loss: -85.339\n",
      " 52%|█████▏    | 518/1000 [22:37<25:45,  3.21s/it]INFO:root:global_step: 518, logpy: 97.664, kl: 14.224, loss: -85.590\n",
      " 52%|█████▏    | 519/1000 [22:40<25:20,  3.16s/it]INFO:root:global_step: 519, logpy: 98.015, kl: 14.245, loss: -85.898\n",
      " 52%|█████▏    | 520/1000 [22:43<25:03,  3.13s/it]INFO:root:global_step: 520, logpy: 98.304, kl: 14.263, loss: -86.148\n",
      " 52%|█████▏    | 521/1000 [22:46<24:53,  3.12s/it]INFO:root:global_step: 521, logpy: 98.613, kl: 14.280, loss: -86.419\n",
      " 52%|█████▏    | 522/1000 [22:49<24:58,  3.13s/it]INFO:root:global_step: 522, logpy: 98.960, kl: 14.302, loss: -86.723\n",
      " 52%|█████▏    | 523/1000 [22:53<24:50,  3.12s/it]INFO:root:global_step: 523, logpy: 99.225, kl: 14.320, loss: -86.949\n",
      " 52%|█████▏    | 524/1000 [22:56<24:46,  3.12s/it]INFO:root:global_step: 524, logpy: 99.506, kl: 14.337, loss: -87.193\n",
      " 52%|█████▎    | 525/1000 [22:59<24:55,  3.15s/it]INFO:root:global_step: 525, logpy: 99.781, kl: 14.354, loss: -87.431\n",
      " 53%|█████▎    | 526/1000 [23:02<24:50,  3.15s/it]INFO:root:global_step: 526, logpy: 100.069, kl: 14.373, loss: -87.680\n",
      " 53%|█████▎    | 527/1000 [23:05<24:32,  3.11s/it]INFO:root:global_step: 527, logpy: 100.347, kl: 14.390, loss: -87.921\n",
      " 53%|█████▎    | 528/1000 [23:08<24:21,  3.10s/it]INFO:root:global_step: 528, logpy: 100.629, kl: 14.407, loss: -88.166\n",
      " 53%|█████▎    | 529/1000 [23:11<24:27,  3.11s/it]INFO:root:global_step: 529, logpy: 100.956, kl: 14.427, loss: -88.454\n",
      " 53%|█████▎    | 530/1000 [23:14<24:15,  3.10s/it]INFO:root:global_step: 530, logpy: 101.226, kl: 14.445, loss: -88.686\n",
      " 53%|█████▎    | 531/1000 [23:17<24:13,  3.10s/it]INFO:root:global_step: 531, logpy: 101.542, kl: 14.462, loss: -88.966\n",
      " 53%|█████▎    | 532/1000 [23:20<24:02,  3.08s/it]INFO:root:global_step: 532, logpy: 101.847, kl: 14.480, loss: -89.235\n",
      " 53%|█████▎    | 533/1000 [23:23<23:55,  3.07s/it]INFO:root:global_step: 533, logpy: 102.108, kl: 14.496, loss: -89.461\n",
      " 53%|█████▎    | 534/1000 [23:27<23:56,  3.08s/it]INFO:root:global_step: 534, logpy: 102.381, kl: 14.517, loss: -89.694\n",
      " 54%|█████▎    | 535/1000 [23:30<23:43,  3.06s/it]INFO:root:global_step: 535, logpy: 102.615, kl: 14.534, loss: -89.893\n",
      " 54%|█████▎    | 536/1000 [23:33<23:41,  3.06s/it]INFO:root:global_step: 536, logpy: 102.891, kl: 14.552, loss: -90.133\n",
      " 54%|█████▎    | 537/1000 [23:36<23:55,  3.10s/it]INFO:root:global_step: 537, logpy: 103.207, kl: 14.566, loss: -90.417\n",
      " 54%|█████▍    | 538/1000 [23:39<23:49,  3.09s/it]INFO:root:global_step: 538, logpy: 103.421, kl: 14.588, loss: -90.591\n",
      " 54%|█████▍    | 539/1000 [23:42<23:55,  3.11s/it]INFO:root:global_step: 539, logpy: 103.700, kl: 14.607, loss: -90.834\n",
      " 54%|█████▍    | 540/1000 [23:45<24:14,  3.16s/it]INFO:root:global_step: 540, logpy: 103.955, kl: 14.626, loss: -91.053\n",
      " 54%|█████▍    | 541/1000 [23:48<24:03,  3.15s/it]INFO:root:global_step: 541, logpy: 104.229, kl: 14.644, loss: -91.290\n",
      " 54%|█████▍    | 542/1000 [23:52<23:46,  3.11s/it]INFO:root:global_step: 542, logpy: 104.514, kl: 14.658, loss: -91.546\n",
      " 54%|█████▍    | 543/1000 [23:55<23:39,  3.11s/it]INFO:root:global_step: 543, logpy: 104.757, kl: 14.676, loss: -91.754\n",
      " 54%|█████▍    | 544/1000 [23:58<23:38,  3.11s/it]INFO:root:global_step: 544, logpy: 105.027, kl: 14.694, loss: -91.988\n",
      " 55%|█████▍    | 545/1000 [24:01<23:26,  3.09s/it]INFO:root:global_step: 545, logpy: 105.261, kl: 14.714, loss: -92.186\n",
      " 55%|█████▍    | 546/1000 [24:04<23:16,  3.08s/it]INFO:root:global_step: 546, logpy: 105.499, kl: 14.732, loss: -92.390\n",
      " 55%|█████▍    | 547/1000 [24:07<23:15,  3.08s/it]INFO:root:global_step: 547, logpy: 105.765, kl: 14.751, loss: -92.620\n",
      " 55%|█████▍    | 548/1000 [24:10<23:30,  3.12s/it]INFO:root:global_step: 548, logpy: 105.987, kl: 14.770, loss: -92.807\n",
      " 55%|█████▍    | 549/1000 [24:13<23:31,  3.13s/it]INFO:root:global_step: 549, logpy: 106.189, kl: 14.786, loss: -92.977\n",
      " 55%|█████▌    | 550/1000 [24:16<23:21,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_550.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 550, logpy: 106.421, kl: 14.802, loss: -93.178\n",
      " 55%|█████▌    | 551/1000 [24:21<26:22,  3.52s/it]INFO:root:global_step: 551, logpy: 106.690, kl: 14.819, loss: -93.414\n",
      " 55%|█████▌    | 552/1000 [24:24<25:23,  3.40s/it]INFO:root:global_step: 552, logpy: 106.937, kl: 14.835, loss: -93.629\n",
      " 55%|█████▌    | 553/1000 [24:27<25:05,  3.37s/it]INFO:root:global_step: 553, logpy: 107.179, kl: 14.853, loss: -93.838\n",
      " 55%|█████▌    | 554/1000 [24:30<24:34,  3.31s/it]INFO:root:global_step: 554, logpy: 107.439, kl: 14.871, loss: -94.065\n",
      " 56%|█████▌    | 555/1000 [24:33<24:01,  3.24s/it]INFO:root:global_step: 555, logpy: 107.681, kl: 14.891, loss: -94.272\n",
      " 56%|█████▌    | 556/1000 [24:37<23:31,  3.18s/it]INFO:root:global_step: 556, logpy: 107.901, kl: 14.909, loss: -94.459\n",
      " 56%|█████▌    | 557/1000 [24:40<23:14,  3.15s/it]INFO:root:global_step: 557, logpy: 108.147, kl: 14.928, loss: -94.671\n",
      " 56%|█████▌    | 558/1000 [24:43<23:13,  3.15s/it]INFO:root:global_step: 558, logpy: 108.384, kl: 14.946, loss: -94.875\n",
      " 56%|█████▌    | 559/1000 [24:46<22:56,  3.12s/it]INFO:root:global_step: 559, logpy: 108.593, kl: 14.968, loss: -95.049\n",
      " 56%|█████▌    | 560/1000 [24:49<22:46,  3.11s/it]INFO:root:global_step: 560, logpy: 108.822, kl: 14.986, loss: -95.245\n",
      " 56%|█████▌    | 561/1000 [24:52<22:41,  3.10s/it]INFO:root:global_step: 561, logpy: 109.095, kl: 15.005, loss: -95.485\n",
      " 56%|█████▌    | 562/1000 [24:55<22:44,  3.12s/it]INFO:root:global_step: 562, logpy: 109.335, kl: 15.027, loss: -95.689\n",
      " 56%|█████▋    | 563/1000 [24:58<22:38,  3.11s/it]INFO:root:global_step: 563, logpy: 109.561, kl: 15.045, loss: -95.883\n",
      " 56%|█████▋    | 564/1000 [25:01<22:41,  3.12s/it]INFO:root:global_step: 564, logpy: 109.795, kl: 15.057, loss: -96.092\n",
      " 56%|█████▋    | 565/1000 [25:05<22:42,  3.13s/it]INFO:root:global_step: 565, logpy: 109.989, kl: 15.080, loss: -96.249\n",
      " 57%|█████▋    | 566/1000 [25:08<22:40,  3.14s/it]INFO:root:global_step: 566, logpy: 110.210, kl: 15.099, loss: -96.438\n",
      " 57%|█████▋    | 567/1000 [25:11<22:46,  3.15s/it]INFO:root:global_step: 567, logpy: 110.447, kl: 15.116, loss: -96.645\n",
      " 57%|█████▋    | 568/1000 [25:14<22:48,  3.17s/it]INFO:root:global_step: 568, logpy: 110.653, kl: 15.135, loss: -96.819\n",
      " 57%|█████▋    | 569/1000 [25:17<22:46,  3.17s/it]INFO:root:global_step: 569, logpy: 110.886, kl: 15.151, loss: -97.023\n",
      " 57%|█████▋    | 570/1000 [25:20<22:52,  3.19s/it]INFO:root:global_step: 570, logpy: 111.079, kl: 15.168, loss: -97.185\n",
      " 57%|█████▋    | 571/1000 [25:24<22:34,  3.16s/it]INFO:root:global_step: 571, logpy: 111.272, kl: 15.190, loss: -97.343\n",
      " 57%|█████▋    | 572/1000 [25:27<22:24,  3.14s/it]INFO:root:global_step: 572, logpy: 111.461, kl: 15.210, loss: -97.500\n",
      " 57%|█████▋    | 573/1000 [25:30<22:16,  3.13s/it]INFO:root:global_step: 573, logpy: 111.674, kl: 15.230, loss: -97.681\n",
      " 57%|█████▋    | 574/1000 [25:33<22:08,  3.12s/it]INFO:root:global_step: 574, logpy: 111.874, kl: 15.250, loss: -97.848\n",
      " 57%|█████▊    | 575/1000 [25:36<22:03,  3.11s/it]INFO:root:global_step: 575, logpy: 112.095, kl: 15.268, loss: -98.039\n",
      " 58%|█████▊    | 576/1000 [25:39<21:52,  3.09s/it]INFO:root:global_step: 576, logpy: 112.259, kl: 15.287, loss: -98.173\n",
      " 58%|█████▊    | 577/1000 [25:42<22:00,  3.12s/it]INFO:root:global_step: 577, logpy: 112.440, kl: 15.305, loss: -98.324\n",
      " 58%|█████▊    | 578/1000 [25:45<22:12,  3.16s/it]INFO:root:global_step: 578, logpy: 112.606, kl: 15.324, loss: -98.458\n",
      " 58%|█████▊    | 579/1000 [25:49<22:11,  3.16s/it]INFO:root:global_step: 579, logpy: 112.790, kl: 15.346, loss: -98.608\n",
      " 58%|█████▊    | 580/1000 [25:52<21:56,  3.13s/it]INFO:root:global_step: 580, logpy: 112.991, kl: 15.366, loss: -98.777\n",
      " 58%|█████▊    | 581/1000 [25:55<21:40,  3.10s/it]INFO:root:global_step: 581, logpy: 113.168, kl: 15.388, loss: -98.922\n",
      " 58%|█████▊    | 582/1000 [25:58<21:49,  3.13s/it]INFO:root:global_step: 582, logpy: 113.357, kl: 15.403, loss: -99.084\n",
      " 58%|█████▊    | 583/1000 [26:01<21:49,  3.14s/it]INFO:root:global_step: 583, logpy: 113.516, kl: 15.424, loss: -99.210\n",
      " 58%|█████▊    | 584/1000 [26:04<21:42,  3.13s/it]INFO:root:global_step: 584, logpy: 113.703, kl: 15.443, loss: -99.366\n",
      " 58%|█████▊    | 585/1000 [26:07<21:31,  3.11s/it]INFO:root:global_step: 585, logpy: 113.917, kl: 15.462, loss: -99.550\n",
      " 59%|█████▊    | 586/1000 [26:10<21:30,  3.12s/it]INFO:root:global_step: 586, logpy: 114.079, kl: 15.484, loss: -99.679\n",
      " 59%|█████▊    | 587/1000 [26:13<21:22,  3.11s/it]INFO:root:global_step: 587, logpy: 114.254, kl: 15.505, loss: -99.823\n",
      " 59%|█████▉    | 588/1000 [26:17<21:16,  3.10s/it]INFO:root:global_step: 588, logpy: 114.416, kl: 15.526, loss: -99.954\n",
      " 59%|█████▉    | 589/1000 [26:20<21:08,  3.09s/it]INFO:root:global_step: 589, logpy: 114.621, kl: 15.543, loss: -100.131\n",
      " 59%|█████▉    | 590/1000 [26:23<21:12,  3.10s/it]INFO:root:global_step: 590, logpy: 114.788, kl: 15.560, loss: -100.271\n",
      " 59%|█████▉    | 591/1000 [26:26<21:13,  3.11s/it]INFO:root:global_step: 591, logpy: 114.994, kl: 15.580, loss: -100.446\n",
      " 59%|█████▉    | 592/1000 [26:29<21:01,  3.09s/it]INFO:root:global_step: 592, logpy: 115.152, kl: 15.598, loss: -100.576\n",
      " 59%|█████▉    | 593/1000 [26:32<21:01,  3.10s/it]INFO:root:global_step: 593, logpy: 115.332, kl: 15.612, loss: -100.731\n",
      " 59%|█████▉    | 594/1000 [26:35<21:15,  3.14s/it]INFO:root:global_step: 594, logpy: 115.534, kl: 15.627, loss: -100.908\n",
      " 60%|█████▉    | 595/1000 [26:38<21:14,  3.15s/it]INFO:root:global_step: 595, logpy: 115.765, kl: 15.646, loss: -101.110\n",
      " 60%|█████▉    | 596/1000 [26:42<21:07,  3.14s/it]INFO:root:global_step: 596, logpy: 116.008, kl: 15.664, loss: -101.325\n",
      " 60%|█████▉    | 597/1000 [26:45<20:57,  3.12s/it]INFO:root:global_step: 597, logpy: 116.210, kl: 15.688, loss: -101.494\n",
      " 60%|█████▉    | 598/1000 [26:48<20:56,  3.12s/it]INFO:root:global_step: 598, logpy: 116.392, kl: 15.709, loss: -101.645\n",
      " 60%|█████▉    | 599/1000 [26:51<20:51,  3.12s/it]INFO:root:global_step: 599, logpy: 116.601, kl: 15.723, loss: -101.830\n",
      " 60%|██████    | 600/1000 [26:54<20:57,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_600.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 600, logpy: 116.753, kl: 15.743, loss: -101.953\n",
      " 60%|██████    | 601/1000 [26:58<23:17,  3.50s/it]INFO:root:global_step: 601, logpy: 116.934, kl: 15.763, loss: -102.104\n",
      " 60%|██████    | 602/1000 [27:02<22:36,  3.41s/it]INFO:root:global_step: 602, logpy: 117.116, kl: 15.781, loss: -102.260\n",
      " 60%|██████    | 603/1000 [27:05<22:13,  3.36s/it]INFO:root:global_step: 603, logpy: 117.293, kl: 15.797, loss: -102.411\n",
      " 60%|██████    | 604/1000 [27:08<21:44,  3.30s/it]INFO:root:global_step: 604, logpy: 117.462, kl: 15.813, loss: -102.555\n",
      " 60%|██████    | 605/1000 [27:11<21:39,  3.29s/it]INFO:root:global_step: 605, logpy: 117.614, kl: 15.832, loss: -102.679\n",
      " 61%|██████    | 606/1000 [27:14<21:21,  3.25s/it]INFO:root:global_step: 606, logpy: 117.831, kl: 15.849, loss: -102.870\n",
      " 61%|██████    | 607/1000 [27:18<21:03,  3.21s/it]INFO:root:global_step: 607, logpy: 118.006, kl: 15.868, loss: -103.017\n",
      " 61%|██████    | 608/1000 [27:21<20:51,  3.19s/it]INFO:root:global_step: 608, logpy: 118.202, kl: 15.885, loss: -103.187\n",
      " 61%|██████    | 609/1000 [27:24<20:38,  3.17s/it]INFO:root:global_step: 609, logpy: 118.366, kl: 15.903, loss: -103.324\n",
      " 61%|██████    | 610/1000 [27:27<20:26,  3.14s/it]INFO:root:global_step: 610, logpy: 118.547, kl: 15.919, loss: -103.481\n",
      " 61%|██████    | 611/1000 [27:30<20:23,  3.15s/it]INFO:root:global_step: 611, logpy: 118.717, kl: 15.937, loss: -103.625\n",
      " 61%|██████    | 612/1000 [27:33<20:19,  3.14s/it]INFO:root:global_step: 612, logpy: 118.867, kl: 15.953, loss: -103.750\n",
      " 61%|██████▏   | 613/1000 [27:36<20:08,  3.12s/it]INFO:root:global_step: 613, logpy: 119.048, kl: 15.972, loss: -103.904\n",
      " 61%|██████▏   | 614/1000 [27:39<19:56,  3.10s/it]INFO:root:global_step: 614, logpy: 119.212, kl: 15.991, loss: -104.041\n",
      " 62%|██████▏   | 615/1000 [27:43<20:09,  3.14s/it]INFO:root:global_step: 615, logpy: 119.387, kl: 16.012, loss: -104.186\n",
      " 62%|██████▏   | 616/1000 [27:46<20:13,  3.16s/it]INFO:root:global_step: 616, logpy: 119.529, kl: 16.030, loss: -104.302\n",
      " 62%|██████▏   | 617/1000 [27:49<20:08,  3.16s/it]INFO:root:global_step: 617, logpy: 119.699, kl: 16.049, loss: -104.444\n",
      " 62%|██████▏   | 618/1000 [27:52<20:12,  3.17s/it]INFO:root:global_step: 618, logpy: 119.860, kl: 16.067, loss: -104.580\n",
      " 62%|██████▏   | 619/1000 [27:55<20:02,  3.16s/it]INFO:root:global_step: 619, logpy: 120.000, kl: 16.089, loss: -104.690\n",
      " 62%|██████▏   | 620/1000 [27:58<19:53,  3.14s/it]INFO:root:global_step: 620, logpy: 120.153, kl: 16.110, loss: -104.814\n",
      " 62%|██████▏   | 621/1000 [28:01<19:52,  3.15s/it]INFO:root:global_step: 621, logpy: 120.314, kl: 16.129, loss: -104.948\n",
      " 62%|██████▏   | 622/1000 [28:05<19:45,  3.14s/it]INFO:root:global_step: 622, logpy: 120.468, kl: 16.148, loss: -105.076\n",
      " 62%|██████▏   | 623/1000 [28:08<19:47,  3.15s/it]INFO:root:global_step: 623, logpy: 120.634, kl: 16.167, loss: -105.216\n",
      " 62%|██████▏   | 624/1000 [28:11<19:46,  3.16s/it]INFO:root:global_step: 624, logpy: 120.788, kl: 16.183, loss: -105.346\n",
      " 62%|██████▎   | 625/1000 [28:14<19:44,  3.16s/it]INFO:root:global_step: 625, logpy: 120.945, kl: 16.202, loss: -105.477\n",
      " 63%|██████▎   | 626/1000 [28:17<19:32,  3.14s/it]INFO:root:global_step: 626, logpy: 121.109, kl: 16.218, loss: -105.617\n",
      " 63%|██████▎   | 627/1000 [28:20<19:37,  3.16s/it]INFO:root:global_step: 627, logpy: 121.258, kl: 16.239, loss: -105.738\n",
      " 63%|██████▎   | 628/1000 [28:23<19:25,  3.13s/it]INFO:root:global_step: 628, logpy: 121.418, kl: 16.255, loss: -105.875\n",
      " 63%|██████▎   | 629/1000 [28:27<19:29,  3.15s/it]INFO:root:global_step: 629, logpy: 121.594, kl: 16.277, loss: -106.022\n",
      " 63%|██████▎   | 630/1000 [28:30<19:35,  3.18s/it]INFO:root:global_step: 630, logpy: 121.753, kl: 16.297, loss: -106.154\n",
      " 63%|██████▎   | 631/1000 [28:33<19:22,  3.15s/it]INFO:root:global_step: 631, logpy: 121.912, kl: 16.313, loss: -106.289\n",
      " 63%|██████▎   | 632/1000 [28:36<19:14,  3.14s/it]INFO:root:global_step: 632, logpy: 122.087, kl: 16.333, loss: -106.438\n",
      " 63%|██████▎   | 633/1000 [28:39<19:12,  3.14s/it]INFO:root:global_step: 633, logpy: 122.251, kl: 16.349, loss: -106.579\n",
      " 63%|██████▎   | 634/1000 [28:42<19:05,  3.13s/it]INFO:root:global_step: 634, logpy: 122.385, kl: 16.366, loss: -106.689\n",
      " 64%|██████▎   | 635/1000 [28:45<18:59,  3.12s/it]INFO:root:global_step: 635, logpy: 122.520, kl: 16.384, loss: -106.798\n",
      " 64%|██████▎   | 636/1000 [28:49<18:58,  3.13s/it]INFO:root:global_step: 636, logpy: 122.638, kl: 16.407, loss: -106.887\n",
      " 64%|██████▎   | 637/1000 [28:52<18:52,  3.12s/it]INFO:root:global_step: 637, logpy: 122.751, kl: 16.426, loss: -106.975\n",
      " 64%|██████▍   | 638/1000 [28:55<18:51,  3.12s/it]INFO:root:global_step: 638, logpy: 122.928, kl: 16.446, loss: -107.126\n",
      " 64%|██████▍   | 639/1000 [28:58<18:45,  3.12s/it]INFO:root:global_step: 639, logpy: 123.083, kl: 16.458, loss: -107.262\n",
      " 64%|██████▍   | 640/1000 [29:01<18:47,  3.13s/it]INFO:root:global_step: 640, logpy: 123.233, kl: 16.476, loss: -107.388\n",
      " 64%|██████▍   | 641/1000 [29:04<18:51,  3.15s/it]INFO:root:global_step: 641, logpy: 123.399, kl: 16.493, loss: -107.531\n",
      " 64%|██████▍   | 642/1000 [29:08<18:55,  3.17s/it]INFO:root:global_step: 642, logpy: 123.572, kl: 16.509, loss: -107.681\n",
      " 64%|██████▍   | 643/1000 [29:11<18:58,  3.19s/it]INFO:root:global_step: 643, logpy: 123.718, kl: 16.525, loss: -107.805\n",
      " 64%|██████▍   | 644/1000 [29:14<18:49,  3.17s/it]INFO:root:global_step: 644, logpy: 123.850, kl: 16.547, loss: -107.909\n",
      " 64%|██████▍   | 645/1000 [29:17<18:43,  3.17s/it]INFO:root:global_step: 645, logpy: 124.035, kl: 16.564, loss: -108.071\n",
      " 65%|██████▍   | 646/1000 [29:20<18:45,  3.18s/it]INFO:root:global_step: 646, logpy: 124.175, kl: 16.584, loss: -108.185\n",
      " 65%|██████▍   | 647/1000 [29:23<18:36,  3.16s/it]INFO:root:global_step: 647, logpy: 124.315, kl: 16.598, loss: -108.304\n",
      " 65%|██████▍   | 648/1000 [29:27<18:29,  3.15s/it]INFO:root:global_step: 648, logpy: 124.458, kl: 16.616, loss: -108.424\n",
      " 65%|██████▍   | 649/1000 [29:30<18:32,  3.17s/it]INFO:root:global_step: 649, logpy: 124.608, kl: 16.634, loss: -108.550\n",
      " 65%|██████▌   | 650/1000 [29:33<18:24,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_650.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 650, logpy: 124.724, kl: 16.655, loss: -108.640\n",
      " 65%|██████▌   | 651/1000 [29:37<20:36,  3.54s/it]INFO:root:global_step: 651, logpy: 124.848, kl: 16.672, loss: -108.741\n",
      " 65%|██████▌   | 652/1000 [29:41<20:15,  3.49s/it]INFO:root:global_step: 652, logpy: 125.003, kl: 16.684, loss: -108.878\n",
      " 65%|██████▌   | 653/1000 [29:44<19:56,  3.45s/it]INFO:root:global_step: 653, logpy: 125.120, kl: 16.706, loss: -108.967\n",
      " 65%|██████▌   | 654/1000 [29:47<19:29,  3.38s/it]INFO:root:global_step: 654, logpy: 125.255, kl: 16.723, loss: -109.080\n",
      " 66%|██████▌   | 655/1000 [29:50<19:01,  3.31s/it]INFO:root:global_step: 655, logpy: 125.417, kl: 16.736, loss: -109.224\n",
      " 66%|██████▌   | 656/1000 [29:54<18:42,  3.26s/it]INFO:root:global_step: 656, logpy: 125.559, kl: 16.760, loss: -109.335\n",
      " 66%|██████▌   | 657/1000 [29:57<18:28,  3.23s/it]INFO:root:global_step: 657, logpy: 125.694, kl: 16.780, loss: -109.446\n",
      " 66%|██████▌   | 658/1000 [30:00<18:17,  3.21s/it]INFO:root:global_step: 658, logpy: 125.814, kl: 16.793, loss: -109.547\n",
      " 66%|██████▌   | 659/1000 [30:03<18:06,  3.19s/it]INFO:root:global_step: 659, logpy: 125.921, kl: 16.808, loss: -109.634\n",
      " 66%|██████▌   | 660/1000 [30:06<18:05,  3.19s/it]INFO:root:global_step: 660, logpy: 126.045, kl: 16.831, loss: -109.730\n",
      " 66%|██████▌   | 661/1000 [30:09<17:59,  3.18s/it]INFO:root:global_step: 661, logpy: 126.148, kl: 16.850, loss: -109.809\n",
      " 66%|██████▌   | 662/1000 [30:13<17:53,  3.18s/it]INFO:root:global_step: 662, logpy: 126.309, kl: 16.867, loss: -109.948\n",
      " 66%|██████▋   | 663/1000 [30:16<17:48,  3.17s/it]INFO:root:global_step: 663, logpy: 126.449, kl: 16.885, loss: -110.064\n",
      " 66%|██████▋   | 664/1000 [30:19<17:41,  3.16s/it]INFO:root:global_step: 664, logpy: 126.609, kl: 16.901, loss: -110.204\n",
      " 66%|██████▋   | 665/1000 [30:22<17:49,  3.19s/it]INFO:root:global_step: 665, logpy: 126.727, kl: 16.918, loss: -110.300\n",
      " 67%|██████▋   | 666/1000 [30:25<17:44,  3.19s/it]INFO:root:global_step: 666, logpy: 126.880, kl: 16.938, loss: -110.428\n",
      " 67%|██████▋   | 667/1000 [30:28<17:35,  3.17s/it]INFO:root:global_step: 667, logpy: 127.014, kl: 16.954, loss: -110.541\n",
      " 67%|██████▋   | 668/1000 [30:32<17:39,  3.19s/it]INFO:root:global_step: 668, logpy: 127.152, kl: 16.970, loss: -110.658\n",
      " 67%|██████▋   | 669/1000 [30:35<17:28,  3.17s/it]INFO:root:global_step: 669, logpy: 127.290, kl: 16.990, loss: -110.772\n",
      " 67%|██████▋   | 670/1000 [30:38<17:26,  3.17s/it]INFO:root:global_step: 670, logpy: 127.411, kl: 17.007, loss: -110.871\n",
      " 67%|██████▋   | 671/1000 [30:41<17:53,  3.26s/it]INFO:root:global_step: 671, logpy: 127.549, kl: 17.024, loss: -110.987\n",
      " 67%|██████▋   | 672/1000 [30:45<18:11,  3.33s/it]INFO:root:global_step: 672, logpy: 127.677, kl: 17.042, loss: -111.093\n",
      " 67%|██████▋   | 673/1000 [30:49<18:47,  3.45s/it]INFO:root:global_step: 673, logpy: 127.830, kl: 17.056, loss: -111.227\n",
      " 67%|██████▋   | 674/1000 [30:52<18:46,  3.46s/it]INFO:root:global_step: 674, logpy: 127.928, kl: 17.075, loss: -111.301\n",
      " 68%|██████▊   | 675/1000 [30:56<18:43,  3.46s/it]INFO:root:global_step: 675, logpy: 128.040, kl: 17.095, loss: -111.388\n",
      " 68%|██████▊   | 676/1000 [30:59<18:19,  3.39s/it]INFO:root:global_step: 676, logpy: 128.141, kl: 17.113, loss: -111.467\n",
      " 68%|██████▊   | 677/1000 [31:02<18:14,  3.39s/it]INFO:root:global_step: 677, logpy: 128.271, kl: 17.129, loss: -111.578\n",
      " 68%|██████▊   | 678/1000 [31:05<17:56,  3.34s/it]INFO:root:global_step: 678, logpy: 128.395, kl: 17.144, loss: -111.681\n",
      " 68%|██████▊   | 679/1000 [31:09<17:43,  3.31s/it]INFO:root:global_step: 679, logpy: 128.506, kl: 17.165, loss: -111.767\n",
      " 68%|██████▊   | 680/1000 [31:12<17:40,  3.32s/it]INFO:root:global_step: 680, logpy: 128.617, kl: 17.182, loss: -111.857\n",
      " 68%|██████▊   | 681/1000 [31:15<17:38,  3.32s/it]INFO:root:global_step: 681, logpy: 128.711, kl: 17.202, loss: -111.927\n",
      " 68%|██████▊   | 682/1000 [31:19<17:31,  3.31s/it]INFO:root:global_step: 682, logpy: 128.829, kl: 17.224, loss: -112.019\n",
      " 68%|██████▊   | 683/1000 [31:22<17:30,  3.31s/it]INFO:root:global_step: 683, logpy: 128.941, kl: 17.243, loss: -112.107\n",
      " 68%|██████▊   | 684/1000 [31:25<17:30,  3.32s/it]INFO:root:global_step: 684, logpy: 129.079, kl: 17.261, loss: -112.223\n",
      " 68%|██████▊   | 685/1000 [31:28<17:12,  3.28s/it]INFO:root:global_step: 685, logpy: 129.182, kl: 17.281, loss: -112.302\n",
      " 69%|██████▊   | 686/1000 [31:32<17:03,  3.26s/it]INFO:root:global_step: 686, logpy: 129.311, kl: 17.301, loss: -112.407\n",
      " 69%|██████▊   | 687/1000 [31:35<16:51,  3.23s/it]INFO:root:global_step: 687, logpy: 129.445, kl: 17.321, loss: -112.518\n",
      " 69%|██████▉   | 688/1000 [31:38<16:43,  3.22s/it]INFO:root:global_step: 688, logpy: 129.541, kl: 17.340, loss: -112.590\n",
      " 69%|██████▉   | 689/1000 [31:41<16:43,  3.23s/it]INFO:root:global_step: 689, logpy: 129.673, kl: 17.358, loss: -112.701\n",
      " 69%|██████▉   | 690/1000 [31:44<16:40,  3.23s/it]INFO:root:global_step: 690, logpy: 129.783, kl: 17.381, loss: -112.783\n",
      " 69%|██████▉   | 691/1000 [31:48<16:44,  3.25s/it]INFO:root:global_step: 691, logpy: 129.900, kl: 17.398, loss: -112.880\n",
      " 69%|██████▉   | 692/1000 [31:51<16:36,  3.24s/it]INFO:root:global_step: 692, logpy: 130.010, kl: 17.410, loss: -112.974\n",
      " 69%|██████▉   | 693/1000 [31:54<16:26,  3.21s/it]INFO:root:global_step: 693, logpy: 130.109, kl: 17.425, loss: -113.054\n",
      " 69%|██████▉   | 694/1000 [31:57<16:20,  3.21s/it]INFO:root:global_step: 694, logpy: 130.238, kl: 17.444, loss: -113.161\n",
      " 70%|██████▉   | 695/1000 [32:00<16:11,  3.19s/it]INFO:root:global_step: 695, logpy: 130.367, kl: 17.463, loss: -113.267\n",
      " 70%|██████▉   | 696/1000 [32:04<16:03,  3.17s/it]INFO:root:global_step: 696, logpy: 130.475, kl: 17.481, loss: -113.353\n",
      " 70%|██████▉   | 697/1000 [32:07<15:59,  3.17s/it]INFO:root:global_step: 697, logpy: 130.621, kl: 17.499, loss: -113.478\n",
      " 70%|██████▉   | 698/1000 [32:10<15:58,  3.17s/it]INFO:root:global_step: 698, logpy: 130.713, kl: 17.519, loss: -113.546\n",
      " 70%|██████▉   | 699/1000 [32:13<15:56,  3.18s/it]INFO:root:global_step: 699, logpy: 130.818, kl: 17.539, loss: -113.627\n",
      " 70%|███████   | 700/1000 [32:16<15:53,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_700.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 700, logpy: 130.921, kl: 17.560, loss: -113.706\n",
      " 70%|███████   | 701/1000 [32:21<17:46,  3.57s/it]INFO:root:global_step: 701, logpy: 131.025, kl: 17.580, loss: -113.787\n",
      " 70%|███████   | 702/1000 [32:24<17:08,  3.45s/it]INFO:root:global_step: 702, logpy: 131.144, kl: 17.596, loss: -113.886\n",
      " 70%|███████   | 703/1000 [32:27<16:53,  3.41s/it]INFO:root:global_step: 703, logpy: 131.246, kl: 17.613, loss: -113.968\n",
      " 70%|███████   | 704/1000 [32:31<16:38,  3.37s/it]INFO:root:global_step: 704, logpy: 131.334, kl: 17.633, loss: -114.033\n",
      " 70%|███████   | 705/1000 [32:34<16:25,  3.34s/it]INFO:root:global_step: 705, logpy: 131.466, kl: 17.647, loss: -114.147\n",
      " 71%|███████   | 706/1000 [32:37<16:15,  3.32s/it]INFO:root:global_step: 706, logpy: 131.579, kl: 17.671, loss: -114.233\n",
      " 71%|███████   | 707/1000 [32:40<15:56,  3.26s/it]INFO:root:global_step: 707, logpy: 131.710, kl: 17.685, loss: -114.347\n",
      " 71%|███████   | 708/1000 [32:44<15:56,  3.28s/it]INFO:root:global_step: 708, logpy: 131.833, kl: 17.700, loss: -114.451\n",
      " 71%|███████   | 709/1000 [32:47<15:44,  3.25s/it]INFO:root:global_step: 709, logpy: 131.963, kl: 17.720, loss: -114.558\n",
      " 71%|███████   | 710/1000 [32:50<15:31,  3.21s/it]INFO:root:global_step: 710, logpy: 132.077, kl: 17.740, loss: -114.649\n",
      " 71%|███████   | 711/1000 [32:53<15:23,  3.20s/it]INFO:root:global_step: 711, logpy: 132.159, kl: 17.760, loss: -114.708\n",
      " 71%|███████   | 712/1000 [32:56<15:15,  3.18s/it]INFO:root:global_step: 712, logpy: 132.267, kl: 17.780, loss: -114.793\n",
      " 71%|███████▏  | 713/1000 [32:59<15:24,  3.22s/it]INFO:root:global_step: 713, logpy: 132.350, kl: 17.796, loss: -114.857\n",
      " 71%|███████▏  | 714/1000 [33:03<15:22,  3.22s/it]INFO:root:global_step: 714, logpy: 132.474, kl: 17.815, loss: -114.958\n",
      " 72%|███████▏  | 715/1000 [33:06<15:14,  3.21s/it]INFO:root:global_step: 715, logpy: 132.581, kl: 17.829, loss: -115.048\n",
      " 72%|███████▏  | 716/1000 [33:09<15:21,  3.25s/it]INFO:root:global_step: 716, logpy: 132.694, kl: 17.851, loss: -115.136\n",
      " 72%|███████▏  | 717/1000 [33:13<15:55,  3.38s/it]INFO:root:global_step: 717, logpy: 132.805, kl: 17.865, loss: -115.231\n",
      " 72%|███████▏  | 718/1000 [33:16<15:43,  3.35s/it]INFO:root:global_step: 718, logpy: 132.913, kl: 17.885, loss: -115.316\n",
      " 72%|███████▏  | 719/1000 [33:19<15:33,  3.32s/it]INFO:root:global_step: 719, logpy: 133.027, kl: 17.903, loss: -115.409\n",
      " 72%|███████▏  | 720/1000 [33:23<15:30,  3.32s/it]INFO:root:global_step: 720, logpy: 133.091, kl: 17.921, loss: -115.452\n",
      " 72%|███████▏  | 721/1000 [33:26<15:21,  3.30s/it]INFO:root:global_step: 721, logpy: 133.206, kl: 17.937, loss: -115.549\n",
      " 72%|███████▏  | 722/1000 [33:29<15:05,  3.26s/it]INFO:root:global_step: 722, logpy: 133.305, kl: 17.955, loss: -115.627\n",
      " 72%|███████▏  | 723/1000 [33:32<14:56,  3.24s/it]INFO:root:global_step: 723, logpy: 133.389, kl: 17.972, loss: -115.691\n",
      " 72%|███████▏  | 724/1000 [33:36<14:49,  3.22s/it]INFO:root:global_step: 724, logpy: 133.467, kl: 17.988, loss: -115.750\n",
      " 72%|███████▎  | 725/1000 [33:39<14:45,  3.22s/it]INFO:root:global_step: 725, logpy: 133.540, kl: 18.011, loss: -115.797\n",
      " 73%|███████▎  | 726/1000 [33:42<14:41,  3.22s/it]INFO:root:global_step: 726, logpy: 133.654, kl: 18.026, loss: -115.894\n",
      " 73%|███████▎  | 727/1000 [33:45<14:59,  3.30s/it]INFO:root:global_step: 727, logpy: 133.755, kl: 18.048, loss: -115.970\n",
      " 73%|███████▎  | 728/1000 [33:49<14:51,  3.28s/it]INFO:root:global_step: 728, logpy: 133.835, kl: 18.066, loss: -116.030\n",
      " 73%|███████▎  | 729/1000 [33:52<14:50,  3.28s/it]INFO:root:global_step: 729, logpy: 133.966, kl: 18.083, loss: -116.141\n",
      " 73%|███████▎  | 730/1000 [33:56<16:01,  3.56s/it]INFO:root:global_step: 730, logpy: 134.079, kl: 18.096, loss: -116.238\n",
      " 73%|███████▎  | 731/1000 [34:00<16:08,  3.60s/it]INFO:root:global_step: 731, logpy: 134.147, kl: 18.110, loss: -116.289\n",
      " 73%|███████▎  | 732/1000 [34:04<16:12,  3.63s/it]INFO:root:global_step: 732, logpy: 134.207, kl: 18.130, loss: -116.328\n",
      " 73%|███████▎  | 733/1000 [34:07<15:54,  3.57s/it]INFO:root:global_step: 733, logpy: 134.285, kl: 18.149, loss: -116.383\n",
      " 73%|███████▎  | 734/1000 [34:10<15:41,  3.54s/it]INFO:root:global_step: 734, logpy: 134.365, kl: 18.168, loss: -116.442\n",
      " 74%|███████▎  | 735/1000 [34:14<15:43,  3.56s/it]INFO:root:global_step: 735, logpy: 134.444, kl: 18.189, loss: -116.498\n",
      " 74%|███████▎  | 736/1000 [34:18<15:48,  3.59s/it]INFO:root:global_step: 736, logpy: 134.556, kl: 18.206, loss: -116.591\n",
      " 74%|███████▎  | 737/1000 [34:22<16:10,  3.69s/it]INFO:root:global_step: 737, logpy: 134.654, kl: 18.221, loss: -116.670\n",
      " 74%|███████▍  | 738/1000 [34:25<16:07,  3.69s/it]INFO:root:global_step: 738, logpy: 134.732, kl: 18.239, loss: -116.728\n",
      " 74%|███████▍  | 739/1000 [34:29<15:49,  3.64s/it]INFO:root:global_step: 739, logpy: 134.851, kl: 18.260, loss: -116.825\n",
      " 74%|███████▍  | 740/1000 [34:32<15:27,  3.57s/it]INFO:root:global_step: 740, logpy: 134.940, kl: 18.281, loss: -116.890\n",
      " 74%|███████▍  | 741/1000 [34:36<15:13,  3.53s/it]INFO:root:global_step: 741, logpy: 135.023, kl: 18.299, loss: -116.953\n",
      " 74%|███████▍  | 742/1000 [34:39<14:59,  3.48s/it]INFO:root:global_step: 742, logpy: 135.124, kl: 18.316, loss: -117.034\n",
      " 74%|███████▍  | 743/1000 [34:42<14:37,  3.41s/it]INFO:root:global_step: 743, logpy: 135.239, kl: 18.332, loss: -117.132\n",
      " 74%|███████▍  | 744/1000 [34:46<14:21,  3.36s/it]INFO:root:global_step: 744, logpy: 135.313, kl: 18.347, loss: -117.188\n",
      " 74%|███████▍  | 745/1000 [34:49<14:12,  3.34s/it]INFO:root:global_step: 745, logpy: 135.369, kl: 18.365, loss: -117.224\n",
      " 75%|███████▍  | 746/1000 [34:52<13:59,  3.31s/it]INFO:root:global_step: 746, logpy: 135.454, kl: 18.382, loss: -117.290\n",
      " 75%|███████▍  | 747/1000 [34:56<14:08,  3.35s/it]INFO:root:global_step: 747, logpy: 135.535, kl: 18.396, loss: -117.354\n",
      " 75%|███████▍  | 748/1000 [34:59<14:21,  3.42s/it]INFO:root:global_step: 748, logpy: 135.615, kl: 18.410, loss: -117.418\n",
      " 75%|███████▍  | 749/1000 [35:02<14:15,  3.41s/it]INFO:root:global_step: 749, logpy: 135.693, kl: 18.426, loss: -117.477\n",
      " 75%|███████▌  | 750/1000 [35:06<14:14,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_750.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 750, logpy: 135.799, kl: 18.442, loss: -117.566\n",
      " 75%|███████▌  | 751/1000 [35:11<16:16,  3.92s/it]INFO:root:global_step: 751, logpy: 135.864, kl: 18.462, loss: -117.610\n",
      " 75%|███████▌  | 752/1000 [35:15<15:45,  3.81s/it]INFO:root:global_step: 752, logpy: 135.953, kl: 18.477, loss: -117.681\n",
      " 75%|███████▌  | 753/1000 [35:18<15:31,  3.77s/it]INFO:root:global_step: 753, logpy: 136.019, kl: 18.492, loss: -117.729\n",
      " 75%|███████▌  | 754/1000 [35:22<15:02,  3.67s/it]INFO:root:global_step: 754, logpy: 136.139, kl: 18.509, loss: -117.830\n",
      " 76%|███████▌  | 755/1000 [35:25<14:34,  3.57s/it]INFO:root:global_step: 755, logpy: 136.217, kl: 18.526, loss: -117.890\n",
      " 76%|███████▌  | 756/1000 [35:28<14:05,  3.47s/it]INFO:root:global_step: 756, logpy: 136.328, kl: 18.540, loss: -117.984\n",
      " 76%|███████▌  | 757/1000 [35:32<14:04,  3.47s/it]INFO:root:global_step: 757, logpy: 136.409, kl: 18.560, loss: -118.044\n",
      " 76%|███████▌  | 758/1000 [35:35<14:07,  3.50s/it]INFO:root:global_step: 758, logpy: 136.522, kl: 18.579, loss: -118.135\n",
      " 76%|███████▌  | 759/1000 [35:39<13:54,  3.46s/it]INFO:root:global_step: 759, logpy: 136.596, kl: 18.599, loss: -118.188\n",
      " 76%|███████▌  | 760/1000 [35:42<13:41,  3.42s/it]INFO:root:global_step: 760, logpy: 136.711, kl: 18.611, loss: -118.289\n",
      " 76%|███████▌  | 761/1000 [35:46<13:52,  3.48s/it]INFO:root:global_step: 761, logpy: 136.784, kl: 18.628, loss: -118.343\n",
      " 76%|███████▌  | 762/1000 [35:49<14:01,  3.54s/it]INFO:root:global_step: 762, logpy: 136.893, kl: 18.646, loss: -118.432\n",
      " 76%|███████▋  | 763/1000 [35:53<13:39,  3.46s/it]INFO:root:global_step: 763, logpy: 137.008, kl: 18.663, loss: -118.528\n",
      " 76%|███████▋  | 764/1000 [35:56<13:20,  3.39s/it]INFO:root:global_step: 764, logpy: 137.085, kl: 18.680, loss: -118.586\n",
      " 76%|███████▋  | 765/1000 [35:59<13:11,  3.37s/it]INFO:root:global_step: 765, logpy: 137.124, kl: 18.703, loss: -118.601\n",
      " 77%|███████▋  | 766/1000 [36:02<13:04,  3.35s/it]INFO:root:global_step: 766, logpy: 137.192, kl: 18.716, loss: -118.654\n",
      " 77%|███████▋  | 767/1000 [36:06<12:54,  3.32s/it]INFO:root:global_step: 767, logpy: 137.279, kl: 18.729, loss: -118.726\n",
      " 77%|███████▋  | 768/1000 [36:09<12:57,  3.35s/it]INFO:root:global_step: 768, logpy: 137.361, kl: 18.747, loss: -118.788\n",
      " 77%|███████▋  | 769/1000 [36:12<12:47,  3.32s/it]INFO:root:global_step: 769, logpy: 137.432, kl: 18.764, loss: -118.841\n",
      " 77%|███████▋  | 770/1000 [36:16<12:37,  3.30s/it]INFO:root:global_step: 770, logpy: 137.545, kl: 18.777, loss: -118.939\n",
      " 77%|███████▋  | 771/1000 [36:19<12:29,  3.27s/it]INFO:root:global_step: 771, logpy: 137.602, kl: 18.793, loss: -118.979\n",
      " 77%|███████▋  | 772/1000 [36:22<12:23,  3.26s/it]INFO:root:global_step: 772, logpy: 137.700, kl: 18.807, loss: -119.060\n",
      " 77%|███████▋  | 773/1000 [36:25<12:22,  3.27s/it]INFO:root:global_step: 773, logpy: 137.796, kl: 18.823, loss: -119.139\n",
      " 77%|███████▋  | 774/1000 [36:29<12:15,  3.26s/it]INFO:root:global_step: 774, logpy: 137.865, kl: 18.839, loss: -119.190\n",
      " 78%|███████▊  | 775/1000 [36:32<12:15,  3.27s/it]INFO:root:global_step: 775, logpy: 137.962, kl: 18.855, loss: -119.269\n",
      " 78%|███████▊  | 776/1000 [36:35<12:20,  3.30s/it]INFO:root:global_step: 776, logpy: 138.068, kl: 18.869, loss: -119.360\n",
      " 78%|███████▊  | 777/1000 [36:39<12:37,  3.40s/it]INFO:root:global_step: 777, logpy: 138.125, kl: 18.885, loss: -119.399\n",
      " 78%|███████▊  | 778/1000 [36:42<12:42,  3.43s/it]INFO:root:global_step: 778, logpy: 138.168, kl: 18.901, loss: -119.424\n",
      " 78%|███████▊  | 779/1000 [36:46<12:32,  3.40s/it]INFO:root:global_step: 779, logpy: 138.261, kl: 18.919, loss: -119.498\n",
      " 78%|███████▊  | 780/1000 [36:49<12:26,  3.40s/it]INFO:root:global_step: 780, logpy: 138.364, kl: 18.935, loss: -119.583\n",
      " 78%|███████▊  | 781/1000 [36:52<12:11,  3.34s/it]INFO:root:global_step: 781, logpy: 138.441, kl: 18.956, loss: -119.638\n",
      " 78%|███████▊  | 782/1000 [36:55<11:56,  3.29s/it]INFO:root:global_step: 782, logpy: 138.513, kl: 18.967, loss: -119.697\n",
      " 78%|███████▊  | 783/1000 [36:59<11:50,  3.28s/it]INFO:root:global_step: 783, logpy: 138.622, kl: 18.983, loss: -119.790\n",
      " 78%|███████▊  | 784/1000 [37:02<11:43,  3.26s/it]INFO:root:global_step: 784, logpy: 138.734, kl: 18.996, loss: -119.887\n",
      " 78%|███████▊  | 785/1000 [37:05<11:38,  3.25s/it]INFO:root:global_step: 785, logpy: 138.851, kl: 19.010, loss: -119.987\n",
      " 79%|███████▊  | 786/1000 [37:08<11:35,  3.25s/it]INFO:root:global_step: 786, logpy: 138.945, kl: 19.024, loss: -120.066\n",
      " 79%|███████▊  | 787/1000 [37:12<11:34,  3.26s/it]INFO:root:global_step: 787, logpy: 139.030, kl: 19.044, loss: -120.130\n",
      " 79%|███████▉  | 788/1000 [37:15<11:28,  3.25s/it]INFO:root:global_step: 788, logpy: 139.122, kl: 19.062, loss: -120.203\n",
      " 79%|███████▉  | 789/1000 [37:18<11:22,  3.24s/it]INFO:root:global_step: 789, logpy: 139.218, kl: 19.078, loss: -120.281\n",
      " 79%|███████▉  | 790/1000 [37:21<11:20,  3.24s/it]INFO:root:global_step: 790, logpy: 139.293, kl: 19.098, loss: -120.335\n",
      " 79%|███████▉  | 791/1000 [37:25<11:13,  3.22s/it]INFO:root:global_step: 791, logpy: 139.357, kl: 19.117, loss: -120.378\n",
      " 79%|███████▉  | 792/1000 [37:28<11:10,  3.22s/it]INFO:root:global_step: 792, logpy: 139.422, kl: 19.132, loss: -120.427\n",
      " 79%|███████▉  | 793/1000 [37:31<11:14,  3.26s/it]INFO:root:global_step: 793, logpy: 139.495, kl: 19.142, loss: -120.488\n",
      " 79%|███████▉  | 794/1000 [37:34<11:11,  3.26s/it]INFO:root:global_step: 794, logpy: 139.566, kl: 19.159, loss: -120.541\n",
      " 80%|███████▉  | 795/1000 [37:38<11:06,  3.25s/it]INFO:root:global_step: 795, logpy: 139.669, kl: 19.175, loss: -120.627\n",
      " 80%|███████▉  | 796/1000 [37:41<11:02,  3.25s/it]INFO:root:global_step: 796, logpy: 139.743, kl: 19.193, loss: -120.681\n",
      " 80%|███████▉  | 797/1000 [37:44<11:02,  3.26s/it]INFO:root:global_step: 797, logpy: 139.864, kl: 19.212, loss: -120.782\n",
      " 80%|███████▉  | 798/1000 [37:48<11:06,  3.30s/it]INFO:root:global_step: 798, logpy: 139.968, kl: 19.225, loss: -120.872\n",
      " 80%|███████▉  | 799/1000 [37:51<11:00,  3.29s/it]INFO:root:global_step: 799, logpy: 140.014, kl: 19.242, loss: -120.899\n",
      " 80%|████████  | 800/1000 [37:54<10:56,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_800.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 800, logpy: 140.078, kl: 19.259, loss: -120.946\n",
      " 80%|████████  | 801/1000 [37:59<12:12,  3.68s/it]INFO:root:global_step: 801, logpy: 140.109, kl: 19.279, loss: -120.955\n",
      " 80%|████████  | 802/1000 [38:02<11:46,  3.57s/it]INFO:root:global_step: 802, logpy: 140.140, kl: 19.298, loss: -120.966\n",
      " 80%|████████  | 803/1000 [38:05<11:33,  3.52s/it]INFO:root:global_step: 803, logpy: 140.210, kl: 19.314, loss: -121.019\n",
      " 80%|████████  | 804/1000 [38:09<11:18,  3.46s/it]INFO:root:global_step: 804, logpy: 140.297, kl: 19.326, loss: -121.092\n",
      " 80%|████████  | 805/1000 [38:12<11:03,  3.40s/it]INFO:root:global_step: 805, logpy: 140.376, kl: 19.345, loss: -121.152\n",
      " 81%|████████  | 806/1000 [38:15<10:56,  3.38s/it]INFO:root:global_step: 806, logpy: 140.462, kl: 19.362, loss: -121.219\n",
      " 81%|████████  | 807/1000 [38:19<10:43,  3.33s/it]INFO:root:global_step: 807, logpy: 140.511, kl: 19.380, loss: -121.248\n",
      " 81%|████████  | 808/1000 [38:22<10:33,  3.30s/it]INFO:root:global_step: 808, logpy: 140.568, kl: 19.395, loss: -121.289\n",
      " 81%|████████  | 809/1000 [38:25<10:23,  3.27s/it]INFO:root:global_step: 809, logpy: 140.607, kl: 19.412, loss: -121.310\n",
      " 81%|████████  | 810/1000 [38:28<10:18,  3.25s/it]INFO:root:global_step: 810, logpy: 140.667, kl: 19.430, loss: -121.351\n",
      " 81%|████████  | 811/1000 [38:31<10:14,  3.25s/it]INFO:root:global_step: 811, logpy: 140.709, kl: 19.447, loss: -121.375\n",
      " 81%|████████  | 812/1000 [38:35<10:17,  3.29s/it]INFO:root:global_step: 812, logpy: 140.762, kl: 19.462, loss: -121.412\n",
      " 81%|████████▏ | 813/1000 [38:38<10:10,  3.26s/it]INFO:root:global_step: 813, logpy: 140.838, kl: 19.476, loss: -121.473\n",
      " 81%|████████▏ | 814/1000 [38:41<10:07,  3.27s/it]INFO:root:global_step: 814, logpy: 140.918, kl: 19.495, loss: -121.533\n",
      " 82%|████████▏ | 815/1000 [38:44<09:57,  3.23s/it]INFO:root:global_step: 815, logpy: 140.987, kl: 19.513, loss: -121.582\n",
      " 82%|████████▏ | 816/1000 [38:48<09:54,  3.23s/it]INFO:root:global_step: 816, logpy: 141.056, kl: 19.532, loss: -121.632\n",
      " 82%|████████▏ | 817/1000 [38:51<09:51,  3.23s/it]INFO:root:global_step: 817, logpy: 141.107, kl: 19.547, loss: -121.667\n",
      " 82%|████████▏ | 818/1000 [38:54<09:50,  3.25s/it]INFO:root:global_step: 818, logpy: 141.191, kl: 19.561, loss: -121.736\n",
      " 82%|████████▏ | 819/1000 [38:57<09:47,  3.24s/it]INFO:root:global_step: 819, logpy: 141.241, kl: 19.577, loss: -121.768\n",
      " 82%|████████▏ | 820/1000 [39:01<09:49,  3.27s/it]INFO:root:global_step: 820, logpy: 141.285, kl: 19.596, loss: -121.792\n",
      " 82%|████████▏ | 821/1000 [39:04<09:44,  3.26s/it]INFO:root:global_step: 821, logpy: 141.358, kl: 19.615, loss: -121.846\n",
      " 82%|████████▏ | 822/1000 [39:07<09:38,  3.25s/it]INFO:root:global_step: 822, logpy: 141.431, kl: 19.631, loss: -121.901\n",
      " 82%|████████▏ | 823/1000 [39:11<09:40,  3.28s/it]INFO:root:global_step: 823, logpy: 141.517, kl: 19.644, loss: -121.973\n",
      " 82%|████████▏ | 824/1000 [39:14<09:34,  3.27s/it]INFO:root:global_step: 824, logpy: 141.591, kl: 19.655, loss: -122.036\n",
      " 82%|████████▎ | 825/1000 [39:17<09:32,  3.27s/it]INFO:root:global_step: 825, logpy: 141.646, kl: 19.671, loss: -122.074\n",
      " 83%|████████▎ | 826/1000 [39:20<09:30,  3.28s/it]INFO:root:global_step: 826, logpy: 141.719, kl: 19.688, loss: -122.129\n",
      " 83%|████████▎ | 827/1000 [39:24<09:28,  3.29s/it]INFO:root:global_step: 827, logpy: 141.792, kl: 19.702, loss: -122.187\n",
      " 83%|████████▎ | 828/1000 [39:27<09:26,  3.29s/it]INFO:root:global_step: 828, logpy: 141.875, kl: 19.713, loss: -122.258\n",
      " 83%|████████▎ | 829/1000 [39:30<09:21,  3.29s/it]INFO:root:global_step: 829, logpy: 141.927, kl: 19.732, loss: -122.289\n",
      " 83%|████████▎ | 830/1000 [39:34<09:18,  3.29s/it]INFO:root:global_step: 830, logpy: 142.006, kl: 19.750, loss: -122.349\n",
      " 83%|████████▎ | 831/1000 [39:37<09:15,  3.29s/it]INFO:root:global_step: 831, logpy: 142.071, kl: 19.764, loss: -122.400\n",
      " 83%|████████▎ | 832/1000 [39:40<09:11,  3.28s/it]INFO:root:global_step: 832, logpy: 142.109, kl: 19.780, loss: -122.421\n",
      " 83%|████████▎ | 833/1000 [39:43<09:10,  3.30s/it]INFO:root:global_step: 833, logpy: 142.187, kl: 19.798, loss: -122.480\n",
      " 83%|████████▎ | 834/1000 [39:47<09:16,  3.35s/it]INFO:root:global_step: 834, logpy: 142.277, kl: 19.811, loss: -122.556\n",
      " 84%|████████▎ | 835/1000 [39:50<09:12,  3.35s/it]INFO:root:global_step: 835, logpy: 142.338, kl: 19.830, loss: -122.597\n",
      " 84%|████████▎ | 836/1000 [39:54<09:07,  3.34s/it]INFO:root:global_step: 836, logpy: 142.395, kl: 19.844, loss: -122.639\n",
      " 84%|████████▎ | 837/1000 [39:57<09:02,  3.33s/it]INFO:root:global_step: 837, logpy: 142.463, kl: 19.862, loss: -122.689\n",
      " 84%|████████▍ | 838/1000 [40:00<08:54,  3.30s/it]INFO:root:global_step: 838, logpy: 142.537, kl: 19.877, loss: -122.746\n",
      " 84%|████████▍ | 839/1000 [40:03<08:51,  3.30s/it]INFO:root:global_step: 839, logpy: 142.607, kl: 19.892, loss: -122.801\n",
      " 84%|████████▍ | 840/1000 [40:07<08:45,  3.28s/it]INFO:root:global_step: 840, logpy: 142.656, kl: 19.904, loss: -122.836\n",
      " 84%|████████▍ | 841/1000 [40:10<08:43,  3.29s/it]INFO:root:global_step: 841, logpy: 142.721, kl: 19.920, loss: -122.885\n",
      " 84%|████████▍ | 842/1000 [40:13<08:40,  3.29s/it]INFO:root:global_step: 842, logpy: 142.791, kl: 19.933, loss: -122.941\n",
      " 84%|████████▍ | 843/1000 [40:17<08:35,  3.28s/it]INFO:root:global_step: 843, logpy: 142.835, kl: 19.951, loss: -122.966\n",
      " 84%|████████▍ | 844/1000 [40:20<08:31,  3.28s/it]INFO:root:global_step: 844, logpy: 142.909, kl: 19.968, loss: -123.022\n",
      " 84%|████████▍ | 845/1000 [40:23<08:26,  3.27s/it]INFO:root:global_step: 845, logpy: 142.977, kl: 19.980, loss: -123.078\n",
      " 85%|████████▍ | 846/1000 [40:26<08:24,  3.28s/it]INFO:root:global_step: 846, logpy: 143.075, kl: 20.002, loss: -123.153\n",
      " 85%|████████▍ | 847/1000 [40:30<08:26,  3.31s/it]INFO:root:global_step: 847, logpy: 143.123, kl: 20.020, loss: -123.183\n",
      " 85%|████████▍ | 848/1000 [40:33<08:19,  3.29s/it]INFO:root:global_step: 848, logpy: 143.173, kl: 20.043, loss: -123.208\n",
      " 85%|████████▍ | 849/1000 [40:36<08:15,  3.28s/it]INFO:root:global_step: 849, logpy: 143.232, kl: 20.058, loss: -123.251\n",
      " 85%|████████▌ | 850/1000 [40:39<08:10,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_850.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 850, logpy: 143.310, kl: 20.073, loss: -123.313\n",
      " 85%|████████▌ | 851/1000 [40:44<09:01,  3.63s/it]INFO:root:global_step: 851, logpy: 143.381, kl: 20.086, loss: -123.370\n",
      " 85%|████████▌ | 852/1000 [40:47<08:48,  3.57s/it]INFO:root:global_step: 852, logpy: 143.439, kl: 20.105, loss: -123.409\n",
      " 85%|████████▌ | 853/1000 [40:51<08:33,  3.49s/it]INFO:root:global_step: 853, logpy: 143.512, kl: 20.122, loss: -123.464\n",
      " 85%|████████▌ | 854/1000 [40:54<08:18,  3.42s/it]INFO:root:global_step: 854, logpy: 143.573, kl: 20.136, loss: -123.510\n",
      " 86%|████████▌ | 855/1000 [40:57<08:12,  3.39s/it]INFO:root:global_step: 855, logpy: 143.629, kl: 20.147, loss: -123.554\n",
      " 86%|████████▌ | 856/1000 [41:01<08:10,  3.40s/it]INFO:root:global_step: 856, logpy: 143.709, kl: 20.159, loss: -123.622\n",
      " 86%|████████▌ | 857/1000 [41:04<07:58,  3.35s/it]INFO:root:global_step: 857, logpy: 143.798, kl: 20.173, loss: -123.696\n",
      " 86%|████████▌ | 858/1000 [41:07<07:50,  3.32s/it]INFO:root:global_step: 858, logpy: 143.870, kl: 20.186, loss: -123.755\n",
      " 86%|████████▌ | 859/1000 [41:11<07:55,  3.37s/it]INFO:root:global_step: 859, logpy: 143.916, kl: 20.202, loss: -123.783\n",
      " 86%|████████▌ | 860/1000 [41:14<07:50,  3.36s/it]INFO:root:global_step: 860, logpy: 143.993, kl: 20.221, loss: -123.841\n",
      " 86%|████████▌ | 861/1000 [41:17<07:42,  3.32s/it]INFO:root:global_step: 861, logpy: 144.035, kl: 20.240, loss: -123.863\n",
      " 86%|████████▌ | 862/1000 [41:21<07:40,  3.33s/it]INFO:root:global_step: 862, logpy: 144.113, kl: 20.252, loss: -123.929\n",
      " 86%|████████▋ | 863/1000 [41:24<07:40,  3.36s/it]INFO:root:global_step: 863, logpy: 144.152, kl: 20.270, loss: -123.950\n",
      " 86%|████████▋ | 864/1000 [41:27<07:33,  3.33s/it]INFO:root:global_step: 864, logpy: 144.208, kl: 20.285, loss: -123.990\n",
      " 86%|████████▋ | 865/1000 [41:31<07:27,  3.31s/it]INFO:root:global_step: 865, logpy: 144.279, kl: 20.297, loss: -124.048\n",
      " 87%|████████▋ | 866/1000 [41:34<07:21,  3.30s/it]INFO:root:global_step: 866, logpy: 144.323, kl: 20.312, loss: -124.076\n",
      " 87%|████████▋ | 867/1000 [41:37<07:19,  3.30s/it]INFO:root:global_step: 867, logpy: 144.381, kl: 20.329, loss: -124.117\n",
      " 87%|████████▋ | 868/1000 [41:40<07:17,  3.31s/it]INFO:root:global_step: 868, logpy: 144.416, kl: 20.344, loss: -124.135\n",
      " 87%|████████▋ | 869/1000 [41:44<07:18,  3.35s/it]INFO:root:global_step: 869, logpy: 144.490, kl: 20.358, loss: -124.195\n",
      " 87%|████████▋ | 870/1000 [41:47<07:19,  3.38s/it]INFO:root:global_step: 870, logpy: 144.537, kl: 20.371, loss: -124.229\n",
      " 87%|████████▋ | 871/1000 [41:51<07:21,  3.42s/it]INFO:root:global_step: 871, logpy: 144.590, kl: 20.386, loss: -124.266\n",
      " 87%|████████▋ | 872/1000 [41:54<07:17,  3.42s/it]INFO:root:global_step: 872, logpy: 144.655, kl: 20.400, loss: -124.316\n",
      " 87%|████████▋ | 873/1000 [41:58<07:16,  3.44s/it]INFO:root:global_step: 873, logpy: 144.712, kl: 20.417, loss: -124.356\n",
      " 87%|████████▋ | 874/1000 [42:01<07:14,  3.45s/it]INFO:root:global_step: 874, logpy: 144.751, kl: 20.432, loss: -124.379\n",
      " 88%|████████▊ | 875/1000 [42:04<07:02,  3.38s/it]INFO:root:global_step: 875, logpy: 144.828, kl: 20.450, loss: -124.437\n",
      " 88%|████████▊ | 876/1000 [42:08<06:54,  3.35s/it]INFO:root:global_step: 876, logpy: 144.873, kl: 20.466, loss: -124.465\n",
      " 88%|████████▊ | 877/1000 [42:11<06:52,  3.36s/it]INFO:root:global_step: 877, logpy: 144.950, kl: 20.481, loss: -124.527\n",
      " 88%|████████▊ | 878/1000 [42:14<06:47,  3.34s/it]INFO:root:global_step: 878, logpy: 144.990, kl: 20.500, loss: -124.548\n",
      " 88%|████████▊ | 879/1000 [42:18<06:41,  3.32s/it]INFO:root:global_step: 879, logpy: 145.068, kl: 20.513, loss: -124.612\n",
      " 88%|████████▊ | 880/1000 [42:21<06:36,  3.30s/it]INFO:root:global_step: 880, logpy: 145.147, kl: 20.530, loss: -124.673\n",
      " 88%|████████▊ | 881/1000 [42:24<06:32,  3.29s/it]INFO:root:global_step: 881, logpy: 145.223, kl: 20.546, loss: -124.733\n",
      " 88%|████████▊ | 882/1000 [42:28<06:30,  3.31s/it]INFO:root:global_step: 882, logpy: 145.303, kl: 20.565, loss: -124.793\n",
      " 88%|████████▊ | 883/1000 [42:31<06:25,  3.30s/it]INFO:root:global_step: 883, logpy: 145.340, kl: 20.579, loss: -124.816\n",
      " 88%|████████▊ | 884/1000 [42:34<06:21,  3.29s/it]INFO:root:global_step: 884, logpy: 145.419, kl: 20.590, loss: -124.884\n",
      " 88%|████████▊ | 885/1000 [42:37<06:18,  3.29s/it]INFO:root:global_step: 885, logpy: 145.473, kl: 20.605, loss: -124.922\n",
      " 89%|████████▊ | 886/1000 [42:41<06:14,  3.28s/it]INFO:root:global_step: 886, logpy: 145.546, kl: 20.614, loss: -124.985\n",
      " 89%|████████▊ | 887/1000 [42:44<06:15,  3.32s/it]INFO:root:global_step: 887, logpy: 145.610, kl: 20.629, loss: -125.033\n",
      " 89%|████████▉ | 888/1000 [42:47<06:10,  3.31s/it]INFO:root:global_step: 888, logpy: 145.680, kl: 20.646, loss: -125.086\n",
      " 89%|████████▉ | 889/1000 [42:51<06:05,  3.29s/it]INFO:root:global_step: 889, logpy: 145.715, kl: 20.664, loss: -125.102\n",
      " 89%|████████▉ | 890/1000 [42:54<06:01,  3.29s/it]INFO:root:global_step: 890, logpy: 145.786, kl: 20.677, loss: -125.160\n",
      " 89%|████████▉ | 891/1000 [42:57<05:59,  3.29s/it]INFO:root:global_step: 891, logpy: 145.866, kl: 20.696, loss: -125.221\n",
      " 89%|████████▉ | 892/1000 [43:00<05:53,  3.27s/it]INFO:root:global_step: 892, logpy: 145.957, kl: 20.712, loss: -125.294\n",
      " 89%|████████▉ | 893/1000 [43:04<05:52,  3.30s/it]INFO:root:global_step: 893, logpy: 146.007, kl: 20.724, loss: -125.332\n",
      " 89%|████████▉ | 894/1000 [43:07<05:54,  3.35s/it]INFO:root:global_step: 894, logpy: 146.077, kl: 20.739, loss: -125.388\n",
      " 90%|████████▉ | 895/1000 [43:11<06:04,  3.47s/it]INFO:root:global_step: 895, logpy: 146.124, kl: 20.754, loss: -125.419\n",
      " 90%|████████▉ | 896/1000 [43:14<05:59,  3.46s/it]INFO:root:global_step: 896, logpy: 146.181, kl: 20.768, loss: -125.461\n",
      " 90%|████████▉ | 897/1000 [43:18<05:54,  3.44s/it]INFO:root:global_step: 897, logpy: 146.225, kl: 20.788, loss: -125.485\n",
      " 90%|████████▉ | 898/1000 [43:21<05:46,  3.39s/it]INFO:root:global_step: 898, logpy: 146.290, kl: 20.801, loss: -125.536\n",
      " 90%|████████▉ | 899/1000 [43:24<05:40,  3.37s/it]INFO:root:global_step: 899, logpy: 146.307, kl: 20.820, loss: -125.533\n",
      " 90%|█████████ | 900/1000 [43:28<05:34,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_900.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 900, logpy: 146.348, kl: 20.834, loss: -125.561\n",
      " 90%|█████████ | 901/1000 [43:33<06:18,  3.82s/it]INFO:root:global_step: 901, logpy: 146.375, kl: 20.854, loss: -125.567\n",
      " 90%|█████████ | 902/1000 [43:36<06:05,  3.73s/it]INFO:root:global_step: 902, logpy: 146.418, kl: 20.867, loss: -125.596\n",
      " 90%|█████████ | 903/1000 [43:40<05:53,  3.65s/it]INFO:root:global_step: 903, logpy: 146.467, kl: 20.886, loss: -125.625\n",
      " 90%|█████████ | 904/1000 [43:43<05:45,  3.60s/it]INFO:root:global_step: 904, logpy: 146.539, kl: 20.904, loss: -125.680\n",
      " 90%|█████████ | 905/1000 [43:47<05:37,  3.55s/it]INFO:root:global_step: 905, logpy: 146.605, kl: 20.917, loss: -125.733\n",
      " 91%|█████████ | 906/1000 [43:50<05:26,  3.47s/it]INFO:root:global_step: 906, logpy: 146.645, kl: 20.930, loss: -125.759\n",
      " 91%|█████████ | 907/1000 [43:53<05:17,  3.41s/it]INFO:root:global_step: 907, logpy: 146.717, kl: 20.945, loss: -125.815\n",
      " 91%|█████████ | 908/1000 [43:56<05:14,  3.41s/it]INFO:root:global_step: 908, logpy: 146.792, kl: 20.962, loss: -125.872\n",
      " 91%|█████████ | 909/1000 [44:00<05:07,  3.38s/it]INFO:root:global_step: 909, logpy: 146.840, kl: 20.978, loss: -125.904\n",
      " 91%|█████████ | 910/1000 [44:03<05:02,  3.36s/it]INFO:root:global_step: 910, logpy: 146.879, kl: 20.998, loss: -125.922\n",
      " 91%|█████████ | 911/1000 [44:06<04:57,  3.34s/it]INFO:root:global_step: 911, logpy: 146.922, kl: 21.008, loss: -125.955\n",
      " 91%|█████████ | 912/1000 [44:10<04:58,  3.39s/it]INFO:root:global_step: 912, logpy: 146.947, kl: 21.026, loss: -125.962\n",
      " 91%|█████████▏| 913/1000 [44:13<04:52,  3.36s/it]INFO:root:global_step: 913, logpy: 146.995, kl: 21.046, loss: -125.990\n",
      " 91%|█████████▏| 914/1000 [44:17<04:51,  3.39s/it]INFO:root:global_step: 914, logpy: 147.056, kl: 21.061, loss: -126.035\n",
      " 92%|█████████▏| 915/1000 [44:20<04:49,  3.41s/it]INFO:root:global_step: 915, logpy: 147.070, kl: 21.083, loss: -126.027\n",
      " 92%|█████████▏| 916/1000 [44:24<04:47,  3.42s/it]INFO:root:global_step: 916, logpy: 147.131, kl: 21.098, loss: -126.073\n",
      " 92%|█████████▏| 917/1000 [44:27<04:42,  3.40s/it]INFO:root:global_step: 917, logpy: 147.220, kl: 21.111, loss: -126.148\n",
      " 92%|█████████▏| 918/1000 [44:30<04:37,  3.39s/it]INFO:root:global_step: 918, logpy: 147.263, kl: 21.124, loss: -126.178\n",
      " 92%|█████████▏| 919/1000 [44:34<04:33,  3.38s/it]INFO:root:global_step: 919, logpy: 147.328, kl: 21.137, loss: -126.229\n",
      " 92%|█████████▏| 920/1000 [44:37<04:29,  3.37s/it]INFO:root:global_step: 920, logpy: 147.426, kl: 21.153, loss: -126.311\n",
      " 92%|█████████▏| 921/1000 [44:40<04:26,  3.37s/it]INFO:root:global_step: 921, logpy: 147.487, kl: 21.170, loss: -126.355\n",
      " 92%|█████████▏| 922/1000 [44:44<04:21,  3.36s/it]INFO:root:global_step: 922, logpy: 147.528, kl: 21.184, loss: -126.381\n",
      " 92%|█████████▏| 923/1000 [44:47<04:18,  3.36s/it]INFO:root:global_step: 923, logpy: 147.566, kl: 21.196, loss: -126.407\n",
      " 92%|█████████▏| 924/1000 [44:50<04:14,  3.35s/it]INFO:root:global_step: 924, logpy: 147.594, kl: 21.212, loss: -126.419\n",
      " 92%|█████████▎| 925/1000 [44:54<04:13,  3.38s/it]INFO:root:global_step: 925, logpy: 147.657, kl: 21.227, loss: -126.465\n",
      " 93%|█████████▎| 926/1000 [44:57<04:09,  3.37s/it]INFO:root:global_step: 926, logpy: 147.707, kl: 21.242, loss: -126.501\n",
      " 93%|█████████▎| 927/1000 [45:01<04:05,  3.36s/it]INFO:root:global_step: 927, logpy: 147.775, kl: 21.255, loss: -126.555\n",
      " 93%|█████████▎| 928/1000 [45:04<04:03,  3.38s/it]INFO:root:global_step: 928, logpy: 147.854, kl: 21.269, loss: -126.620\n",
      " 93%|█████████▎| 929/1000 [45:07<03:59,  3.38s/it]INFO:root:global_step: 929, logpy: 147.912, kl: 21.285, loss: -126.661\n",
      " 93%|█████████▎| 930/1000 [45:11<03:57,  3.39s/it]INFO:root:global_step: 930, logpy: 147.979, kl: 21.302, loss: -126.711\n",
      " 93%|█████████▎| 931/1000 [45:14<03:54,  3.40s/it]INFO:root:global_step: 931, logpy: 148.037, kl: 21.317, loss: -126.754\n",
      " 93%|█████████▎| 932/1000 [45:18<03:50,  3.39s/it]INFO:root:global_step: 932, logpy: 148.082, kl: 21.330, loss: -126.786\n",
      " 93%|█████████▎| 933/1000 [45:21<03:45,  3.37s/it]INFO:root:global_step: 933, logpy: 148.117, kl: 21.344, loss: -126.807\n",
      " 93%|█████████▎| 934/1000 [45:24<03:42,  3.37s/it]INFO:root:global_step: 934, logpy: 148.183, kl: 21.359, loss: -126.857\n",
      " 94%|█████████▎| 935/1000 [45:28<03:39,  3.38s/it]INFO:root:global_step: 935, logpy: 148.203, kl: 21.379, loss: -126.856\n",
      " 94%|█████████▎| 936/1000 [45:31<03:34,  3.36s/it]INFO:root:global_step: 936, logpy: 148.223, kl: 21.394, loss: -126.861\n",
      " 94%|█████████▎| 937/1000 [45:34<03:31,  3.35s/it]INFO:root:global_step: 937, logpy: 148.257, kl: 21.413, loss: -126.875\n",
      " 94%|█████████▍| 938/1000 [45:38<03:27,  3.35s/it]INFO:root:global_step: 938, logpy: 148.281, kl: 21.430, loss: -126.883\n",
      " 94%|█████████▍| 939/1000 [45:41<03:27,  3.39s/it]INFO:root:global_step: 939, logpy: 148.328, kl: 21.452, loss: -126.907\n",
      " 94%|█████████▍| 940/1000 [45:45<03:24,  3.41s/it]INFO:root:global_step: 940, logpy: 148.390, kl: 21.462, loss: -126.959\n",
      " 94%|█████████▍| 941/1000 [45:48<03:22,  3.43s/it]INFO:root:global_step: 941, logpy: 148.426, kl: 21.474, loss: -126.982\n",
      " 94%|█████████▍| 942/1000 [45:51<03:17,  3.40s/it]INFO:root:global_step: 942, logpy: 148.464, kl: 21.487, loss: -127.008\n",
      " 94%|█████████▍| 943/1000 [45:55<03:12,  3.37s/it]INFO:root:global_step: 943, logpy: 148.486, kl: 21.499, loss: -127.017\n",
      " 94%|█████████▍| 944/1000 [45:58<03:08,  3.37s/it]INFO:root:global_step: 944, logpy: 148.522, kl: 21.512, loss: -127.040\n",
      " 94%|█████████▍| 945/1000 [46:01<03:06,  3.38s/it]INFO:root:global_step: 945, logpy: 148.563, kl: 21.528, loss: -127.065\n",
      " 95%|█████████▍| 946/1000 [46:05<03:01,  3.37s/it]INFO:root:global_step: 946, logpy: 148.621, kl: 21.545, loss: -127.105\n",
      " 95%|█████████▍| 947/1000 [46:08<02:58,  3.36s/it]INFO:root:global_step: 947, logpy: 148.694, kl: 21.556, loss: -127.168\n",
      " 95%|█████████▍| 948/1000 [46:12<02:55,  3.37s/it]INFO:root:global_step: 948, logpy: 148.737, kl: 21.574, loss: -127.192\n",
      " 95%|█████████▍| 949/1000 [46:15<02:51,  3.36s/it]INFO:root:global_step: 949, logpy: 148.788, kl: 21.588, loss: -127.229\n",
      " 95%|█████████▌| 950/1000 [46:18<02:47,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_950.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 950, logpy: 148.844, kl: 21.606, loss: -127.265\n",
      " 95%|█████████▌| 951/1000 [46:23<03:03,  3.74s/it]INFO:root:global_step: 951, logpy: 148.900, kl: 21.619, loss: -127.309\n",
      " 95%|█████████▌| 952/1000 [46:27<02:58,  3.73s/it]INFO:root:global_step: 952, logpy: 148.957, kl: 21.636, loss: -127.348\n",
      " 95%|█████████▌| 953/1000 [46:30<02:50,  3.63s/it]INFO:root:global_step: 953, logpy: 149.010, kl: 21.647, loss: -127.390\n",
      " 95%|█████████▌| 954/1000 [46:33<02:43,  3.56s/it]INFO:root:global_step: 954, logpy: 149.085, kl: 21.660, loss: -127.453\n",
      " 96%|█████████▌| 955/1000 [46:37<02:37,  3.49s/it]INFO:root:global_step: 955, logpy: 149.124, kl: 21.676, loss: -127.475\n",
      " 96%|█████████▌| 956/1000 [46:40<02:31,  3.44s/it]INFO:root:global_step: 956, logpy: 149.171, kl: 21.690, loss: -127.507\n",
      " 96%|█████████▌| 957/1000 [46:43<02:26,  3.41s/it]INFO:root:global_step: 957, logpy: 149.208, kl: 21.701, loss: -127.534\n",
      " 96%|█████████▌| 958/1000 [46:47<02:22,  3.40s/it]INFO:root:global_step: 958, logpy: 149.265, kl: 21.714, loss: -127.576\n",
      " 96%|█████████▌| 959/1000 [46:50<02:18,  3.39s/it]INFO:root:global_step: 959, logpy: 149.318, kl: 21.729, loss: -127.614\n",
      " 96%|█████████▌| 960/1000 [46:53<02:14,  3.37s/it]INFO:root:global_step: 960, logpy: 149.378, kl: 21.744, loss: -127.659\n",
      " 96%|█████████▌| 961/1000 [46:57<02:11,  3.36s/it]INFO:root:global_step: 961, logpy: 149.424, kl: 21.756, loss: -127.692\n",
      " 96%|█████████▌| 962/1000 [47:00<02:07,  3.36s/it]INFO:root:global_step: 962, logpy: 149.451, kl: 21.767, loss: -127.709\n",
      " 96%|█████████▋| 963/1000 [47:03<02:04,  3.36s/it]INFO:root:global_step: 963, logpy: 149.516, kl: 21.782, loss: -127.759\n",
      " 96%|█████████▋| 964/1000 [47:07<02:00,  3.36s/it]INFO:root:global_step: 964, logpy: 149.555, kl: 21.798, loss: -127.781\n",
      " 96%|█████████▋| 965/1000 [47:10<01:59,  3.41s/it]INFO:root:global_step: 965, logpy: 149.613, kl: 21.811, loss: -127.826\n",
      " 97%|█████████▋| 966/1000 [47:14<01:55,  3.40s/it]INFO:root:global_step: 966, logpy: 149.664, kl: 21.826, loss: -127.862\n",
      " 97%|█████████▋| 967/1000 [47:17<01:52,  3.41s/it]INFO:root:global_step: 967, logpy: 149.732, kl: 21.837, loss: -127.918\n",
      " 97%|█████████▋| 968/1000 [47:21<01:48,  3.40s/it]INFO:root:global_step: 968, logpy: 149.776, kl: 21.852, loss: -127.947\n",
      " 97%|█████████▋| 969/1000 [47:24<01:44,  3.38s/it]INFO:root:global_step: 969, logpy: 149.811, kl: 21.868, loss: -127.966\n",
      " 97%|█████████▋| 970/1000 [47:27<01:42,  3.41s/it]INFO:root:global_step: 970, logpy: 149.867, kl: 21.886, loss: -128.004\n",
      " 97%|█████████▋| 971/1000 [47:31<01:38,  3.38s/it]INFO:root:global_step: 971, logpy: 149.915, kl: 21.900, loss: -128.037\n",
      " 97%|█████████▋| 972/1000 [47:34<01:34,  3.38s/it]INFO:root:global_step: 972, logpy: 149.965, kl: 21.910, loss: -128.078\n",
      " 97%|█████████▋| 973/1000 [47:37<01:31,  3.38s/it]INFO:root:global_step: 973, logpy: 149.998, kl: 21.924, loss: -128.096\n",
      " 97%|█████████▋| 974/1000 [47:41<01:28,  3.40s/it]INFO:root:global_step: 974, logpy: 150.025, kl: 21.939, loss: -128.107\n",
      " 98%|█████████▊| 975/1000 [47:44<01:26,  3.45s/it]INFO:root:global_step: 975, logpy: 150.090, kl: 21.957, loss: -128.155\n",
      " 98%|█████████▊| 976/1000 [47:48<01:22,  3.43s/it]INFO:root:global_step: 976, logpy: 150.116, kl: 21.973, loss: -128.165\n",
      " 98%|█████████▊| 977/1000 [47:51<01:18,  3.41s/it]INFO:root:global_step: 977, logpy: 150.164, kl: 21.981, loss: -128.204\n",
      " 98%|█████████▊| 978/1000 [47:55<01:15,  3.43s/it]INFO:root:global_step: 978, logpy: 150.230, kl: 21.993, loss: -128.258\n",
      " 98%|█████████▊| 979/1000 [47:58<01:11,  3.41s/it]INFO:root:global_step: 979, logpy: 150.253, kl: 22.006, loss: -128.268\n",
      " 98%|█████████▊| 980/1000 [48:01<01:08,  3.40s/it]INFO:root:global_step: 980, logpy: 150.318, kl: 22.022, loss: -128.317\n",
      " 98%|█████████▊| 981/1000 [48:05<01:04,  3.39s/it]INFO:root:global_step: 981, logpy: 150.361, kl: 22.034, loss: -128.347\n",
      " 98%|█████████▊| 982/1000 [48:08<01:01,  3.39s/it]INFO:root:global_step: 982, logpy: 150.396, kl: 22.045, loss: -128.371\n",
      " 98%|█████████▊| 983/1000 [48:12<00:57,  3.40s/it]INFO:root:global_step: 983, logpy: 150.419, kl: 22.057, loss: -128.382\n",
      " 98%|█████████▊| 984/1000 [48:15<00:54,  3.38s/it]INFO:root:global_step: 984, logpy: 150.481, kl: 22.071, loss: -128.430\n",
      " 98%|█████████▊| 985/1000 [48:18<00:50,  3.37s/it]INFO:root:global_step: 985, logpy: 150.550, kl: 22.082, loss: -128.487\n",
      " 99%|█████████▊| 986/1000 [48:22<00:46,  3.35s/it]INFO:root:global_step: 986, logpy: 150.574, kl: 22.102, loss: -128.492\n",
      " 99%|█████████▊| 987/1000 [48:25<00:43,  3.34s/it]INFO:root:global_step: 987, logpy: 150.605, kl: 22.110, loss: -128.514\n",
      " 99%|█████████▉| 988/1000 [48:28<00:40,  3.35s/it]INFO:root:global_step: 988, logpy: 150.639, kl: 22.124, loss: -128.534\n",
      " 99%|█████████▉| 989/1000 [48:32<00:36,  3.36s/it]INFO:root:global_step: 989, logpy: 150.685, kl: 22.134, loss: -128.570\n",
      " 99%|█████████▉| 990/1000 [48:35<00:33,  3.35s/it]INFO:root:global_step: 990, logpy: 150.745, kl: 22.145, loss: -128.619\n",
      " 99%|█████████▉| 991/1000 [48:38<00:30,  3.35s/it]INFO:root:global_step: 991, logpy: 150.787, kl: 22.159, loss: -128.646\n",
      " 99%|█████████▉| 992/1000 [48:42<00:26,  3.36s/it]INFO:root:global_step: 992, logpy: 150.829, kl: 22.172, loss: -128.675\n",
      " 99%|█████████▉| 993/1000 [48:45<00:23,  3.40s/it]INFO:root:global_step: 993, logpy: 150.889, kl: 22.185, loss: -128.722\n",
      " 99%|█████████▉| 994/1000 [48:49<00:20,  3.39s/it]INFO:root:global_step: 994, logpy: 150.939, kl: 22.200, loss: -128.757\n",
      "100%|█████████▉| 995/1000 [48:52<00:16,  3.39s/it]INFO:root:global_step: 995, logpy: 151.012, kl: 22.216, loss: -128.814\n",
      "100%|█████████▉| 996/1000 [48:55<00:13,  3.40s/it]INFO:root:global_step: 996, logpy: 151.066, kl: 22.230, loss: -128.854\n",
      "100%|█████████▉| 997/1000 [48:59<00:10,  3.41s/it]INFO:root:global_step: 997, logpy: 151.105, kl: 22.243, loss: -128.879\n",
      "100%|█████████▉| 998/1000 [49:02<00:06,  3.39s/it]INFO:root:global_step: 998, logpy: 151.119, kl: 22.260, loss: -128.876\n",
      "100%|█████████▉| 999/1000 [49:06<00:03,  3.40s/it]INFO:root:global_step: 999, logpy: 151.145, kl: 22.275, loss: -128.887\n",
      "100%|██████████| 1000/1000 [49:09<00:00,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "manual_seed(args['seed'])\n",
    "\n",
    "if args['debug']:\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "ckpt_dir = os.path.join('./sim/', 'ckpts')\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "sdeint_fn = torchsde.sdeint_adjoint if args['adjoint'] else torchsde.sdeint\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356ad31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
