{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4dd7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from typing import Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import distributions, nn, optim\n",
    "\n",
    "import torchsde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0aeebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the gpu is available or not, if yes, use gpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2902c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the tuple for data, \n",
    "Data = namedtuple('Data', ['ts_', 'ts_ext_', 'ts_vis_', 'ts', 'ts_ext', 'ts_vis', 'ys', 'ys_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9198a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"train_iters\": 1000,\n",
    "    \"pause_iters\": 50,\n",
    "    \"hide_ticks\": False,\n",
    "    \"save_ckpt\":True,\n",
    "    \"likelihood\":\"laplace\",\n",
    "    \"scale\": 0.001,\n",
    "    \"adjoint\": True,\n",
    "    \"debug\": True,\n",
    "    \"seed\": 42,\n",
    "    'data':'segmented_cosine',\n",
    "    \"dt\": 1e-2,\n",
    "    \"batch_size\": 256,\n",
    "    \"method\": 'euler',\n",
    "    \"adaptive\": 'False',\n",
    "    \"rtol\": 1e-3,\n",
    "    \"atol\": 1e-3\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40420fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output values from 0 to maxval with iters steps\n",
    "class LinearScheduler(object):\n",
    "    def __init__(self, iters, maxval=1.0):\n",
    "        self._iters = max(1,iters)\n",
    "        self._val = maxval/self._iters\n",
    "        self._maxval = maxval\n",
    "    \n",
    "    def step(self):\n",
    "        self._val = min(self._maxval, self._val+self._maxval/self._iters)\n",
    "        \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93c09c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMAMetric(object):\n",
    "    def __init__(self, gamma: Optional[float]=0.99):\n",
    "        super(EMAMetric, self).__init__()\n",
    "        self._val=0\n",
    "        self._gamma = gamma\n",
    "    def step(self, x:Union[torch.Tensor, np.ndarray]):\n",
    "        x = x.detach().cpu().numpy() if torch.is_tensor(x) else x\n",
    "        self._val = self._gamma * self._val + (1-self._gamma)*x\n",
    "        return self._val\n",
    "    \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a4ec86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "def manual_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98a2f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the dvision is stable\n",
    "def _stable_division(a,b,epsilon=1e-7):\n",
    "    b = torch.where(b.abs().detach() > epsilon, b, torch.full_like(b, fill_value=epsilon)*b.sign())\n",
    "    return a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "522d3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentSDE(torchsde.SDEIto): # sde with ito calculus\n",
    "    def __init__(self, theta=1.0, mu=0.0, sigma=1):\n",
    "        super(LatentSDE, self).__init__(noise_type=\"diagonal\")\n",
    "        logvar = math.log(sigma ** 2/(2.*theta))\n",
    "        \n",
    "        # prior drift\n",
    "        #self.register_buffer(\"theta\",torch.tensor([[theta]])) # prior parameters, register 成buffer, 参数不会进行更新\n",
    "        self.register_buffer(\"mu\", torch.tensor([[mu]]))\n",
    "        self.register_buffer(\"sigma\",torch.tensor([[sigma]]))\n",
    "        \n",
    "        # p(y0)\n",
    "        self.register_buffer(\"py0_mean\", torch.tensor([[mu]]))\n",
    "        self.register_buffer(\"py0_logvar\", torch.tensor([[logvar]]))\n",
    "        \n",
    "        # approximate posterior drift: Takes in 2 positional encodings and the state\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3,200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200,200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200,1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialization the parameters\n",
    "        self.net[-1].weight.data.fill_(0.) # 初始化最后一层的参数\n",
    "        self.net[-1].bias.data.fill_(0.)\n",
    "            \n",
    "        # q(y0)\n",
    "        self.qy0_mean = nn.Parameter(torch.tensor([[mu]]), requires_grad=True) # 创建parameters\n",
    "        self.qy0_logvar = nn.Parameter(torch.tensor([[logvar]]), requires_grad=True) # 创建parameters\n",
    "        self.theta = nn.Parameter(torch.tensor([[theta]]),requires_grad=True)\n",
    "        #self.theta = nn.Parameter(torch.tensor([[theta]]),requires_grad=True)\n",
    "            \n",
    "    def f(self, t, y):  # Approximate posterior drift.\n",
    "        if t.dim() == 0:\n",
    "            t = torch.full_like(y, fill_value=t) # create a tensor of t\n",
    "        # Positional encoding in transformers for time-inhomogeneous posterior.\n",
    "        return self.net(torch.cat((torch.sin(t), torch.cos(t), y), dim=-1))\n",
    "\n",
    "    def g(self, t, y):  # Shared diffusion.\n",
    "        return self.sigma.repeat(y.size(0), 1) # 重复复制, 创建一个size为[y.size[0],1]\n",
    "\n",
    "    def h(self, t, y):  # Prior drift.\n",
    "        return self.theta * (self.mu - y)\n",
    "\n",
    "    def f_aug(self, t, y):  # Drift for augmented dynamics with logqp term.\n",
    "        y = y[:, 0:1] # 提取第一列，保持列的形态\n",
    "        f, g, h = self.f(t, y), self.g(t, y), self.h(t, y)\n",
    "        u = _stable_division(f - h, g) # 计算u(z,t)\n",
    "        f_logqp = .5 * (u ** 2).sum(dim=1, keepdim=True) # 计算integral\n",
    "        return torch.cat([f, f_logqp], dim=1)\n",
    "\n",
    "    def g_aug(self, t, y):  # Diffusion for augmented dynamics with logqp term.\n",
    "        y = y[:, 0:1]\n",
    "        g = self.g(t, y)\n",
    "        g_logqp = torch.zeros_like(y)\n",
    "        return torch.cat([g, g_logqp], dim=1)\n",
    "\n",
    "    def forward(self, ts, batch_size, eps=None):\n",
    "        eps = torch.randn(batch_size, 1).to(self.qy0_std) if eps is None else eps\n",
    "        y0 = self.qy0_mean + eps * self.qy0_std # the latent variable\n",
    "        qy0 = distributions.Normal(loc=self.qy0_mean, scale=self.qy0_std) # approximate posterior distribution\n",
    "        py0 = distributions.Normal(loc=self.py0_mean, scale=self.py0_std) # prior distribution\n",
    "        logqp0 = distributions.kl_divergence(qy0, py0).sum(dim=1)  # KL(t=0). calculate the kl divergence\n",
    "        #print(y0.size()) # (256, 1)\n",
    "        aug_y0 = torch.cat([y0, torch.zeros(batch_size, 1).to(y0)], dim=1) # create the augmented initial value\n",
    "        #print(aug_y0.size()) # [256, 2]\n",
    "        aug_ys = sdeint_fn(\n",
    "            sde=self,\n",
    "            y0=aug_y0,\n",
    "            ts=ts,\n",
    "            method=args['method'],\n",
    "            dt=args['dt'],\n",
    "            adaptive=args['adaptive'],\n",
    "            rtol=args['rtol'],\n",
    "            atol=args['atol'],\n",
    "            names={'drift': 'f_aug', 'diffusion': 'g_aug'}\n",
    "        ) # call the sde solver to \n",
    "        # print(aug_ys.size()) # [22, 256, 2]\n",
    "        ys, logqp_path = aug_ys[:, :, 0:1], aug_ys[-1, :, 1] # get the integral of the u(z,t) at the last time\n",
    "        \n",
    "        logqp = (logqp0 + logqp_path).mean(dim=0)  # KL(t=0) + KL(path).\n",
    "        return ys, logqp\n",
    "\n",
    "    def sample_p(self, ts, batch_size, eps=None, bm=None):\n",
    "        eps = torch.randn(batch_size, 1).to(self.py0_mean) if eps is None else eps\n",
    "        y0 = self.py0_mean + eps * self.py0_std\n",
    "        return sdeint_fn(self, y0, ts, bm=bm, method=args['method'], dt=args['dt'], names={'drift': 'h'}) # prior sde\n",
    "\n",
    "    def sample_q(self, ts, batch_size, eps=None, bm=None):\n",
    "        eps = torch.randn(batch_size, 1).to(self.qy0_mean) if eps is None else eps\n",
    "        y0 = self.qy0_mean + eps * self.qy0_std\n",
    "        return sdeint_fn(self, y0, ts, bm=bm, method=args['method'], dt=args['dt']) # posterior sde\n",
    "\n",
    "    @property\n",
    "    def py0_std(self):\n",
    "        return torch.exp(.5 * self.py0_logvar)\n",
    "\n",
    "    @property\n",
    "    def qy0_std(self):\n",
    "        return torch.exp(.5 * self.qy0_logvar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80736fff",
   "metadata": {},
   "source": [
    "## 5. Simulation of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd15aaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = 30\n",
    "#beta = 0.1694\n",
    "beta=1\n",
    "def make_time_series():\n",
    "    \n",
    "    \n",
    "    phi = 0.9805\n",
    "    sigma_v = 0.003342\n",
    "    sigma_u = 0.00528\n",
    "    rho = -0.856\n",
    "    cov_uv = rho * sigma_u * sigma_v\n",
    "\n",
    "    # generating shocks\n",
    "    mu = [0,0]\n",
    "    cov = [[sigma_u**2, cov_uv], [cov_uv, sigma_v**2]]\n",
    "    shocks = np.random.multivariate_normal(mu, cov, T)\n",
    "\n",
    "    z0 = np.random.normal(0, sigma_u**2/(1-phi**2),1)\n",
    "    r0 = shocks[0][0]\n",
    "\n",
    "    z = np.zeros(T)\n",
    "    r = np.zeros(T)\n",
    "    z[0] = z0\n",
    "    r[0] = r0\n",
    "\n",
    "    for idx_t in range(T-1):\n",
    "        z[idx_t+1] = phi*z[idx_t] + shocks[idx_t+1][1]\n",
    "        r[idx_t+1] = beta*z[idx_t] + shocks[idx_t+1][0]\n",
    "    return z, r\n",
    "z, r = make_time_series()\n",
    "plt.figure(figsize=(15,5))\n",
    "xvalues = np.array(range(T))\n",
    "plt.plot(xvalues, r, linestyle='-', color='g', label=\"observed $r_t$\")\n",
    "plt.plot(xvalues, z, linestyle=\"--\", color=\"r\", label=r\"hidden variable $\\beta * z_t$\", linewidth=3.0)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('%')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Simulated $r_t$ and hidden $\\beta * z_t$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0cf9fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    ts_ = np.linspace(0.1,3.0,30)\n",
    "    ts_ext_ = np.array([0.] + list(ts_) + [3.1])\n",
    "    ts_vis_ = np.linspace(0.1, 3.1, 31)\n",
    "    ys_ = r[:,None]\n",
    "    ts = torch.tensor(ts_).float()\n",
    "    ts_ext = torch.tensor(ts_ext_).float()\n",
    "    ts_vis = torch.tensor(ts_vis_).float()\n",
    "    ys = torch.tensor(ys_).float().to(device)\n",
    "    return Data(ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99a1fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Dataset\n",
    "    ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_ = make_data()\n",
    "    mu = torch.mean(ys)\n",
    "    sigma = torch.std(ys)\n",
    "    \n",
    "    # plotting parameters\n",
    "    vis_batch_size = 1024\n",
    "    ylims = (-0.02, 0.02)\n",
    "    alphas = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55]\n",
    "    percentiles = [0.999, 0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    vis_idx = np.random.permutation(vis_batch_size)\n",
    "    sample_colors = ('#8c96c6', '#8c6bb1', '#810f7c')\n",
    "    fill_color = '#9ebcda'\n",
    "    mean_color = '#4d004b'\n",
    "    num_samples = len(sample_colors)\n",
    "    \n",
    "    eps = torch.randn(vis_batch_size, 1).to(device)\n",
    "    bm = torchsde.BrownianInterval(\n",
    "        t0=ts_vis[0],\n",
    "        t1=ts_vis[-1],\n",
    "        size=(vis_batch_size,1),\n",
    "        device=device,\n",
    "        levy_area_approximation=\"space-time\")\n",
    "    \n",
    "    # Model\n",
    "    model = LatentSDE(theta=0.01,mu=mu,sigma=sigma).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    kl_scheduler = LinearScheduler(iters=100)\n",
    "    criterion = nn.BCELoss(reduction='sum')\n",
    "    \n",
    "    logpy_metric = EMAMetric()\n",
    "    kl_metric = EMAMetric()\n",
    "    loss_metric = EMAMetric()\n",
    "    \n",
    "    \n",
    "    # show prior\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        zs = model.sample_p(ts=ts_vis, batch_size = vis_batch_size, eps = eps, bm=bm).squeeze()\n",
    "       \n",
    "        ts_vis_, zs_ = ts_vis.cpu().numpy(), zs.cpu().numpy()\n",
    "        #plt.scatter(ts_vis_, ys_, color='r', label=\"prior\")\n",
    "        zs_ = np.sort(zs_,axis=1)\n",
    "        img_dir = os.path.join('./sim/','prior.png')\n",
    "        plt.subplot(frameon=False)\n",
    "        for alpha, percentile in zip(alphas, percentiles):\n",
    "            idx = int((1 - percentile) / 2. * vis_batch_size)\n",
    "            zs_bot_ = zs_[:, idx]\n",
    "            zs_top_ = zs_[:, -idx]\n",
    "            plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
    "\n",
    "        # `zorder` determines who's on top; the larger the more at the top.\n",
    "        \n",
    "        plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # Data.\n",
    "        plt.plot(ts_, ys_, marker='x', zorder=3, color='k',label=\"observed $r_t$\")\n",
    "        plt.plot(ts_, z[:,None], color='g', linewidth=3.0, label=r\"hidden variable $z_t$\")\n",
    "        plt.ylim(ylims)\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$Y_t$')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.savefig(img_dir, dpi=300)\n",
    "        plt.close()\n",
    "        logging.info(f'Saved prior figure at: {img_dir}')\n",
    "    \n",
    "    \n",
    "    for global_step in tqdm.tqdm(range(args['train_iters'])):\n",
    "        \n",
    "        # Plot and save.\n",
    "        if global_step % args['pause_iters'] == 0:\n",
    "            img_path = os.path.join(\"./sim/\", f'global_step_{global_step}.png')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                zs = model.sample_q(ts=ts_vis, batch_size=vis_batch_size, eps=eps, bm=bm).squeeze()\n",
    "                samples = zs[:, vis_idx]\n",
    "                ts_vis_, zs_, samples_ = ts_vis.cpu().numpy(), zs.cpu().numpy(), samples.cpu().numpy()\n",
    "                zs_ = np.sort(zs_, axis=1)\n",
    "                plt.subplot(frameon=False)\n",
    "\n",
    "                if True: # args.show_percentiles:\n",
    "                    for alpha, percentile in zip(alphas, percentiles):\n",
    "                        idx = int((1 - percentile) / 2. * vis_batch_size)\n",
    "                        zs_bot_, zs_top_ = zs_[:, idx], zs_[:, -idx]\n",
    "                        plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
    "\n",
    "                if True: #args.show_mean:\n",
    "                    plt.plot(ts_vis_, zs_.mean(axis=1), color=mean_color)\n",
    "\n",
    "                if True: #args.show_samples:\n",
    "                    #for j in range(num_samples):\n",
    "                    #    plt.plot(ts_vis_, samples_[:, j], color=sample_colors[j], linewidth=1.0)\n",
    "                    plt.plot(ts_, z[:,None], color='g', linewidth=3.0, label=r\"hidden variable $z_t$\")\n",
    "                    plt.plot(ts_vis_, samples_.mean(axis=1), color='r',label=r'mean of latent variables')\n",
    "\n",
    "                if True: #args.show_arrows:\n",
    "                    num, dt = 3, 0.12\n",
    "                    t, y = torch.meshgrid(\n",
    "                        [torch.linspace(0, 3, num).to(device), torch.linspace(-0.3, 0.3, num).to(device)]\n",
    "                    )\n",
    "                    t, y = t.reshape(-1, 1), y.reshape(-1, 1)\n",
    "                    fty = model.f(t=t, y=y).reshape(num, num)\n",
    "                    dt = torch.zeros(num, num).fill_(dt).to(device)\n",
    "                    dy = fty * dt\n",
    "                    dt_, dy_, t_, y_ = dt.cpu().numpy(), dy.cpu().numpy(), t.cpu().numpy(), y.cpu().numpy()\n",
    "                    plt.quiver(t_, y_, dt_, dy_, alpha=0.3, edgecolors='k', width=0.0035, scale=50)\n",
    "\n",
    "                if False: #args.hide_ticks:\n",
    "                    plt.xticks([], [])\n",
    "                    plt.yticks([], [])\n",
    "\n",
    "                plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # Data.\n",
    "                plt.plot(ts_, ys_, marker='x', zorder=3, color='k', label=\"observed $r_t$ \") # new added\n",
    "            \n",
    "\n",
    "                \n",
    "                plt.ylim(ylims)\n",
    "                plt.xlabel('$t$')\n",
    "                plt.ylabel('$Y_t$')\n",
    "                plt.tight_layout()\n",
    "                plt.legend()\n",
    "                plt.savefig(img_path, dpi=300)\n",
    "                plt.close()\n",
    "                logging.info(f'Saved figure at: {img_path}')\n",
    "\n",
    "                \n",
    "        \n",
    "        # Train.\n",
    "        optimizer.zero_grad() # zero the gradient\n",
    "        zs, kl = model(ts=ts_ext, batch_size=args['batch_size']) # pass through the model, ys, logqp\n",
    "        zs = zs.squeeze() # remove the dimensions of input of size 1\n",
    "        zs = zs[1:-1]  # Drop first and last which are only used to penalize out-of-data region and spread uncertainty.\n",
    "\n",
    "        likelihood_constructor = {\"laplace\": distributions.Laplace, \"normal\": distributions.Normal}[args['likelihood']]\n",
    "        likelihood = likelihood_constructor(loc=zs, scale=args['scale']) # create the laplace distribution\n",
    "        logpy = likelihood.log_prob(ys).sum(dim=0).mean(dim=0) # got the log likelihood\n",
    "\n",
    "        loss = -logpy + kl * kl_scheduler.val\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        kl_scheduler.step()\n",
    "\n",
    "        logpy_metric.step(logpy)\n",
    "        kl_metric.step(kl)\n",
    "        loss_metric.step(loss)\n",
    "\n",
    "        logging.info(\n",
    "            f'global_step: {global_step}, '\n",
    "            f'logpy: {logpy_metric.val:.3f}, '\n",
    "            f'kl: {kl_metric.val:.3f}, '\n",
    "            f'loss: {loss_metric.val:.3f}'\n",
    "        )\n",
    "    torch.save(\n",
    "        {'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'kl_scheduler': kl_scheduler},\n",
    "        os.path.join('./sim/', f'global_step_{global_step}.ckpt')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "74b55c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved prior figure at: ./sim/prior.png\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]INFO:root:Saved figure at: ./sim/global_step_0.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 0, logpy: -9.224, kl: 0.000, loss: 9.224\n",
      "  0%|          | 1/1000 [00:02<37:00,  2.22s/it]INFO:root:global_step: 1, logpy: -157.019, kl: 30.217, loss: 157.624\n",
      "  0%|          | 2/1000 [00:03<32:04,  1.93s/it]INFO:root:global_step: 2, logpy: -299.558, kl: 58.241, loss: 301.006\n",
      "  0%|          | 3/1000 [00:04<28:00,  1.69s/it]INFO:root:global_step: 3, logpy: -323.671, kl: 58.820, loss: 325.151\n",
      "  0%|          | 4/1000 [00:05<25:48,  1.56s/it]INFO:root:global_step: 4, logpy: -350.659, kl: 59.522, loss: 352.189\n",
      "  0%|          | 5/1000 [00:07<24:15,  1.46s/it]INFO:root:global_step: 5, logpy: -427.527, kl: 68.772, loss: 429.632\n",
      "  1%|          | 6/1000 [00:08<22:43,  1.37s/it]INFO:root:global_step: 6, logpy: -435.659, kl: 68.897, loss: 437.801\n",
      "  1%|          | 7/1000 [00:09<22:19,  1.35s/it]INFO:root:global_step: 7, logpy: -440.192, kl: 69.274, loss: 442.397\n",
      "  1%|          | 8/1000 [00:10<22:02,  1.33s/it]INFO:root:global_step: 8, logpy: -440.352, kl: 69.085, loss: 442.580\n",
      "  1%|          | 9/1000 [00:12<22:07,  1.34s/it]INFO:root:global_step: 9, logpy: -446.685, kl: 69.338, loss: 448.986\n",
      "  1%|          | 10/1000 [00:13<22:25,  1.36s/it]INFO:root:global_step: 10, logpy: -456.992, kl: 72.266, loss: 459.668\n",
      "  1%|          | 11/1000 [00:14<22:19,  1.35s/it]INFO:root:global_step: 11, logpy: -467.552, kl: 75.646, loss: 470.693\n",
      "  1%|          | 12/1000 [00:16<22:50,  1.39s/it]INFO:root:global_step: 12, logpy: -471.738, kl: 76.879, loss: 475.106\n",
      "  1%|▏         | 13/1000 [00:17<23:12,  1.41s/it]INFO:root:global_step: 13, logpy: -477.925, kl: 76.971, loss: 481.380\n",
      "  1%|▏         | 14/1000 [00:19<23:32,  1.43s/it]INFO:root:global_step: 14, logpy: -476.515, kl: 77.162, loss: 480.079\n",
      "  2%|▏         | 15/1000 [00:20<23:46,  1.45s/it]INFO:root:global_step: 15, logpy: -479.950, kl: 77.542, loss: 483.663\n",
      "  2%|▏         | 16/1000 [00:22<23:51,  1.45s/it]INFO:root:global_step: 16, logpy: -481.113, kl: 77.780, loss: 484.962\n",
      "  2%|▏         | 17/1000 [00:23<23:57,  1.46s/it]INFO:root:global_step: 17, logpy: -480.757, kl: 77.737, loss: 484.700\n",
      "  2%|▏         | 18/1000 [00:25<23:53,  1.46s/it]INFO:root:global_step: 18, logpy: -477.266, kl: 77.461, loss: 481.265\n",
      "  2%|▏         | 19/1000 [00:26<24:08,  1.48s/it]INFO:root:global_step: 19, logpy: -478.539, kl: 77.390, loss: 482.638\n",
      "  2%|▏         | 20/1000 [00:28<24:20,  1.49s/it]INFO:root:global_step: 20, logpy: -477.782, kl: 77.391, loss: 482.003\n",
      "  2%|▏         | 21/1000 [00:29<24:40,  1.51s/it]INFO:root:global_step: 21, logpy: -476.918, kl: 77.135, loss: 481.210\n",
      "  2%|▏         | 22/1000 [00:31<25:03,  1.54s/it]INFO:root:global_step: 22, logpy: -477.182, kl: 76.843, loss: 481.542\n",
      "  2%|▏         | 23/1000 [00:33<25:34,  1.57s/it]INFO:root:global_step: 23, logpy: -473.378, kl: 76.617, loss: 477.824\n",
      "  2%|▏         | 24/1000 [00:34<25:51,  1.59s/it]INFO:root:global_step: 24, logpy: -474.347, kl: 76.427, loss: 478.893\n",
      "  2%|▎         | 25/1000 [00:36<26:09,  1.61s/it]INFO:root:global_step: 25, logpy: -475.628, kl: 76.224, loss: 480.275\n",
      "  3%|▎         | 26/1000 [00:38<26:18,  1.62s/it]INFO:root:global_step: 26, logpy: -473.834, kl: 76.161, loss: 478.623\n",
      "  3%|▎         | 27/1000 [00:39<26:40,  1.64s/it]INFO:root:global_step: 27, logpy: -473.327, kl: 76.248, loss: 478.305\n",
      "  3%|▎         | 28/1000 [00:41<27:06,  1.67s/it]INFO:root:global_step: 28, logpy: -472.377, kl: 76.101, loss: 477.484\n",
      "  3%|▎         | 29/1000 [00:43<27:13,  1.68s/it]INFO:root:global_step: 29, logpy: -468.674, kl: 75.989, loss: 473.924\n",
      "  3%|▎         | 30/1000 [00:44<27:19,  1.69s/it]INFO:root:global_step: 30, logpy: -468.807, kl: 76.081, loss: 474.270\n",
      "  3%|▎         | 31/1000 [00:46<27:35,  1.71s/it]INFO:root:global_step: 31, logpy: -468.350, kl: 76.083, loss: 474.002\n",
      "  3%|▎         | 32/1000 [00:48<27:23,  1.70s/it]INFO:root:global_step: 32, logpy: -464.975, kl: 76.000, loss: 470.794\n",
      "  3%|▎         | 33/1000 [00:49<27:17,  1.69s/it]INFO:root:global_step: 33, logpy: -463.042, kl: 75.841, loss: 469.007\n",
      "  3%|▎         | 34/1000 [00:51<27:38,  1.72s/it]INFO:root:global_step: 34, logpy: -459.897, kl: 75.796, loss: 466.052\n",
      "  4%|▎         | 35/1000 [00:53<27:24,  1.70s/it]INFO:root:global_step: 35, logpy: -457.322, kl: 75.840, loss: 463.704\n",
      "  4%|▎         | 36/1000 [00:55<27:13,  1.69s/it]INFO:root:global_step: 36, logpy: -455.913, kl: 75.785, loss: 462.492\n",
      "  4%|▎         | 37/1000 [00:56<26:50,  1.67s/it]INFO:root:global_step: 37, logpy: -452.321, kl: 75.821, loss: 459.135\n",
      "  4%|▍         | 38/1000 [00:58<26:55,  1.68s/it]INFO:root:global_step: 38, logpy: -450.129, kl: 75.743, loss: 457.141\n",
      "  4%|▍         | 39/1000 [01:00<27:21,  1.71s/it]INFO:root:global_step: 39, logpy: -449.144, kl: 75.640, loss: 456.348\n",
      "  4%|▍         | 40/1000 [01:01<27:49,  1.74s/it]INFO:root:global_step: 40, logpy: -446.900, kl: 75.598, loss: 454.324\n",
      "  4%|▍         | 41/1000 [01:03<27:43,  1.74s/it]INFO:root:global_step: 41, logpy: -443.422, kl: 75.544, loss: 451.067\n",
      "  4%|▍         | 42/1000 [01:05<27:47,  1.74s/it]INFO:root:global_step: 42, logpy: -440.922, kl: 75.503, loss: 448.798\n",
      "  4%|▍         | 43/1000 [01:07<27:53,  1.75s/it]INFO:root:global_step: 43, logpy: -436.950, kl: 75.508, loss: 445.082\n",
      "  4%|▍         | 44/1000 [01:08<27:49,  1.75s/it]INFO:root:global_step: 44, logpy: -433.802, kl: 75.605, loss: 442.236\n",
      "  4%|▍         | 45/1000 [01:10<27:45,  1.74s/it]INFO:root:global_step: 45, logpy: -430.423, kl: 75.738, loss: 439.182\n",
      "  5%|▍         | 46/1000 [01:12<27:44,  1.74s/it]INFO:root:global_step: 46, logpy: -426.038, kl: 75.585, loss: 434.992\n",
      "  5%|▍         | 47/1000 [01:14<27:56,  1.76s/it]INFO:root:global_step: 47, logpy: -422.675, kl: 75.492, loss: 431.858\n",
      "  5%|▍         | 48/1000 [01:16<28:31,  1.80s/it]INFO:root:global_step: 48, logpy: -418.614, kl: 75.525, loss: 428.091\n",
      "  5%|▍         | 49/1000 [01:17<28:34,  1.80s/it]INFO:root:global_step: 49, logpy: -414.884, kl: 75.551, loss: 424.657\n",
      "  5%|▌         | 50/1000 [01:19<28:37,  1.81s/it]INFO:root:Saved figure at: ./sim/global_step_50.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 50, logpy: -410.666, kl: 75.589, loss: 420.746\n",
      "  5%|▌         | 51/1000 [01:22<34:02,  2.15s/it]INFO:root:global_step: 51, logpy: -407.078, kl: 75.606, loss: 417.459\n",
      "  5%|▌         | 52/1000 [01:24<32:42,  2.07s/it]INFO:root:global_step: 52, logpy: -402.868, kl: 75.540, loss: 413.511\n",
      "  5%|▌         | 53/1000 [01:26<32:04,  2.03s/it]INFO:root:global_step: 53, logpy: -399.099, kl: 75.596, loss: 410.074\n",
      "  5%|▌         | 54/1000 [01:28<31:07,  1.97s/it]INFO:root:global_step: 54, logpy: -395.211, kl: 75.547, loss: 406.465\n",
      "  6%|▌         | 55/1000 [01:30<30:44,  1.95s/it]INFO:root:global_step: 55, logpy: -391.338, kl: 75.543, loss: 402.900\n",
      "  6%|▌         | 56/1000 [01:32<30:17,  1.93s/it]INFO:root:global_step: 56, logpy: -387.342, kl: 75.513, loss: 399.202\n",
      "  6%|▌         | 57/1000 [01:33<29:43,  1.89s/it]INFO:root:global_step: 57, logpy: -383.568, kl: 75.561, loss: 395.775\n",
      "  6%|▌         | 58/1000 [01:35<29:25,  1.87s/it]INFO:root:global_step: 58, logpy: -379.760, kl: 75.686, loss: 392.364\n",
      "  6%|▌         | 59/1000 [01:37<29:01,  1.85s/it]INFO:root:global_step: 59, logpy: -375.931, kl: 75.680, loss: 388.860\n",
      "  6%|▌         | 60/1000 [01:39<29:04,  1.86s/it]INFO:root:global_step: 60, logpy: -372.249, kl: 75.696, loss: 385.520\n",
      "  6%|▌         | 61/1000 [01:41<29:20,  1.87s/it]INFO:root:global_step: 61, logpy: -368.341, kl: 75.639, loss: 381.914\n",
      "  6%|▌         | 62/1000 [01:43<30:16,  1.94s/it]INFO:root:global_step: 62, logpy: -364.659, kl: 75.667, loss: 378.591\n",
      "  6%|▋         | 63/1000 [01:45<30:17,  1.94s/it]INFO:root:global_step: 63, logpy: -360.791, kl: 75.689, loss: 375.081\n",
      "  6%|▋         | 64/1000 [01:47<30:18,  1.94s/it]INFO:root:global_step: 64, logpy: -357.204, kl: 75.678, loss: 371.836\n",
      "  6%|▋         | 65/1000 [01:49<31:16,  2.01s/it]INFO:root:global_step: 65, logpy: -353.387, kl: 75.591, loss: 368.315\n",
      "  7%|▋         | 66/1000 [01:51<30:51,  1.98s/it]INFO:root:global_step: 66, logpy: -349.849, kl: 75.645, loss: 365.170\n",
      "  7%|▋         | 67/1000 [01:53<30:15,  1.95s/it]INFO:root:global_step: 67, logpy: -346.077, kl: 75.598, loss: 361.727\n",
      "  7%|▋         | 68/1000 [01:55<30:09,  1.94s/it]INFO:root:global_step: 68, logpy: -342.641, kl: 75.688, loss: 358.718\n",
      "  7%|▋         | 69/1000 [01:57<29:51,  1.92s/it]INFO:root:global_step: 69, logpy: -338.972, kl: 75.567, loss: 355.334\n",
      "  7%|▋         | 70/1000 [01:58<29:30,  1.90s/it]INFO:root:global_step: 70, logpy: -335.512, kl: 75.485, loss: 352.189\n",
      "  7%|▋         | 71/1000 [02:00<29:29,  1.91s/it]INFO:root:global_step: 71, logpy: -331.927, kl: 75.479, loss: 348.976\n",
      "  7%|▋         | 72/1000 [02:02<29:19,  1.90s/it]INFO:root:global_step: 72, logpy: -328.480, kl: 75.516, loss: 345.937\n",
      "  7%|▋         | 73/1000 [02:04<29:07,  1.88s/it]INFO:root:global_step: 73, logpy: -325.022, kl: 75.542, loss: 342.882\n",
      "  7%|▋         | 74/1000 [02:06<28:54,  1.87s/it]INFO:root:global_step: 74, logpy: -321.633, kl: 75.517, loss: 339.863\n",
      "  8%|▊         | 75/1000 [02:08<28:50,  1.87s/it]INFO:root:global_step: 75, logpy: -318.180, kl: 75.479, loss: 336.772\n",
      "  8%|▊         | 76/1000 [02:10<29:03,  1.89s/it]INFO:root:global_step: 76, logpy: -314.831, kl: 75.390, loss: 333.749\n",
      "  8%|▊         | 77/1000 [02:12<29:03,  1.89s/it]INFO:root:global_step: 77, logpy: -311.419, kl: 75.348, loss: 330.704\n",
      "  8%|▊         | 78/1000 [02:14<29:18,  1.91s/it]INFO:root:global_step: 78, logpy: -308.121, kl: 75.247, loss: 327.728\n",
      "  8%|▊         | 79/1000 [02:15<29:11,  1.90s/it]INFO:root:global_step: 79, logpy: -304.741, kl: 75.131, loss: 324.662\n",
      "  8%|▊         | 80/1000 [02:17<29:07,  1.90s/it]INFO:root:global_step: 80, logpy: -301.504, kl: 75.074, loss: 321.787\n",
      "  8%|▊         | 81/1000 [02:19<29:10,  1.91s/it]INFO:root:global_step: 81, logpy: -298.193, kl: 74.998, loss: 318.827\n",
      "  8%|▊         | 82/1000 [02:21<29:42,  1.94s/it]INFO:root:global_step: 82, logpy: -294.974, kl: 74.890, loss: 315.935\n",
      "  8%|▊         | 83/1000 [02:23<29:45,  1.95s/it]INFO:root:global_step: 83, logpy: -291.730, kl: 74.772, loss: 313.011\n",
      "  8%|▊         | 84/1000 [02:25<29:57,  1.96s/it]INFO:root:global_step: 84, logpy: -288.584, kl: 74.709, loss: 310.234\n",
      "  8%|▊         | 85/1000 [02:27<29:39,  1.94s/it]INFO:root:global_step: 85, logpy: -285.416, kl: 74.659, loss: 307.450\n",
      "  9%|▊         | 86/1000 [02:29<29:17,  1.92s/it]INFO:root:global_step: 86, logpy: -282.289, kl: 74.524, loss: 304.633\n",
      "  9%|▊         | 87/1000 [02:31<29:17,  1.93s/it]INFO:root:global_step: 87, logpy: -279.197, kl: 74.470, loss: 301.927\n",
      "  9%|▉         | 88/1000 [02:33<29:30,  1.94s/it]INFO:root:global_step: 88, logpy: -276.112, kl: 74.381, loss: 299.198\n",
      "  9%|▉         | 89/1000 [02:35<29:20,  1.93s/it]INFO:root:global_step: 89, logpy: -273.058, kl: 74.280, loss: 296.492\n",
      "  9%|▉         | 90/1000 [02:37<29:13,  1.93s/it]INFO:root:global_step: 90, logpy: -270.013, kl: 74.173, loss: 293.791\n",
      "  9%|▉         | 91/1000 [02:39<29:04,  1.92s/it]INFO:root:global_step: 91, logpy: -267.009, kl: 74.088, loss: 291.153\n",
      "  9%|▉         | 92/1000 [02:41<28:50,  1.91s/it]INFO:root:global_step: 92, logpy: -264.027, kl: 73.959, loss: 288.498\n",
      "  9%|▉         | 93/1000 [02:42<28:51,  1.91s/it]INFO:root:global_step: 93, logpy: -261.082, kl: 73.920, loss: 285.968\n",
      "  9%|▉         | 94/1000 [02:44<29:04,  1.93s/it]INFO:root:global_step: 94, logpy: -258.155, kl: 73.821, loss: 283.400\n",
      " 10%|▉         | 95/1000 [02:46<28:56,  1.92s/it]INFO:root:global_step: 95, logpy: -255.263, kl: 73.646, loss: 280.796\n",
      " 10%|▉         | 96/1000 [02:48<29:02,  1.93s/it]INFO:root:global_step: 96, logpy: -252.385, kl: 73.509, loss: 278.244\n",
      " 10%|▉         | 97/1000 [02:50<28:52,  1.92s/it]INFO:root:global_step: 97, logpy: -249.525, kl: 73.408, loss: 275.747\n",
      " 10%|▉         | 98/1000 [02:52<29:14,  1.95s/it]INFO:root:global_step: 98, logpy: -246.669, kl: 73.141, loss: 273.092\n",
      " 10%|▉         | 99/1000 [02:54<29:26,  1.96s/it]INFO:root:global_step: 99, logpy: -243.895, kl: 73.004, loss: 270.648\n",
      " 10%|█         | 100/1000 [02:56<29:22,  1.96s/it]INFO:root:Saved figure at: ./sim/global_step_100.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 100, logpy: -241.130, kl: 72.840, loss: 268.181\n",
      " 10%|█         | 101/1000 [02:59<35:01,  2.34s/it]INFO:root:global_step: 101, logpy: -238.389, kl: 72.699, loss: 265.757\n",
      " 10%|█         | 102/1000 [03:02<35:25,  2.37s/it]INFO:root:global_step: 102, logpy: -235.677, kl: 72.534, loss: 263.333\n",
      " 10%|█         | 103/1000 [03:04<33:33,  2.24s/it]INFO:root:global_step: 103, logpy: -232.985, kl: 72.330, loss: 260.886\n",
      " 10%|█         | 104/1000 [03:06<32:38,  2.19s/it]INFO:root:global_step: 104, logpy: -230.352, kl: 72.227, loss: 258.594\n",
      " 10%|█         | 105/1000 [03:08<31:19,  2.10s/it]INFO:root:global_step: 105, logpy: -227.717, kl: 72.050, loss: 256.222\n",
      " 11%|█         | 106/1000 [03:10<30:41,  2.06s/it]INFO:root:global_step: 106, logpy: -225.086, kl: 71.860, loss: 253.837\n",
      " 11%|█         | 107/1000 [03:12<30:24,  2.04s/it]INFO:root:global_step: 107, logpy: -222.503, kl: 71.658, loss: 251.483\n",
      " 11%|█         | 108/1000 [03:14<29:47,  2.00s/it]INFO:root:global_step: 108, logpy: -219.920, kl: 71.437, loss: 249.106\n",
      " 11%|█         | 109/1000 [03:16<29:32,  1.99s/it]INFO:root:global_step: 109, logpy: -217.364, kl: 71.188, loss: 246.724\n",
      " 11%|█         | 110/1000 [03:18<29:14,  1.97s/it]INFO:root:global_step: 110, logpy: -214.837, kl: 71.013, loss: 244.440\n",
      " 11%|█         | 111/1000 [03:19<28:55,  1.95s/it]INFO:root:global_step: 111, logpy: -212.338, kl: 70.750, loss: 242.092\n",
      " 11%|█         | 112/1000 [03:21<28:44,  1.94s/it]INFO:root:global_step: 112, logpy: -209.881, kl: 70.541, loss: 239.836\n",
      " 11%|█▏        | 113/1000 [03:23<28:23,  1.92s/it]INFO:root:global_step: 113, logpy: -207.428, kl: 70.310, loss: 237.557\n",
      " 11%|█▏        | 114/1000 [03:25<28:09,  1.91s/it]INFO:root:global_step: 114, logpy: -205.021, kl: 70.151, loss: 235.394\n",
      " 12%|█▏        | 115/1000 [03:27<28:12,  1.91s/it]INFO:root:global_step: 115, logpy: -202.621, kl: 69.917, loss: 233.157\n",
      " 12%|█▏        | 116/1000 [03:29<27:56,  1.90s/it]INFO:root:global_step: 116, logpy: -200.235, kl: 69.701, loss: 230.949\n",
      " 12%|█▏        | 117/1000 [03:31<28:01,  1.90s/it]INFO:root:global_step: 117, logpy: -197.893, kl: 69.537, loss: 228.832\n",
      " 12%|█▏        | 118/1000 [03:33<27:48,  1.89s/it]INFO:root:global_step: 118, logpy: -195.558, kl: 69.327, loss: 226.674\n",
      " 12%|█▏        | 119/1000 [03:35<27:51,  1.90s/it]INFO:root:global_step: 119, logpy: -193.258, kl: 69.105, loss: 224.535\n",
      " 12%|█▏        | 120/1000 [03:36<27:46,  1.89s/it]INFO:root:global_step: 120, logpy: -190.977, kl: 68.904, loss: 222.430\n",
      " 12%|█▏        | 121/1000 [03:38<27:39,  1.89s/it]INFO:root:global_step: 121, logpy: -188.717, kl: 68.698, loss: 220.339\n",
      " 12%|█▏        | 122/1000 [03:40<27:35,  1.89s/it]INFO:root:global_step: 122, logpy: -186.468, kl: 68.442, loss: 218.204\n",
      " 12%|█▏        | 123/1000 [03:42<27:23,  1.87s/it]INFO:root:global_step: 123, logpy: -184.252, kl: 68.230, loss: 216.143\n",
      " 12%|█▏        | 124/1000 [03:44<27:47,  1.90s/it]INFO:root:global_step: 124, logpy: -182.067, kl: 68.040, loss: 214.132\n",
      " 12%|█▎        | 125/1000 [03:46<27:57,  1.92s/it]INFO:root:global_step: 125, logpy: -179.878, kl: 67.778, loss: 212.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 126/1000 [03:48<27:55,  1.92s/it]INFO:root:global_step: 126, logpy: -177.709, kl: 67.516, loss: 209.966\n",
      " 13%|█▎        | 127/1000 [03:50<27:55,  1.92s/it]INFO:root:global_step: 127, logpy: -175.559, kl: 67.253, loss: 207.905\n",
      " 13%|█▎        | 128/1000 [03:52<27:43,  1.91s/it]INFO:root:global_step: 128, logpy: -173.448, kl: 67.009, loss: 205.900\n",
      " 13%|█▎        | 129/1000 [03:54<27:31,  1.90s/it]INFO:root:global_step: 129, logpy: -171.347, kl: 66.765, loss: 203.901\n",
      " 13%|█▎        | 130/1000 [03:55<27:41,  1.91s/it]INFO:root:global_step: 130, logpy: -169.279, kl: 66.563, loss: 201.972\n",
      " 13%|█▎        | 131/1000 [03:57<27:39,  1.91s/it]INFO:root:global_step: 131, logpy: -167.236, kl: 66.411, loss: 200.116\n",
      " 13%|█▎        | 132/1000 [03:59<27:42,  1.92s/it]INFO:root:global_step: 132, logpy: -165.203, kl: 66.166, loss: 198.172\n",
      " 13%|█▎        | 133/1000 [04:01<27:31,  1.90s/it]INFO:root:global_step: 133, logpy: -163.175, kl: 65.900, loss: 196.211\n",
      " 13%|█▎        | 134/1000 [04:03<27:33,  1.91s/it]INFO:root:global_step: 134, logpy: -161.187, kl: 65.691, loss: 194.343\n",
      " 14%|█▎        | 135/1000 [04:05<27:30,  1.91s/it]INFO:root:global_step: 135, logpy: -159.213, kl: 65.435, loss: 192.439\n",
      " 14%|█▎        | 136/1000 [04:07<27:20,  1.90s/it]INFO:root:global_step: 136, logpy: -157.267, kl: 65.200, loss: 190.578\n",
      " 14%|█▎        | 137/1000 [04:09<27:01,  1.88s/it]INFO:root:global_step: 137, logpy: -155.342, kl: 64.957, loss: 188.730\n",
      " 14%|█▍        | 138/1000 [04:11<26:57,  1.88s/it]INFO:root:global_step: 138, logpy: -153.425, kl: 64.689, loss: 186.860\n",
      " 14%|█▍        | 139/1000 [04:13<27:11,  1.89s/it]INFO:root:global_step: 139, logpy: -151.530, kl: 64.467, loss: 185.056\n",
      " 14%|█▍        | 140/1000 [04:14<27:18,  1.90s/it]INFO:root:global_step: 140, logpy: -149.662, kl: 64.285, loss: 183.315\n",
      " 14%|█▍        | 141/1000 [04:16<27:31,  1.92s/it]INFO:root:global_step: 141, logpy: -147.821, kl: 64.089, loss: 181.585\n",
      " 14%|█▍        | 142/1000 [04:18<27:18,  1.91s/it]INFO:root:global_step: 142, logpy: -145.970, kl: 63.807, loss: 179.755\n",
      " 14%|█▍        | 143/1000 [04:20<27:28,  1.92s/it]INFO:root:global_step: 143, logpy: -144.153, kl: 63.602, loss: 178.033\n",
      " 14%|█▍        | 144/1000 [04:22<27:10,  1.90s/it]INFO:root:global_step: 144, logpy: -142.331, kl: 63.323, loss: 176.230\n",
      " 14%|█▍        | 145/1000 [04:24<27:27,  1.93s/it]INFO:root:global_step: 145, logpy: -140.554, kl: 63.101, loss: 174.525\n",
      " 15%|█▍        | 146/1000 [04:26<27:30,  1.93s/it]INFO:root:global_step: 146, logpy: -138.771, kl: 62.816, loss: 172.748\n",
      " 15%|█▍        | 147/1000 [04:28<27:44,  1.95s/it]INFO:root:global_step: 147, logpy: -137.016, kl: 62.553, loss: 171.019\n",
      " 15%|█▍        | 148/1000 [04:30<27:38,  1.95s/it]INFO:root:global_step: 148, logpy: -135.283, kl: 62.306, loss: 169.324\n",
      " 15%|█▍        | 149/1000 [04:32<27:16,  1.92s/it]INFO:root:global_step: 149, logpy: -133.559, kl: 62.039, loss: 167.616\n",
      " 15%|█▌        | 150/1000 [04:34<27:11,  1.92s/it]INFO:root:Saved figure at: ./sim/global_step_150.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 150, logpy: -131.842, kl: 61.772, loss: 165.912\n",
      " 15%|█▌        | 151/1000 [04:37<32:08,  2.27s/it]INFO:root:global_step: 151, logpy: -130.151, kl: 61.507, loss: 164.232\n",
      " 15%|█▌        | 152/1000 [04:39<30:40,  2.17s/it]INFO:root:global_step: 152, logpy: -128.501, kl: 61.295, loss: 162.645\n",
      " 15%|█▌        | 153/1000 [04:41<29:29,  2.09s/it]INFO:root:global_step: 153, logpy: -126.858, kl: 61.047, loss: 161.025\n",
      " 15%|█▌        | 154/1000 [04:43<28:43,  2.04s/it]INFO:root:global_step: 154, logpy: -125.203, kl: 60.778, loss: 159.370\n",
      " 16%|█▌        | 155/1000 [04:45<28:23,  2.02s/it]INFO:root:global_step: 155, logpy: -123.576, kl: 60.530, loss: 157.761\n",
      " 16%|█▌        | 156/1000 [04:46<27:38,  1.97s/it]INFO:root:global_step: 156, logpy: -121.965, kl: 60.287, loss: 156.170\n",
      " 16%|█▌        | 157/1000 [04:48<27:06,  1.93s/it]INFO:root:global_step: 157, logpy: -120.379, kl: 60.060, loss: 154.618\n",
      " 16%|█▌        | 158/1000 [04:50<26:39,  1.90s/it]INFO:root:global_step: 158, logpy: -118.801, kl: 59.832, loss: 153.071\n",
      " 16%|█▌        | 159/1000 [04:52<26:30,  1.89s/it]INFO:root:global_step: 159, logpy: -117.257, kl: 59.607, loss: 151.557\n",
      " 16%|█▌        | 160/1000 [04:54<26:15,  1.88s/it]INFO:root:global_step: 160, logpy: -115.721, kl: 59.400, loss: 150.068\n",
      " 16%|█▌        | 161/1000 [04:56<26:31,  1.90s/it]INFO:root:global_step: 161, logpy: -114.206, kl: 59.187, loss: 148.590\n",
      " 16%|█▌        | 162/1000 [04:58<26:15,  1.88s/it]INFO:root:global_step: 162, logpy: -112.667, kl: 58.890, loss: 147.002\n",
      " 16%|█▋        | 163/1000 [05:00<26:27,  1.90s/it]INFO:root:global_step: 163, logpy: -111.160, kl: 58.645, loss: 145.495\n",
      " 16%|█▋        | 164/1000 [05:01<26:32,  1.91s/it]INFO:root:global_step: 164, logpy: -109.679, kl: 58.448, loss: 144.060\n",
      " 16%|█▋        | 165/1000 [05:03<26:14,  1.89s/it]INFO:root:global_step: 165, logpy: -108.205, kl: 58.174, loss: 142.553\n",
      " 17%|█▋        | 166/1000 [05:05<26:05,  1.88s/it]INFO:root:global_step: 166, logpy: -106.742, kl: 57.914, loss: 141.069\n",
      " 17%|█▋        | 167/1000 [05:07<26:04,  1.88s/it]INFO:root:global_step: 167, logpy: -105.295, kl: 57.685, loss: 139.628\n",
      " 17%|█▋        | 168/1000 [05:09<26:01,  1.88s/it]INFO:root:global_step: 168, logpy: -103.878, kl: 57.464, loss: 138.224\n",
      " 17%|█▋        | 169/1000 [05:11<25:54,  1.87s/it]INFO:root:global_step: 169, logpy: -102.447, kl: 57.192, loss: 136.753\n",
      " 17%|█▋        | 170/1000 [05:13<25:56,  1.87s/it]INFO:root:global_step: 170, logpy: -101.049, kl: 56.957, loss: 135.348\n",
      " 17%|█▋        | 171/1000 [05:14<25:40,  1.86s/it]INFO:root:global_step: 171, logpy: -99.670, kl: 56.737, loss: 133.976\n",
      " 17%|█▋        | 172/1000 [05:16<25:54,  1.88s/it]INFO:root:global_step: 172, logpy: -98.300, kl: 56.508, loss: 132.601\n",
      " 17%|█▋        | 173/1000 [05:18<25:47,  1.87s/it]INFO:root:global_step: 173, logpy: -96.921, kl: 56.246, loss: 131.182\n",
      " 17%|█▋        | 174/1000 [05:20<25:39,  1.86s/it]INFO:root:global_step: 174, logpy: -95.566, kl: 55.991, loss: 129.792\n",
      " 18%|█▊        | 175/1000 [05:22<25:46,  1.87s/it]INFO:root:global_step: 175, logpy: -94.216, kl: 55.731, loss: 128.399\n",
      " 18%|█▊        | 176/1000 [05:24<25:37,  1.87s/it]INFO:root:global_step: 176, logpy: -92.900, kl: 55.507, loss: 127.075\n",
      " 18%|█▊        | 177/1000 [05:26<25:35,  1.87s/it]INFO:root:global_step: 177, logpy: -91.588, kl: 55.259, loss: 125.728\n",
      " 18%|█▊        | 178/1000 [05:28<25:39,  1.87s/it]INFO:root:global_step: 178, logpy: -90.289, kl: 55.002, loss: 124.384\n",
      " 18%|█▊        | 179/1000 [05:29<25:31,  1.87s/it]INFO:root:global_step: 179, logpy: -89.003, kl: 54.752, loss: 123.057\n",
      " 18%|█▊        | 180/1000 [05:31<25:42,  1.88s/it]INFO:root:global_step: 180, logpy: -87.722, kl: 54.514, loss: 121.745\n",
      " 18%|█▊        | 181/1000 [05:33<25:45,  1.89s/it]INFO:root:global_step: 181, logpy: -86.441, kl: 54.247, loss: 120.401\n",
      " 18%|█▊        | 182/1000 [05:35<25:46,  1.89s/it]INFO:root:global_step: 182, logpy: -85.190, kl: 54.022, loss: 119.128\n",
      " 18%|█▊        | 183/1000 [05:37<25:41,  1.89s/it]INFO:root:global_step: 183, logpy: -83.951, kl: 53.793, loss: 117.861\n",
      " 18%|█▊        | 184/1000 [05:39<25:43,  1.89s/it]INFO:root:global_step: 184, logpy: -82.720, kl: 53.546, loss: 116.582\n",
      " 18%|█▊        | 185/1000 [05:41<25:37,  1.89s/it]INFO:root:global_step: 185, logpy: -81.514, kl: 53.318, loss: 115.344\n",
      " 19%|█▊        | 186/1000 [05:43<25:30,  1.88s/it]INFO:root:global_step: 186, logpy: -80.322, kl: 53.106, loss: 114.135\n",
      " 19%|█▊        | 187/1000 [05:45<25:25,  1.88s/it]INFO:root:global_step: 187, logpy: -79.107, kl: 52.830, loss: 112.837\n",
      " 19%|█▉        | 188/1000 [05:47<25:54,  1.91s/it]INFO:root:global_step: 188, logpy: -77.919, kl: 52.599, loss: 111.609\n",
      " 19%|█▉        | 189/1000 [05:48<25:48,  1.91s/it]INFO:root:global_step: 189, logpy: -76.753, kl: 52.364, loss: 110.398\n",
      " 19%|█▉        | 190/1000 [05:50<25:35,  1.90s/it]INFO:root:global_step: 190, logpy: -75.595, kl: 52.141, loss: 109.204\n",
      " 19%|█▉        | 191/1000 [05:52<25:24,  1.88s/it]INFO:root:global_step: 191, logpy: -74.454, kl: 51.935, loss: 108.043\n",
      " 19%|█▉        | 192/1000 [05:54<25:19,  1.88s/it]INFO:root:global_step: 192, logpy: -73.299, kl: 51.697, loss: 106.832\n",
      " 19%|█▉        | 193/1000 [05:56<25:33,  1.90s/it]INFO:root:global_step: 193, logpy: -72.159, kl: 51.439, loss: 105.615\n",
      " 19%|█▉        | 194/1000 [05:58<25:28,  1.90s/it]INFO:root:global_step: 194, logpy: -71.045, kl: 51.215, loss: 104.458\n",
      " 20%|█▉        | 195/1000 [06:00<25:26,  1.90s/it]INFO:root:global_step: 195, logpy: -69.966, kl: 50.998, loss: 103.340\n",
      " 20%|█▉        | 196/1000 [06:02<25:18,  1.89s/it]INFO:root:global_step: 196, logpy: -68.883, kl: 50.782, loss: 102.218\n",
      " 20%|█▉        | 197/1000 [06:04<25:15,  1.89s/it]INFO:root:global_step: 197, logpy: -67.798, kl: 50.570, loss: 101.095\n",
      " 20%|█▉        | 198/1000 [06:05<25:14,  1.89s/it]INFO:root:global_step: 198, logpy: -66.730, kl: 50.368, loss: 99.997\n",
      " 20%|█▉        | 199/1000 [06:07<25:05,  1.88s/it]INFO:root:global_step: 199, logpy: -65.661, kl: 50.119, loss: 98.851\n",
      " 20%|██        | 200/1000 [06:09<25:09,  1.89s/it]INFO:root:Saved figure at: ./sim/global_step_200.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 200, logpy: -64.641, kl: 49.892, loss: 97.772\n",
      " 20%|██        | 201/1000 [06:12<29:49,  2.24s/it]INFO:root:global_step: 201, logpy: -63.611, kl: 49.664, loss: 96.682\n",
      " 20%|██        | 202/1000 [06:14<28:22,  2.13s/it]INFO:root:global_step: 202, logpy: -62.573, kl: 49.438, loss: 95.584\n",
      " 20%|██        | 203/1000 [06:16<28:04,  2.11s/it]INFO:root:global_step: 203, logpy: -61.590, kl: 49.298, loss: 94.626\n",
      " 20%|██        | 204/1000 [06:18<27:21,  2.06s/it]INFO:root:global_step: 204, logpy: -60.598, kl: 49.083, loss: 93.581\n",
      " 20%|██        | 205/1000 [06:20<26:52,  2.03s/it]INFO:root:global_step: 205, logpy: -59.624, kl: 48.864, loss: 92.549\n",
      " 21%|██        | 206/1000 [06:22<26:38,  2.01s/it]INFO:root:global_step: 206, logpy: -58.641, kl: 48.640, loss: 91.501\n",
      " 21%|██        | 207/1000 [06:24<26:24,  2.00s/it]INFO:root:global_step: 207, logpy: -57.651, kl: 48.415, loss: 90.444\n",
      " 21%|██        | 208/1000 [06:26<25:59,  1.97s/it]INFO:root:global_step: 208, logpy: -56.682, kl: 48.207, loss: 89.424\n",
      " 21%|██        | 209/1000 [06:28<25:39,  1.95s/it]INFO:root:global_step: 209, logpy: -55.721, kl: 47.995, loss: 88.406\n",
      " 21%|██        | 210/1000 [06:30<25:18,  1.92s/it]INFO:root:global_step: 210, logpy: -54.784, kl: 47.776, loss: 87.403\n",
      " 21%|██        | 211/1000 [06:32<25:12,  1.92s/it]INFO:root:global_step: 211, logpy: -53.863, kl: 47.585, loss: 86.442\n",
      " 21%|██        | 212/1000 [06:33<25:01,  1.91s/it]INFO:root:global_step: 212, logpy: -52.925, kl: 47.372, loss: 85.442\n",
      " 21%|██▏       | 213/1000 [06:35<24:52,  1.90s/it]INFO:root:global_step: 213, logpy: -51.998, kl: 47.182, loss: 84.472\n",
      " 21%|██▏       | 214/1000 [06:37<24:51,  1.90s/it]INFO:root:global_step: 214, logpy: -51.123, kl: 46.999, loss: 83.563\n",
      " 22%|██▏       | 215/1000 [06:39<24:32,  1.88s/it]INFO:root:global_step: 215, logpy: -50.294, kl: 46.801, loss: 82.680\n",
      " 22%|██▏       | 216/1000 [06:41<24:18,  1.86s/it]INFO:root:global_step: 216, logpy: -49.419, kl: 46.592, loss: 81.741\n",
      " 22%|██▏       | 217/1000 [06:43<24:29,  1.88s/it]INFO:root:global_step: 217, logpy: -48.500, kl: 46.384, loss: 80.756\n",
      " 22%|██▏       | 218/1000 [06:45<24:31,  1.88s/it]INFO:root:global_step: 218, logpy: -47.627, kl: 46.190, loss: 79.831\n",
      " 22%|██▏       | 219/1000 [06:47<24:30,  1.88s/it]INFO:root:global_step: 219, logpy: -46.771, kl: 45.984, loss: 78.908\n",
      " 22%|██▏       | 220/1000 [06:49<24:41,  1.90s/it]INFO:root:global_step: 220, logpy: -45.906, kl: 45.772, loss: 77.970\n",
      " 22%|██▏       | 221/1000 [06:50<24:26,  1.88s/it]INFO:root:global_step: 221, logpy: -45.058, kl: 45.584, loss: 77.071\n",
      " 22%|██▏       | 222/1000 [06:52<24:22,  1.88s/it]INFO:root:global_step: 222, logpy: -44.202, kl: 45.406, loss: 76.172\n",
      " 22%|██▏       | 223/1000 [06:54<24:17,  1.88s/it]INFO:root:global_step: 223, logpy: -43.347, kl: 45.230, loss: 75.276\n",
      " 22%|██▏       | 224/1000 [06:56<24:47,  1.92s/it]INFO:root:global_step: 224, logpy: -42.488, kl: 45.018, loss: 74.338\n",
      " 22%|██▎       | 225/1000 [06:58<24:42,  1.91s/it]INFO:root:global_step: 225, logpy: -41.657, kl: 44.822, loss: 73.443\n",
      " 23%|██▎       | 226/1000 [07:00<24:36,  1.91s/it]INFO:root:global_step: 226, logpy: -40.834, kl: 44.648, loss: 72.576\n",
      " 23%|██▎       | 227/1000 [07:02<24:34,  1.91s/it]INFO:root:global_step: 227, logpy: -40.012, kl: 44.461, loss: 71.696\n",
      " 23%|██▎       | 228/1000 [07:04<24:29,  1.90s/it]INFO:root:global_step: 228, logpy: -39.199, kl: 44.265, loss: 70.816\n",
      " 23%|██▎       | 229/1000 [07:06<24:19,  1.89s/it]INFO:root:global_step: 229, logpy: -38.409, kl: 44.074, loss: 69.960\n",
      " 23%|██▎       | 230/1000 [07:07<24:05,  1.88s/it]INFO:root:global_step: 230, logpy: -37.617, kl: 43.885, loss: 69.105\n",
      " 23%|██▎       | 231/1000 [07:09<23:58,  1.87s/it]INFO:root:global_step: 231, logpy: -36.834, kl: 43.701, loss: 68.262\n",
      " 23%|██▎       | 232/1000 [07:11<24:08,  1.89s/it]INFO:root:global_step: 232, logpy: -36.050, kl: 43.522, loss: 67.421\n",
      " 23%|██▎       | 233/1000 [07:13<23:56,  1.87s/it]INFO:root:global_step: 233, logpy: -35.276, kl: 43.351, loss: 66.598\n",
      " 23%|██▎       | 234/1000 [07:15<23:56,  1.88s/it]INFO:root:global_step: 234, logpy: -34.505, kl: 43.156, loss: 65.752\n",
      " 24%|██▎       | 235/1000 [07:17<23:48,  1.87s/it]INFO:root:global_step: 235, logpy: -33.737, kl: 42.961, loss: 64.908\n",
      " 24%|██▎       | 236/1000 [07:19<23:49,  1.87s/it]INFO:root:global_step: 236, logpy: -32.991, kl: 42.778, loss: 64.097\n",
      " 24%|██▎       | 237/1000 [07:21<24:12,  1.90s/it]INFO:root:global_step: 237, logpy: -32.243, kl: 42.585, loss: 63.272\n",
      " 24%|██▍       | 238/1000 [07:22<23:55,  1.88s/it]INFO:root:global_step: 238, logpy: -31.518, kl: 42.404, loss: 62.482\n",
      " 24%|██▍       | 239/1000 [07:24<23:50,  1.88s/it]INFO:root:global_step: 239, logpy: -30.787, kl: 42.222, loss: 61.684\n",
      " 24%|██▍       | 240/1000 [07:26<23:39,  1.87s/it]INFO:root:global_step: 240, logpy: -30.049, kl: 42.022, loss: 60.860\n",
      " 24%|██▍       | 241/1000 [07:28<23:50,  1.88s/it]INFO:root:global_step: 241, logpy: -29.344, kl: 41.839, loss: 60.083\n",
      " 24%|██▍       | 242/1000 [07:30<23:47,  1.88s/it]INFO:root:global_step: 242, logpy: -28.659, kl: 41.668, loss: 59.338\n",
      " 24%|██▍       | 243/1000 [07:32<23:36,  1.87s/it]INFO:root:global_step: 243, logpy: -27.980, kl: 41.491, loss: 58.591\n",
      " 24%|██▍       | 244/1000 [07:34<23:35,  1.87s/it]INFO:root:global_step: 244, logpy: -27.277, kl: 41.308, loss: 57.815\n",
      " 24%|██▍       | 245/1000 [07:36<23:37,  1.88s/it]INFO:root:global_step: 245, logpy: -26.592, kl: 41.150, loss: 57.080\n",
      " 25%|██▍       | 246/1000 [07:38<23:39,  1.88s/it]INFO:root:global_step: 246, logpy: -25.921, kl: 40.982, loss: 56.347\n",
      " 25%|██▍       | 247/1000 [07:39<23:36,  1.88s/it]INFO:root:global_step: 247, logpy: -25.255, kl: 40.826, loss: 55.631\n",
      " 25%|██▍       | 248/1000 [07:41<23:30,  1.88s/it]INFO:root:global_step: 248, logpy: -24.579, kl: 40.643, loss: 54.876\n",
      " 25%|██▍       | 249/1000 [07:43<23:30,  1.88s/it]INFO:root:global_step: 249, logpy: -23.916, kl: 40.459, loss: 54.133\n",
      " 25%|██▌       | 250/1000 [07:45<23:32,  1.88s/it]INFO:root:Saved figure at: ./sim/global_step_250.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 250, logpy: -23.259, kl: 40.307, loss: 53.426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 251/1000 [07:48<27:44,  2.22s/it]INFO:root:global_step: 251, logpy: -22.602, kl: 40.158, loss: 52.722\n",
      " 25%|██▌       | 252/1000 [07:50<26:58,  2.16s/it]INFO:root:global_step: 252, logpy: -21.962, kl: 40.021, loss: 52.045\n",
      " 25%|██▌       | 253/1000 [07:52<26:14,  2.11s/it]INFO:root:global_step: 253, logpy: -21.314, kl: 39.857, loss: 51.332\n",
      " 25%|██▌       | 254/1000 [07:54<25:28,  2.05s/it]INFO:root:global_step: 254, logpy: -20.693, kl: 39.700, loss: 50.653\n",
      " 26%|██▌       | 255/1000 [07:56<24:57,  2.01s/it]INFO:root:global_step: 255, logpy: -20.074, kl: 39.542, loss: 49.973\n",
      " 26%|██▌       | 256/1000 [07:58<24:29,  1.97s/it]INFO:root:global_step: 256, logpy: -19.463, kl: 39.375, loss: 49.291\n",
      " 26%|██▌       | 257/1000 [08:00<24:05,  1.95s/it]INFO:root:global_step: 257, logpy: -18.865, kl: 39.230, loss: 48.644\n",
      " 26%|██▌       | 258/1000 [08:02<23:46,  1.92s/it]INFO:root:global_step: 258, logpy: -18.258, kl: 39.081, loss: 47.982\n",
      " 26%|██▌       | 259/1000 [08:03<23:34,  1.91s/it]INFO:root:global_step: 259, logpy: -17.646, kl: 38.919, loss: 47.302\n",
      " 26%|██▌       | 260/1000 [08:05<23:26,  1.90s/it]INFO:root:global_step: 260, logpy: -17.038, kl: 38.755, loss: 46.623\n",
      " 26%|██▌       | 261/1000 [08:07<23:43,  1.93s/it]INFO:root:global_step: 261, logpy: -16.450, kl: 38.584, loss: 45.955\n",
      " 26%|██▌       | 262/1000 [08:09<23:32,  1.91s/it]INFO:root:global_step: 262, logpy: -15.901, kl: 38.432, loss: 45.345\n",
      " 26%|██▋       | 263/1000 [08:11<23:22,  1.90s/it]INFO:root:global_step: 263, logpy: -15.331, kl: 38.261, loss: 44.694\n",
      " 26%|██▋       | 264/1000 [08:13<23:20,  1.90s/it]INFO:root:global_step: 264, logpy: -14.761, kl: 38.086, loss: 44.038\n",
      " 26%|██▋       | 265/1000 [08:15<23:12,  1.89s/it]INFO:root:global_step: 265, logpy: -14.192, kl: 37.920, loss: 43.391\n",
      " 27%|██▋       | 266/1000 [08:17<23:03,  1.88s/it]INFO:root:global_step: 266, logpy: -13.614, kl: 37.758, loss: 42.738\n",
      " 27%|██▋       | 267/1000 [08:19<23:00,  1.88s/it]INFO:root:global_step: 267, logpy: -13.056, kl: 37.629, loss: 42.138\n",
      " 27%|██▋       | 268/1000 [08:20<22:59,  1.88s/it]INFO:root:global_step: 268, logpy: -12.485, kl: 37.465, loss: 41.488\n",
      " 27%|██▋       | 269/1000 [08:22<23:06,  1.90s/it]INFO:root:global_step: 269, logpy: -11.926, kl: 37.307, loss: 40.856\n",
      " 27%|██▋       | 270/1000 [08:24<23:20,  1.92s/it]INFO:root:global_step: 270, logpy: -11.359, kl: 37.138, loss: 40.204\n",
      " 27%|██▋       | 271/1000 [08:26<23:17,  1.92s/it]INFO:root:global_step: 271, logpy: -10.833, kl: 37.005, loss: 39.627\n",
      " 27%|██▋       | 272/1000 [08:28<22:59,  1.90s/it]INFO:root:global_step: 272, logpy: -10.325, kl: 36.848, loss: 39.044\n",
      " 27%|██▋       | 273/1000 [08:30<23:01,  1.90s/it]INFO:root:global_step: 273, logpy: -9.836, kl: 36.700, loss: 38.488\n",
      " 27%|██▋       | 274/1000 [08:32<22:56,  1.90s/it]INFO:root:global_step: 274, logpy: -9.370, kl: 36.554, loss: 37.957\n",
      " 28%|██▊       | 275/1000 [08:34<22:52,  1.89s/it]INFO:root:global_step: 275, logpy: -8.904, kl: 36.411, loss: 37.427\n",
      " 28%|██▊       | 276/1000 [08:36<22:48,  1.89s/it]INFO:root:global_step: 276, logpy: -8.397, kl: 36.265, loss: 36.853\n",
      " 28%|██▊       | 277/1000 [08:38<22:44,  1.89s/it]INFO:root:global_step: 277, logpy: -7.885, kl: 36.121, loss: 36.276\n",
      " 28%|██▊       | 278/1000 [08:39<22:35,  1.88s/it]INFO:root:global_step: 278, logpy: -7.403, kl: 36.010, loss: 35.760\n",
      " 28%|██▊       | 279/1000 [08:41<22:31,  1.87s/it]INFO:root:global_step: 279, logpy: -6.923, kl: 35.865, loss: 35.212\n",
      " 28%|██▊       | 280/1000 [08:43<22:29,  1.87s/it]INFO:root:global_step: 280, logpy: -6.428, kl: 35.713, loss: 34.641\n",
      " 28%|██▊       | 281/1000 [08:45<22:39,  1.89s/it]INFO:root:global_step: 281, logpy: -5.965, kl: 35.624, loss: 34.164\n",
      " 28%|██▊       | 282/1000 [08:47<22:27,  1.88s/it]INFO:root:global_step: 282, logpy: -5.503, kl: 35.509, loss: 33.661\n",
      " 28%|██▊       | 283/1000 [08:49<22:19,  1.87s/it]INFO:root:global_step: 283, logpy: -5.020, kl: 35.383, loss: 33.125\n",
      " 28%|██▊       | 284/1000 [08:51<22:21,  1.87s/it]INFO:root:global_step: 284, logpy: -4.523, kl: 35.239, loss: 32.557\n",
      " 28%|██▊       | 285/1000 [08:53<22:30,  1.89s/it]INFO:root:global_step: 285, logpy: -4.036, kl: 35.092, loss: 31.995\n",
      " 29%|██▊       | 286/1000 [08:55<23:00,  1.93s/it]INFO:root:global_step: 286, logpy: -3.565, kl: 34.956, loss: 31.460\n",
      " 29%|██▊       | 287/1000 [08:56<22:43,  1.91s/it]INFO:root:global_step: 287, logpy: -3.083, kl: 34.808, loss: 30.901\n",
      " 29%|██▉       | 288/1000 [08:58<22:54,  1.93s/it]INFO:root:global_step: 288, logpy: -2.623, kl: 34.704, loss: 30.406\n",
      " 29%|██▉       | 289/1000 [09:00<23:03,  1.95s/it]INFO:root:global_step: 289, logpy: -2.177, kl: 34.582, loss: 29.907\n",
      " 29%|██▉       | 290/1000 [09:02<23:03,  1.95s/it]INFO:root:global_step: 290, logpy: -1.747, kl: 34.472, loss: 29.436\n",
      " 29%|██▉       | 291/1000 [09:04<22:49,  1.93s/it]INFO:root:global_step: 291, logpy: -1.330, kl: 34.334, loss: 28.949\n",
      " 29%|██▉       | 292/1000 [09:06<23:04,  1.96s/it]INFO:root:global_step: 292, logpy: -0.991, kl: 34.201, loss: 28.544\n",
      " 29%|██▉       | 293/1000 [09:08<23:04,  1.96s/it]INFO:root:global_step: 293, logpy: -0.624, kl: 34.063, loss: 28.106\n",
      " 29%|██▉       | 294/1000 [09:10<23:00,  1.96s/it]INFO:root:global_step: 294, logpy: -0.225, kl: 33.921, loss: 27.630\n",
      " 30%|██▉       | 295/1000 [09:12<22:55,  1.95s/it]INFO:root:global_step: 295, logpy: 0.210, kl: 33.797, loss: 27.136\n",
      " 30%|██▉       | 296/1000 [09:14<22:47,  1.94s/it]INFO:root:global_step: 296, logpy: 0.632, kl: 33.680, loss: 26.662\n",
      " 30%|██▉       | 297/1000 [09:16<22:29,  1.92s/it]INFO:root:global_step: 297, logpy: 1.058, kl: 33.557, loss: 26.177\n",
      " 30%|██▉       | 298/1000 [09:18<22:20,  1.91s/it]INFO:root:global_step: 298, logpy: 1.481, kl: 33.427, loss: 25.686\n",
      " 30%|██▉       | 299/1000 [09:20<22:31,  1.93s/it]INFO:root:global_step: 299, logpy: 1.899, kl: 33.316, loss: 25.220\n",
      " 30%|███       | 300/1000 [09:22<22:23,  1.92s/it]INFO:root:Saved figure at: ./sim/global_step_300.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 300, logpy: 2.303, kl: 33.177, loss: 24.740\n",
      " 30%|███       | 301/1000 [09:25<26:35,  2.28s/it]INFO:root:global_step: 301, logpy: 2.709, kl: 33.058, loss: 24.276\n",
      " 30%|███       | 302/1000 [09:27<26:05,  2.24s/it]INFO:root:global_step: 302, logpy: 3.092, kl: 32.940, loss: 23.834\n",
      " 30%|███       | 303/1000 [09:29<25:51,  2.23s/it]INFO:root:global_step: 303, logpy: 3.413, kl: 32.824, loss: 23.459\n",
      " 30%|███       | 304/1000 [09:31<24:46,  2.14s/it]INFO:root:global_step: 304, logpy: 3.805, kl: 32.692, loss: 22.995\n",
      " 30%|███       | 305/1000 [09:33<23:52,  2.06s/it]INFO:root:global_step: 305, logpy: 4.171, kl: 32.594, loss: 22.589\n",
      " 31%|███       | 306/1000 [09:35<23:20,  2.02s/it]INFO:root:global_step: 306, logpy: 4.513, kl: 32.478, loss: 22.189\n",
      " 31%|███       | 307/1000 [09:37<22:55,  1.99s/it]INFO:root:global_step: 307, logpy: 4.828, kl: 32.353, loss: 21.807\n",
      " 31%|███       | 308/1000 [09:39<22:45,  1.97s/it]INFO:root:global_step: 308, logpy: 5.164, kl: 32.236, loss: 21.411\n",
      " 31%|███       | 309/1000 [09:41<22:26,  1.95s/it]INFO:root:global_step: 309, logpy: 5.537, kl: 32.120, loss: 20.979\n",
      " 31%|███       | 310/1000 [09:43<22:19,  1.94s/it]INFO:root:global_step: 310, logpy: 5.920, kl: 31.994, loss: 20.526\n",
      " 31%|███       | 311/1000 [09:45<22:23,  1.95s/it]INFO:root:global_step: 311, logpy: 6.314, kl: 31.886, loss: 20.080\n",
      " 31%|███       | 312/1000 [09:46<22:26,  1.96s/it]INFO:root:global_step: 312, logpy: 6.702, kl: 31.782, loss: 19.642\n",
      " 31%|███▏      | 313/1000 [09:48<22:12,  1.94s/it]INFO:root:global_step: 313, logpy: 7.040, kl: 31.676, loss: 19.253\n",
      " 31%|███▏      | 314/1000 [09:50<22:08,  1.94s/it]INFO:root:global_step: 314, logpy: 7.370, kl: 31.569, loss: 18.870\n",
      " 32%|███▏      | 315/1000 [09:52<22:01,  1.93s/it]INFO:root:global_step: 315, logpy: 7.688, kl: 31.457, loss: 18.493\n",
      " 32%|███▏      | 316/1000 [09:54<22:05,  1.94s/it]INFO:root:global_step: 316, logpy: 8.036, kl: 31.348, loss: 18.088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 317/1000 [09:56<22:34,  1.98s/it]INFO:root:global_step: 317, logpy: 8.384, kl: 31.247, loss: 17.692\n",
      " 32%|███▏      | 318/1000 [09:58<22:43,  2.00s/it]INFO:root:global_step: 318, logpy: 8.774, kl: 31.118, loss: 17.224\n",
      " 32%|███▏      | 319/1000 [10:00<23:00,  2.03s/it]INFO:root:global_step: 319, logpy: 9.125, kl: 31.022, loss: 16.828\n",
      " 32%|███▏      | 320/1000 [10:02<22:34,  1.99s/it]INFO:root:global_step: 320, logpy: 9.487, kl: 30.922, loss: 16.417\n",
      " 32%|███▏      | 321/1000 [10:04<22:19,  1.97s/it]INFO:root:global_step: 321, logpy: 9.850, kl: 30.825, loss: 16.007\n",
      " 32%|███▏      | 322/1000 [10:06<22:17,  1.97s/it]INFO:root:global_step: 322, logpy: 10.221, kl: 30.716, loss: 15.577\n",
      " 32%|███▏      | 323/1000 [10:08<22:12,  1.97s/it]INFO:root:global_step: 323, logpy: 10.572, kl: 30.614, loss: 15.174\n",
      " 32%|███▏      | 324/1000 [10:10<22:00,  1.95s/it]INFO:root:global_step: 324, logpy: 10.908, kl: 30.518, loss: 14.790\n",
      " 32%|███▎      | 325/1000 [10:12<21:45,  1.93s/it]INFO:root:global_step: 325, logpy: 11.251, kl: 30.416, loss: 14.393\n",
      " 33%|███▎      | 326/1000 [10:14<21:36,  1.92s/it]INFO:root:global_step: 326, logpy: 11.594, kl: 30.323, loss: 14.005\n",
      " 33%|███▎      | 327/1000 [10:16<21:24,  1.91s/it]INFO:root:global_step: 327, logpy: 11.919, kl: 30.224, loss: 13.628\n",
      " 33%|███▎      | 328/1000 [10:18<21:18,  1.90s/it]INFO:root:global_step: 328, logpy: 12.242, kl: 30.133, loss: 13.260\n",
      " 33%|███▎      | 329/1000 [10:20<21:30,  1.92s/it]INFO:root:global_step: 329, logpy: 12.564, kl: 30.042, loss: 12.894\n",
      " 33%|███▎      | 330/1000 [10:22<21:32,  1.93s/it]INFO:root:global_step: 330, logpy: 12.901, kl: 29.950, loss: 12.511\n",
      " 33%|███▎      | 331/1000 [10:24<21:40,  1.94s/it]INFO:root:global_step: 331, logpy: 13.239, kl: 29.841, loss: 12.110\n",
      " 33%|███▎      | 332/1000 [10:26<22:01,  1.98s/it]INFO:root:global_step: 332, logpy: 13.566, kl: 29.746, loss: 11.732\n",
      " 33%|███▎      | 333/1000 [10:28<22:11,  2.00s/it]INFO:root:global_step: 333, logpy: 13.893, kl: 29.648, loss: 11.352\n",
      " 33%|███▎      | 334/1000 [10:30<21:58,  1.98s/it]INFO:root:global_step: 334, logpy: 14.186, kl: 29.570, loss: 11.025\n",
      " 34%|███▎      | 335/1000 [10:31<21:41,  1.96s/it]INFO:root:global_step: 335, logpy: 14.488, kl: 29.488, loss: 10.684\n",
      " 34%|███▎      | 336/1000 [10:33<21:28,  1.94s/it]INFO:root:global_step: 336, logpy: 14.807, kl: 29.373, loss: 10.294\n",
      " 34%|███▎      | 337/1000 [10:35<21:40,  1.96s/it]INFO:root:global_step: 337, logpy: 15.133, kl: 29.275, loss: 9.912\n",
      " 34%|███▍      | 338/1000 [10:37<21:30,  1.95s/it]INFO:root:global_step: 338, logpy: 15.443, kl: 29.176, loss: 9.546\n",
      " 34%|███▍      | 339/1000 [10:39<21:18,  1.93s/it]INFO:root:global_step: 339, logpy: 15.739, kl: 29.100, loss: 9.215\n",
      " 34%|███▍      | 340/1000 [10:41<21:08,  1.92s/it]INFO:root:global_step: 340, logpy: 16.042, kl: 28.997, loss: 8.851\n",
      " 34%|███▍      | 341/1000 [10:43<21:02,  1.92s/it]INFO:root:global_step: 341, logpy: 16.305, kl: 28.911, loss: 8.543\n",
      " 34%|███▍      | 342/1000 [10:45<20:58,  1.91s/it]INFO:root:global_step: 342, logpy: 16.562, kl: 28.818, loss: 8.234\n",
      " 34%|███▍      | 343/1000 [10:47<20:53,  1.91s/it]INFO:root:global_step: 343, logpy: 16.850, kl: 28.707, loss: 7.875\n",
      " 34%|███▍      | 344/1000 [10:49<20:59,  1.92s/it]INFO:root:global_step: 344, logpy: 17.126, kl: 28.616, loss: 7.548\n",
      " 34%|███▍      | 345/1000 [10:51<20:52,  1.91s/it]INFO:root:global_step: 345, logpy: 17.420, kl: 28.507, loss: 7.184\n",
      " 35%|███▍      | 346/1000 [10:53<20:54,  1.92s/it]INFO:root:global_step: 346, logpy: 17.716, kl: 28.399, loss: 6.819\n",
      " 35%|███▍      | 347/1000 [10:55<20:59,  1.93s/it]INFO:root:global_step: 347, logpy: 18.002, kl: 28.312, loss: 6.484\n",
      " 35%|███▍      | 348/1000 [10:56<20:51,  1.92s/it]INFO:root:global_step: 348, logpy: 18.298, kl: 28.219, loss: 6.134\n",
      " 35%|███▍      | 349/1000 [10:58<20:38,  1.90s/it]INFO:root:global_step: 349, logpy: 18.596, kl: 28.118, loss: 5.773\n",
      " 35%|███▌      | 350/1000 [11:00<20:42,  1.91s/it]INFO:root:Saved figure at: ./sim/global_step_350.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 350, logpy: 18.879, kl: 28.029, loss: 5.438\n",
      " 35%|███▌      | 351/1000 [11:03<24:29,  2.26s/it]INFO:root:global_step: 351, logpy: 19.165, kl: 27.943, loss: 5.104\n",
      " 35%|███▌      | 352/1000 [11:05<23:30,  2.18s/it]INFO:root:global_step: 352, logpy: 19.442, kl: 27.861, loss: 4.781\n",
      " 35%|███▌      | 353/1000 [11:07<23:12,  2.15s/it]INFO:root:global_step: 353, logpy: 19.707, kl: 27.774, loss: 4.466\n",
      " 35%|███▌      | 354/1000 [11:09<22:29,  2.09s/it]INFO:root:global_step: 354, logpy: 19.960, kl: 27.701, loss: 4.175\n",
      " 36%|███▌      | 355/1000 [11:11<22:00,  2.05s/it]INFO:root:global_step: 355, logpy: 20.196, kl: 27.619, loss: 3.893\n",
      " 36%|███▌      | 356/1000 [11:13<21:28,  2.00s/it]INFO:root:global_step: 356, logpy: 20.400, kl: 27.533, loss: 3.638\n",
      " 36%|███▌      | 357/1000 [11:15<21:06,  1.97s/it]INFO:root:global_step: 357, logpy: 20.642, kl: 27.435, loss: 3.333\n",
      " 36%|███▌      | 358/1000 [11:17<20:51,  1.95s/it]INFO:root:global_step: 358, logpy: 20.874, kl: 27.357, loss: 3.058\n",
      " 36%|███▌      | 359/1000 [11:19<20:44,  1.94s/it]INFO:root:global_step: 359, logpy: 21.128, kl: 27.259, loss: 2.741\n",
      " 36%|███▌      | 360/1000 [11:21<20:33,  1.93s/it]INFO:root:global_step: 360, logpy: 21.385, kl: 27.172, loss: 2.431\n",
      " 36%|███▌      | 361/1000 [11:23<20:35,  1.93s/it]INFO:root:global_step: 361, logpy: 21.639, kl: 27.099, loss: 2.137\n",
      " 36%|███▌      | 362/1000 [11:25<20:28,  1.93s/it]INFO:root:global_step: 362, logpy: 21.891, kl: 27.006, loss: 1.825\n",
      " 36%|███▋      | 363/1000 [11:27<20:28,  1.93s/it]INFO:root:global_step: 363, logpy: 22.137, kl: 26.940, loss: 1.546\n",
      " 36%|███▋      | 364/1000 [11:28<20:25,  1.93s/it]INFO:root:global_step: 364, logpy: 22.379, kl: 26.873, loss: 1.270\n",
      " 36%|███▋      | 365/1000 [11:30<20:16,  1.92s/it]INFO:root:global_step: 365, logpy: 22.620, kl: 26.794, loss: 0.981\n",
      " 37%|███▋      | 366/1000 [11:32<20:14,  1.91s/it]INFO:root:global_step: 366, logpy: 22.878, kl: 26.693, loss: 0.655\n",
      " 37%|███▋      | 367/1000 [11:34<20:12,  1.92s/it]INFO:root:global_step: 367, logpy: 23.113, kl: 26.633, loss: 0.391\n",
      " 37%|███▋      | 368/1000 [11:36<20:15,  1.92s/it]INFO:root:global_step: 368, logpy: 23.359, kl: 26.548, loss: 0.091\n",
      " 37%|███▋      | 369/1000 [11:38<20:11,  1.92s/it]INFO:root:global_step: 369, logpy: 23.598, kl: 26.451, loss: -0.213\n",
      " 37%|███▋      | 370/1000 [11:40<20:10,  1.92s/it]INFO:root:global_step: 370, logpy: 23.817, kl: 26.358, loss: -0.495\n",
      " 37%|███▋      | 371/1000 [11:42<20:35,  1.96s/it]INFO:root:global_step: 371, logpy: 24.026, kl: 26.273, loss: -0.759\n",
      " 37%|███▋      | 372/1000 [11:44<20:28,  1.96s/it]INFO:root:global_step: 372, logpy: 24.235, kl: 26.225, loss: -0.986\n",
      " 37%|███▋      | 373/1000 [11:46<20:15,  1.94s/it]INFO:root:global_step: 373, logpy: 24.474, kl: 26.150, loss: -1.270\n",
      " 37%|███▋      | 374/1000 [11:48<20:08,  1.93s/it]INFO:root:global_step: 374, logpy: 24.707, kl: 26.063, loss: -1.559\n",
      " 38%|███▊      | 375/1000 [11:50<20:06,  1.93s/it]INFO:root:global_step: 375, logpy: 24.922, kl: 25.994, loss: -1.815\n",
      " 38%|███▊      | 376/1000 [11:52<20:04,  1.93s/it]INFO:root:global_step: 376, logpy: 25.124, kl: 25.916, loss: -2.066\n",
      " 38%|███▊      | 377/1000 [11:54<20:02,  1.93s/it]INFO:root:global_step: 377, logpy: 25.312, kl: 25.859, loss: -2.282\n",
      " 38%|███▊      | 378/1000 [11:56<20:07,  1.94s/it]INFO:root:global_step: 378, logpy: 25.462, kl: 25.797, loss: -2.466\n",
      " 38%|███▊      | 379/1000 [11:58<20:09,  1.95s/it]INFO:root:global_step: 379, logpy: 25.644, kl: 25.726, loss: -2.691\n",
      " 38%|███▊      | 380/1000 [11:59<20:02,  1.94s/it]INFO:root:global_step: 380, logpy: 25.830, kl: 25.669, loss: -2.906\n",
      " 38%|███▊      | 381/1000 [12:01<20:10,  1.96s/it]INFO:root:global_step: 381, logpy: 26.011, kl: 25.599, loss: -3.130\n",
      " 38%|███▊      | 382/1000 [12:03<20:10,  1.96s/it]INFO:root:global_step: 382, logpy: 26.221, kl: 25.529, loss: -3.383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 383/1000 [12:05<19:59,  1.94s/it]INFO:root:global_step: 383, logpy: 26.425, kl: 25.464, loss: -3.625\n",
      " 38%|███▊      | 384/1000 [12:07<19:50,  1.93s/it]INFO:root:global_step: 384, logpy: 26.628, kl: 25.397, loss: -3.868\n",
      " 38%|███▊      | 385/1000 [12:09<19:38,  1.92s/it]INFO:root:global_step: 385, logpy: 26.826, kl: 25.324, loss: -4.113\n",
      " 39%|███▊      | 386/1000 [12:11<19:39,  1.92s/it]INFO:root:global_step: 386, logpy: 26.956, kl: 25.278, loss: -4.262\n",
      " 39%|███▊      | 387/1000 [12:13<19:38,  1.92s/it]INFO:root:global_step: 387, logpy: 27.044, kl: 25.209, loss: -4.394\n",
      " 39%|███▉      | 388/1000 [12:15<19:40,  1.93s/it]INFO:root:global_step: 388, logpy: 27.064, kl: 25.157, loss: -4.441\n",
      " 39%|███▉      | 389/1000 [12:17<19:48,  1.94s/it]INFO:root:global_step: 389, logpy: 27.217, kl: 25.106, loss: -4.619\n",
      " 39%|███▉      | 390/1000 [12:19<19:56,  1.96s/it]INFO:root:global_step: 390, logpy: 27.387, kl: 25.034, loss: -4.835\n",
      " 39%|███▉      | 391/1000 [12:21<20:08,  1.98s/it]INFO:root:global_step: 391, logpy: 27.438, kl: 24.960, loss: -4.936\n",
      " 39%|███▉      | 392/1000 [12:23<19:53,  1.96s/it]INFO:root:global_step: 392, logpy: 27.522, kl: 24.904, loss: -5.051\n",
      " 39%|███▉      | 393/1000 [12:25<19:50,  1.96s/it]INFO:root:global_step: 393, logpy: 27.680, kl: 24.839, loss: -5.250\n",
      " 39%|███▉      | 394/1000 [12:27<19:42,  1.95s/it]INFO:root:global_step: 394, logpy: 27.873, kl: 24.773, loss: -5.485\n",
      " 40%|███▉      | 395/1000 [12:29<19:35,  1.94s/it]INFO:root:global_step: 395, logpy: 28.044, kl: 24.721, loss: -5.684\n",
      " 40%|███▉      | 396/1000 [12:31<19:29,  1.94s/it]INFO:root:global_step: 396, logpy: 28.240, kl: 24.667, loss: -5.911\n",
      " 40%|███▉      | 397/1000 [12:32<19:25,  1.93s/it]INFO:root:global_step: 397, logpy: 28.420, kl: 24.619, loss: -6.116\n",
      " 40%|███▉      | 398/1000 [12:34<19:26,  1.94s/it]INFO:root:global_step: 398, logpy: 28.607, kl: 24.545, loss: -6.353\n",
      " 40%|███▉      | 399/1000 [12:36<19:27,  1.94s/it]INFO:root:global_step: 399, logpy: 28.781, kl: 24.466, loss: -6.583\n",
      " 40%|████      | 400/1000 [12:38<19:21,  1.94s/it]INFO:root:Saved figure at: ./sim/global_step_400.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 400, logpy: 28.944, kl: 24.394, loss: -6.795\n",
      " 40%|████      | 401/1000 [12:41<22:56,  2.30s/it]INFO:root:global_step: 401, logpy: 29.091, kl: 24.347, loss: -6.967\n",
      " 40%|████      | 402/1000 [12:43<22:03,  2.21s/it]INFO:root:global_step: 402, logpy: 29.232, kl: 24.282, loss: -7.151\n",
      " 40%|████      | 403/1000 [12:45<21:19,  2.14s/it]INFO:root:global_step: 403, logpy: 29.397, kl: 24.224, loss: -7.352\n",
      " 40%|████      | 404/1000 [12:47<20:44,  2.09s/it]INFO:root:global_step: 404, logpy: 29.582, kl: 24.163, loss: -7.577\n",
      " 40%|████      | 405/1000 [12:49<20:35,  2.08s/it]INFO:root:global_step: 405, logpy: 29.747, kl: 24.102, loss: -7.781\n",
      " 41%|████      | 406/1000 [12:51<20:02,  2.02s/it]INFO:root:global_step: 406, logpy: 29.929, kl: 24.050, loss: -7.993\n",
      " 41%|████      | 407/1000 [12:53<19:47,  2.00s/it]INFO:root:global_step: 407, logpy: 30.097, kl: 23.999, loss: -8.191\n",
      " 41%|████      | 408/1000 [12:55<19:45,  2.00s/it]INFO:root:global_step: 408, logpy: 30.260, kl: 23.945, loss: -8.387\n",
      " 41%|████      | 409/1000 [12:57<19:36,  1.99s/it]INFO:root:global_step: 409, logpy: 30.412, kl: 23.872, loss: -8.591\n",
      " 41%|████      | 410/1000 [12:59<19:32,  1.99s/it]INFO:root:global_step: 410, logpy: 30.572, kl: 23.817, loss: -8.785\n",
      " 41%|████      | 411/1000 [13:01<19:19,  1.97s/it]INFO:root:global_step: 411, logpy: 30.735, kl: 23.767, loss: -8.978\n",
      " 41%|████      | 412/1000 [13:03<19:05,  1.95s/it]INFO:root:global_step: 412, logpy: 30.912, kl: 23.705, loss: -9.197\n",
      " 41%|████▏     | 413/1000 [13:05<19:04,  1.95s/it]INFO:root:global_step: 413, logpy: 31.086, kl: 23.643, loss: -9.413\n",
      " 41%|████▏     | 414/1000 [13:07<19:11,  1.97s/it]INFO:root:global_step: 414, logpy: 31.245, kl: 23.588, loss: -9.607\n",
      " 42%|████▏     | 415/1000 [13:09<19:04,  1.96s/it]INFO:root:global_step: 415, logpy: 31.395, kl: 23.548, loss: -9.778\n",
      " 42%|████▏     | 416/1000 [13:11<19:01,  1.95s/it]INFO:root:global_step: 416, logpy: 31.542, kl: 23.505, loss: -9.949\n",
      " 42%|████▏     | 417/1000 [13:13<19:01,  1.96s/it]INFO:root:global_step: 417, logpy: 31.683, kl: 23.463, loss: -10.112\n",
      " 42%|████▏     | 418/1000 [13:15<19:01,  1.96s/it]INFO:root:global_step: 418, logpy: 31.831, kl: 23.400, loss: -10.305\n",
      " 42%|████▏     | 419/1000 [13:17<18:58,  1.96s/it]INFO:root:global_step: 419, logpy: 31.973, kl: 23.335, loss: -10.493\n",
      " 42%|████▏     | 420/1000 [13:19<19:03,  1.97s/it]INFO:root:global_step: 420, logpy: 32.139, kl: 23.272, loss: -10.703\n",
      " 42%|████▏     | 421/1000 [13:21<19:03,  1.97s/it]INFO:root:global_step: 421, logpy: 32.292, kl: 23.217, loss: -10.893\n",
      " 42%|████▏     | 422/1000 [13:23<19:10,  1.99s/it]INFO:root:global_step: 422, logpy: 32.462, kl: 23.153, loss: -11.109\n",
      " 42%|████▏     | 423/1000 [13:25<19:23,  2.02s/it]INFO:root:global_step: 423, logpy: 32.620, kl: 23.111, loss: -11.291\n",
      " 42%|████▏     | 424/1000 [13:27<19:07,  1.99s/it]INFO:root:global_step: 424, logpy: 32.785, kl: 23.056, loss: -11.493\n",
      " 42%|████▎     | 425/1000 [13:29<18:57,  1.98s/it]INFO:root:global_step: 425, logpy: 32.966, kl: 22.995, loss: -11.717\n",
      " 43%|████▎     | 426/1000 [13:31<18:52,  1.97s/it]INFO:root:global_step: 426, logpy: 33.137, kl: 22.931, loss: -11.935\n",
      " 43%|████▎     | 427/1000 [13:33<18:43,  1.96s/it]INFO:root:global_step: 427, logpy: 33.287, kl: 22.882, loss: -12.117\n",
      " 43%|████▎     | 428/1000 [13:35<18:40,  1.96s/it]INFO:root:global_step: 428, logpy: 33.421, kl: 22.827, loss: -12.289\n",
      " 43%|████▎     | 429/1000 [13:37<18:31,  1.95s/it]INFO:root:global_step: 429, logpy: 33.524, kl: 22.779, loss: -12.423\n",
      " 43%|████▎     | 430/1000 [13:39<18:36,  1.96s/it]INFO:root:global_step: 430, logpy: 33.649, kl: 22.730, loss: -12.579\n",
      " 43%|████▎     | 431/1000 [13:40<18:21,  1.94s/it]INFO:root:global_step: 431, logpy: 33.789, kl: 22.675, loss: -12.759\n",
      " 43%|████▎     | 432/1000 [13:42<18:23,  1.94s/it]INFO:root:global_step: 432, logpy: 33.937, kl: 22.632, loss: -12.933\n",
      " 43%|████▎     | 433/1000 [13:44<18:25,  1.95s/it]INFO:root:global_step: 433, logpy: 34.089, kl: 22.590, loss: -13.111\n",
      " 43%|████▎     | 434/1000 [13:46<18:25,  1.95s/it]INFO:root:global_step: 434, logpy: 34.225, kl: 22.545, loss: -13.276\n",
      " 44%|████▎     | 435/1000 [13:48<18:21,  1.95s/it]INFO:root:global_step: 435, logpy: 34.377, kl: 22.482, loss: -13.475\n",
      " 44%|████▎     | 436/1000 [13:50<18:23,  1.96s/it]INFO:root:global_step: 436, logpy: 34.536, kl: 22.425, loss: -13.675\n",
      " 44%|████▎     | 437/1000 [13:52<18:24,  1.96s/it]INFO:root:global_step: 437, logpy: 34.688, kl: 22.368, loss: -13.869\n",
      " 44%|████▍     | 438/1000 [13:54<18:23,  1.96s/it]INFO:root:global_step: 438, logpy: 34.833, kl: 22.308, loss: -14.057\n",
      " 44%|████▍     | 439/1000 [13:56<18:28,  1.98s/it]INFO:root:global_step: 439, logpy: 34.956, kl: 22.253, loss: -14.221\n",
      " 44%|████▍     | 440/1000 [13:58<18:39,  2.00s/it]INFO:root:global_step: 440, logpy: 35.046, kl: 22.187, loss: -14.361\n",
      " 44%|████▍     | 441/1000 [14:00<18:33,  1.99s/it]INFO:root:global_step: 441, logpy: 35.118, kl: 22.141, loss: -14.464\n",
      " 44%|████▍     | 442/1000 [14:02<18:22,  1.98s/it]INFO:root:global_step: 442, logpy: 35.228, kl: 22.095, loss: -14.606\n",
      " 44%|████▍     | 443/1000 [14:04<18:20,  1.98s/it]INFO:root:global_step: 443, logpy: 35.373, kl: 22.043, loss: -14.787\n",
      " 44%|████▍     | 444/1000 [14:06<18:11,  1.96s/it]INFO:root:global_step: 444, logpy: 35.494, kl: 21.993, loss: -14.945\n",
      " 44%|████▍     | 445/1000 [14:08<18:08,  1.96s/it]INFO:root:global_step: 445, logpy: 35.574, kl: 21.959, loss: -15.043\n",
      " 45%|████▍     | 446/1000 [14:10<17:59,  1.95s/it]INFO:root:global_step: 446, logpy: 35.620, kl: 21.923, loss: -15.112\n",
      " 45%|████▍     | 447/1000 [14:12<17:58,  1.95s/it]INFO:root:global_step: 447, logpy: 35.741, kl: 21.879, loss: -15.263\n",
      " 45%|████▍     | 448/1000 [14:14<17:54,  1.95s/it]INFO:root:global_step: 448, logpy: 35.886, kl: 21.825, loss: -15.447\n",
      " 45%|████▍     | 449/1000 [14:16<17:53,  1.95s/it]INFO:root:global_step: 449, logpy: 36.010, kl: 21.779, loss: -15.603\n",
      " 45%|████▌     | 450/1000 [14:18<17:56,  1.96s/it]INFO:root:Saved figure at: ./sim/global_step_450.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 450, logpy: 36.106, kl: 21.738, loss: -15.726\n",
      " 45%|████▌     | 451/1000 [14:21<21:16,  2.33s/it]INFO:root:global_step: 451, logpy: 36.188, kl: 21.691, loss: -15.842\n",
      " 45%|████▌     | 452/1000 [14:23<20:22,  2.23s/it]INFO:root:global_step: 452, logpy: 36.308, kl: 21.641, loss: -15.999\n",
      " 45%|████▌     | 453/1000 [14:25<19:56,  2.19s/it]INFO:root:global_step: 453, logpy: 36.417, kl: 21.601, loss: -16.133\n",
      " 45%|████▌     | 454/1000 [14:27<19:18,  2.12s/it]INFO:root:global_step: 454, logpy: 36.559, kl: 21.552, loss: -16.313\n",
      " 46%|████▌     | 455/1000 [14:29<19:01,  2.09s/it]INFO:root:global_step: 455, logpy: 36.678, kl: 21.515, loss: -16.455\n",
      " 46%|████▌     | 456/1000 [14:31<18:31,  2.04s/it]INFO:root:global_step: 456, logpy: 36.747, kl: 21.477, loss: -16.549\n",
      " 46%|████▌     | 457/1000 [14:33<18:13,  2.01s/it]INFO:root:global_step: 457, logpy: 36.817, kl: 21.436, loss: -16.647\n",
      " 46%|████▌     | 458/1000 [14:35<18:00,  1.99s/it]INFO:root:global_step: 458, logpy: 36.895, kl: 21.391, loss: -16.757\n",
      " 46%|████▌     | 459/1000 [14:37<17:49,  1.98s/it]INFO:root:global_step: 459, logpy: 36.982, kl: 21.352, loss: -16.871\n",
      " 46%|████▌     | 460/1000 [14:39<17:47,  1.98s/it]INFO:root:global_step: 460, logpy: 37.071, kl: 21.304, loss: -16.996\n",
      " 46%|████▌     | 461/1000 [14:41<17:42,  1.97s/it]INFO:root:global_step: 461, logpy: 37.195, kl: 21.244, loss: -17.167\n",
      " 46%|████▌     | 462/1000 [14:43<17:40,  1.97s/it]INFO:root:global_step: 462, logpy: 37.316, kl: 21.200, loss: -17.321\n",
      " 46%|████▋     | 463/1000 [14:45<17:39,  1.97s/it]INFO:root:global_step: 463, logpy: 37.424, kl: 21.156, loss: -17.459\n",
      " 46%|████▋     | 464/1000 [14:47<17:38,  1.97s/it]INFO:root:global_step: 464, logpy: 37.525, kl: 21.122, loss: -17.584\n",
      " 46%|████▋     | 465/1000 [14:49<17:32,  1.97s/it]INFO:root:global_step: 465, logpy: 37.657, kl: 21.067, loss: -17.759\n",
      " 47%|████▋     | 466/1000 [14:51<17:32,  1.97s/it]INFO:root:global_step: 466, logpy: 37.782, kl: 21.022, loss: -17.917\n",
      " 47%|████▋     | 467/1000 [14:53<17:32,  1.97s/it]INFO:root:global_step: 467, logpy: 37.904, kl: 20.984, loss: -18.065\n",
      " 47%|████▋     | 468/1000 [14:55<17:35,  1.98s/it]INFO:root:global_step: 468, logpy: 38.017, kl: 20.962, loss: -18.188\n",
      " 47%|████▋     | 469/1000 [14:56<17:25,  1.97s/it]INFO:root:global_step: 469, logpy: 38.146, kl: 20.921, loss: -18.348\n",
      " 47%|████▋     | 470/1000 [14:58<17:27,  1.98s/it]INFO:root:global_step: 470, logpy: 38.262, kl: 20.874, loss: -18.500\n",
      " 47%|████▋     | 471/1000 [15:00<17:19,  1.97s/it]INFO:root:global_step: 471, logpy: 38.369, kl: 20.839, loss: -18.630\n",
      " 47%|████▋     | 472/1000 [15:02<17:11,  1.95s/it]INFO:root:global_step: 472, logpy: 38.492, kl: 20.792, loss: -18.789\n",
      " 47%|████▋     | 473/1000 [15:04<17:23,  1.98s/it]INFO:root:global_step: 473, logpy: 38.607, kl: 20.750, loss: -18.936\n",
      " 47%|████▋     | 474/1000 [15:06<17:22,  1.98s/it]INFO:root:global_step: 474, logpy: 38.737, kl: 20.711, loss: -19.093\n",
      " 48%|████▊     | 475/1000 [15:08<17:22,  1.99s/it]INFO:root:global_step: 475, logpy: 38.864, kl: 20.667, loss: -19.254\n",
      " 48%|████▊     | 476/1000 [15:10<17:16,  1.98s/it]INFO:root:global_step: 476, logpy: 38.957, kl: 20.645, loss: -19.358\n",
      " 48%|████▊     | 477/1000 [15:12<17:10,  1.97s/it]INFO:root:global_step: 477, logpy: 39.064, kl: 20.611, loss: -19.489\n",
      " 48%|████▊     | 478/1000 [15:14<17:11,  1.98s/it]INFO:root:global_step: 478, logpy: 39.175, kl: 20.568, loss: -19.632\n",
      " 48%|████▊     | 479/1000 [15:16<17:04,  1.97s/it]INFO:root:global_step: 479, logpy: 39.286, kl: 20.527, loss: -19.773\n",
      " 48%|████▊     | 480/1000 [15:18<17:01,  1.97s/it]INFO:root:global_step: 480, logpy: 39.404, kl: 20.486, loss: -19.923\n",
      " 48%|████▊     | 481/1000 [15:20<16:57,  1.96s/it]INFO:root:global_step: 481, logpy: 39.504, kl: 20.447, loss: -20.052\n",
      " 48%|████▊     | 482/1000 [15:22<16:52,  1.96s/it]INFO:root:global_step: 482, logpy: 39.597, kl: 20.414, loss: -20.167\n",
      " 48%|████▊     | 483/1000 [15:24<16:50,  1.95s/it]INFO:root:global_step: 483, logpy: 39.693, kl: 20.377, loss: -20.292\n",
      " 48%|████▊     | 484/1000 [15:26<16:49,  1.96s/it]INFO:root:global_step: 484, logpy: 39.804, kl: 20.329, loss: -20.440\n",
      " 48%|████▊     | 485/1000 [15:28<16:58,  1.98s/it]INFO:root:global_step: 485, logpy: 39.915, kl: 20.299, loss: -20.572\n",
      " 49%|████▊     | 486/1000 [15:30<16:56,  1.98s/it]INFO:root:global_step: 486, logpy: 40.018, kl: 20.269, loss: -20.695\n",
      " 49%|████▊     | 487/1000 [15:32<17:00,  1.99s/it]INFO:root:global_step: 487, logpy: 40.135, kl: 20.225, loss: -20.847\n",
      " 49%|████▉     | 488/1000 [15:34<17:03,  2.00s/it]INFO:root:global_step: 488, logpy: 40.238, kl: 20.213, loss: -20.953\n",
      " 49%|████▉     | 489/1000 [15:36<16:54,  1.99s/it]INFO:root:global_step: 489, logpy: 40.322, kl: 20.190, loss: -21.050\n",
      " 49%|████▉     | 490/1000 [15:38<16:48,  1.98s/it]INFO:root:global_step: 490, logpy: 40.419, kl: 20.152, loss: -21.176\n",
      " 49%|████▉     | 491/1000 [15:40<16:57,  2.00s/it]INFO:root:global_step: 491, logpy: 40.519, kl: 20.116, loss: -21.302\n",
      " 49%|████▉     | 492/1000 [15:42<16:57,  2.00s/it]INFO:root:global_step: 492, logpy: 40.606, kl: 20.091, loss: -21.405\n",
      " 49%|████▉     | 493/1000 [15:44<16:52,  2.00s/it]INFO:root:global_step: 493, logpy: 40.701, kl: 20.060, loss: -21.523\n",
      " 49%|████▉     | 494/1000 [15:46<16:52,  2.00s/it]INFO:root:global_step: 494, logpy: 40.786, kl: 20.039, loss: -21.620\n",
      " 50%|████▉     | 495/1000 [15:48<16:47,  2.00s/it]INFO:root:global_step: 495, logpy: 40.887, kl: 19.995, loss: -21.757\n",
      " 50%|████▉     | 496/1000 [15:50<16:42,  1.99s/it]INFO:root:global_step: 496, logpy: 40.964, kl: 19.973, loss: -21.847\n",
      " 50%|████▉     | 497/1000 [15:52<16:39,  1.99s/it]INFO:root:global_step: 497, logpy: 41.060, kl: 19.945, loss: -21.961\n",
      " 50%|████▉     | 498/1000 [15:54<16:40,  1.99s/it]INFO:root:global_step: 498, logpy: 41.158, kl: 19.904, loss: -22.093\n",
      " 50%|████▉     | 499/1000 [15:56<16:48,  2.01s/it]INFO:root:global_step: 499, logpy: 41.259, kl: 19.866, loss: -22.223\n",
      " 50%|█████     | 500/1000 [15:58<16:47,  2.02s/it]INFO:root:Saved figure at: ./sim/global_step_500.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 500, logpy: 41.346, kl: 19.837, loss: -22.332\n",
      " 50%|█████     | 501/1000 [16:01<20:06,  2.42s/it]INFO:root:global_step: 501, logpy: 41.449, kl: 19.798, loss: -22.465\n",
      " 50%|█████     | 502/1000 [16:04<20:02,  2.41s/it]INFO:root:global_step: 502, logpy: 41.539, kl: 19.779, loss: -22.565\n",
      " 50%|█████     | 503/1000 [16:06<19:09,  2.31s/it]INFO:root:global_step: 503, logpy: 41.633, kl: 19.739, loss: -22.692\n",
      " 50%|█████     | 504/1000 [16:10<15:54,  1.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_brownian/brownian_interval.py\u001b[0m in \u001b[0;36m_increment_and_space_time_levy_area\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_increment_and_space_time_levy_area_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/boltons/cacheutils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_link_and_move_to_front_of_ll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/boltons/cacheutils.py\u001b[0m in \u001b[0;36m_get_link_and_move_to_front_of_ll\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# KeyError, which is useful to __getitem__ and __setitem__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mnewest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_link_lookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: <torchsde._brownian.brownian_interval._Interval object at 0x7fb6ddf08f70>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-69b973d8e99a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msdeint_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchsde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdeint_adjoint\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adjoint'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorchsde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdeint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-2a88b004317b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlogpy\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkl\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkl_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    197\u001b[0m                                \"of them.\")\n\u001b[1;32m    198\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/adjoint.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_ys, *grad_extra_solver_state)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             (_, aug_state), *extra_solver_state = _SdeintAdjointMethod.apply(adjoint_sde,\n\u001b[0m\u001b[1;32m     99\u001b[0m                                                                              \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                                                                              \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/adjoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, sde, ts, dt, bm, solver, method, adjoint_method, adjoint_adaptive, adjoint_rtol, adjoint_atol, dt_min, adjoint_options, len_extras, y0, *extras_and_adjoint_params)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Necessary for the same reason\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mextra_solver_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mextra_solver_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_solver_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegrate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_solver_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMETHODS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreversible_heun\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0madjoint_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMETHODS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjoint_reversible_heun\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/base_solver.py\u001b[0m in \u001b[0;36mintegrate\u001b[0;34m(self, y0, ts, extra0)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     \u001b[0mprev_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                     \u001b[0mcurr_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_extra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_extra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                     \u001b[0mcurr_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_interp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/methods/milstein.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mextra0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mI_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_term\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_brownian/derived.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, ta, tb, return_U, return_A)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Whether or not to negate the statistics depends on the return value of the adjoint SDE. Currently, the adjoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# returns negated drift and diffusion, so we don't negate here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_brownian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_U\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_U\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_A\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_brownian/brownian_interval.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, ta, tb, return_U, return_A)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintervals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m             \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintervals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_increment_and_levy_area\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintervals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# If we have multiple intervals then add up their increments and Levy areas.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_brownian/brownian_interval.py\u001b[0m in \u001b[0;36m_increment_and_levy_area\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_increment_and_levy_area\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrampoline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrampoline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_increment_and_space_time_levy_area\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         A = _davie_foster_approximation(W, H, self._end - self._start, self._top._levy_area_approximation,\n\u001b[1;32m    171\u001b[0m                                         self._randn_levy)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/trampoline/__init__.py\u001b[0m in \u001b[0;36mtrampoline\u001b[0;34m(call)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/trampoline/__init__.py\u001b[0m in \u001b[0;36mtrampoline\u001b[0;34m(call)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mretval\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;31m# We use next() here for nicer exceptions if an invalid value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_brownian/brownian_interval.py\u001b[0m in \u001b[0;36m_increment_and_space_time_levy_area\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_increment_and_space_time_levy_area\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mh_reciprocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mleft_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_midway\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/trampoline/__init__.py\u001b[0m in \u001b[0;36mtrampoline\u001b[0;34m(call)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_brownian/brownian_interval.py\u001b[0m in \u001b[0;36m_increment_and_space_time_levy_area\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mout_H\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_increment_and_space_time_levy_area_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_H\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/boltons/cacheutils.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    236\u001b[0m                     \u001b[0mevicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_key_and_evict_last_in_ll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLRI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__delitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLRI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0mlink\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVALUE\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "manual_seed(args['seed'])\n",
    "\n",
    "if args['debug']:\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "ckpt_dir = os.path.join('./sim/', 'ckpts')\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "sdeint_fn = torchsde.sdeint_adjoint if args['adjoint'] else torchsde.sdeint\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356ad31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712260f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
