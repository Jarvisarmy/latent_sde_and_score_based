{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451594fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from typing import Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import distributions, nn, optim\n",
    "\n",
    "import torchsde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "338520f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the gpu is available or not, if yes, use gpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023f728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"likelihood\": \"laplace\", # specify the likelihood distribution,p(x|z)\n",
    "    \"adjoint\": True, # specify whether to use adjoint sensitivty method to backward the sde\n",
    "    \"debug\": True, # specify whether to debug or not\n",
    "    \"data\": \"segmented_cosine\",# specify which data to use\n",
    "    \"kl_anneal_iters\": 100, # number of the iterations of the annealing kl divergence schedule\n",
    "    \"train_iters\": 1000, # number of iterations to train the model\n",
    "    \"batch_size\": 100, \n",
    "    \"adaptive\": False, # whether use adaptive solver or not\n",
    "    \"method\": \"euler\", # the method of sde solver\n",
    "    \"dt\": 1e-2, # the parameter dt of sde solver\n",
    "    \"rtol\": 1e-3, # the parameter rtol of sde solver\n",
    "    \"atol\": 1e-3, # the atol of sde solver\n",
    "    \"scale\": 1, # the scale, of the likelihood distribution\n",
    "    \"dpi\": 500, # dpi of images\n",
    "    \"pause_iters\": 50 # the interval to evaluate the model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9aba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w/ underscore -> numpy; w/o underscore -> torch.\n",
    "'''\n",
    "    ts: original time series; can be segmented or irregular\n",
    "    ts_ext: with extended time outisde the time series, use to generate latent outside to penalize out-of-data region and spread uncertainty\n",
    "    ts_vis: regular time series used to plot the data\n",
    "    ys: the observed dynamic, same size as ts\n",
    "'''\n",
    "\n",
    "Data = namedtuple('Data', ['ts_', 'ts_ext_', 'ts_vis_', 'ts', 'ts_ext', 'ts_vis', 'ys', 'ys_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fcfe81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScheduler(object):\n",
    "    '''\n",
    "        output a value follows linear schedule from maxval/iters to maxval, with 'iters' steps\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "        iters = 100,\n",
    "        maxval = 1,\n",
    "        1/100 = 0.001, 0.002, .... 1\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, iters, maxval=1.0):\n",
    "        self._iters = max(1, iters)\n",
    "        self._val = maxval / self._iters\n",
    "        self._maxval = maxval\n",
    "\n",
    "    def step(self):\n",
    "        self._val = min(self._maxval, self._val + self._maxval / self._iters)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4b80998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMAMetric(object):\n",
    "    '''\n",
    "        Exponential moving average, used to calculate the average\n",
    "    '''\n",
    "    def __init__(self, gamma: Optional[float] = .99):\n",
    "        super(EMAMetric, self).__init__()\n",
    "        self._val = 0.\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def step(self, x: Union[torch.Tensor, np.ndarray]):\n",
    "        x = x.detach().cpu().numpy() if torch.is_tensor(x) else x\n",
    "        self._val = self._gamma * self._val + (1 - self._gamma) * x\n",
    "        return self._val\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e1bbc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_seed(seed: int):\n",
    "    '''\n",
    "        set the random seed, make sure the result keeps the same for each call\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5ae816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stable_division(a, b, epsilon=1e-7): #a/b\n",
    "    '''\n",
    "        change all elements x in b s.t -epsilon < x < epsilon to epsilon, to make sure the division is stable, won't cause a really large number\n",
    "    '''\n",
    "    \n",
    "    b = torch.where(b.abs().detach() > epsilon, b, torch.full_like(b, fill_value=epsilon) * b.sign())\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6930276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentSDE(torchsde.SDEIto):\n",
    "\n",
    "    def __init__(self, theta=1.0, mu=0.0, sigma=0.5):\n",
    "        super(LatentSDE, self).__init__(noise_type=\"diagonal\")\n",
    "        logvar = math.log(sigma ** 2 / (2. * theta)) # calculate the log variance\n",
    "\n",
    "        # Prior drift.\n",
    "        self.register_buffer(\"theta\", torch.tensor([[theta]])) # prior parameters, register 成buffer, 参数不会进行更新\n",
    "        self.register_buffer(\"mu\", torch.tensor([[mu]]))\n",
    "        self.register_buffer(\"sigma\", torch.tensor([[sigma]]))\n",
    "\n",
    "        # p(z0).\n",
    "        self.register_buffer(\"py0_mean\", torch.tensor([[mu]])) # setup the prior distribution\n",
    "        self.register_buffer(\"py0_logvar\", torch.tensor([[logvar]]))\n",
    "\n",
    "        # Approximate posterior drift: Takes in 2 positional encodings and the state. f(t,y)\n",
    "        self.net = nn.Sequential( #h\\Phi()\n",
    "            nn.Linear(3, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 1)\n",
    "        )\n",
    "        # Initialization trick from Glow. \n",
    "        self.net[-1].weight.data.fill_(0.)\n",
    "        self.net[-1].bias.data.fill_(0.)\n",
    "\n",
    "        # q(y0). the initial value of the approx posterior distribution\n",
    "        self.qy0_mean = nn.Parameter(torch.tensor([[mu]]), requires_grad=True) \n",
    "        self.qy0_logvar = nn.Parameter(torch.tensor([[logvar]]), requires_grad=True)\n",
    "\n",
    "    def f(self, t, y):  # Approximate posterior drift. f(t, y) = t # h\\Phi\n",
    "        if t.dim() == 0:\n",
    "            t = torch.full_like(y, fill_value=t)\n",
    "        # Positional encoding in transformers for time-inhomogeneous posterior.\n",
    "        return self.net(torch.cat((torch.sin(t), torch.cos(t), y), dim=-1))\n",
    "\n",
    "    def g(self, t, y):  # Shared diffusion. g(t,y) = sigma\n",
    "        return self.sigma.repeat(y.size(0), 1)\n",
    "\n",
    "    def h(self, t, y):  # Prior drift. h(t,y) = theta * (mu-y) # h\\Theta\n",
    "        return self.theta * (self.mu - y) # need to figure out\n",
    "\n",
    "    def f_aug(self, t, y):  # Drift for augmented dynamics with logqp term.\n",
    "        '''\n",
    "             y has two columns, the first column is y0, the one we want to generate the SDE dynamic\n",
    "             the second column is 0, used to generate the sampling paths from the posterior process, and used to estimate the kl divergence\n",
    "        '''\n",
    "        y = y[:, 0:1] # get the first column of y, that is to get y0 # z0\n",
    "        f, g, h = self.f(t, y), self.g(t, y), self.h(t, y) # calculate f, g, h\n",
    "        u = _stable_division(f - h, g) # u(z,t) = (f-h)/g\n",
    "        f_logqp = .5 * (u ** 2).sum(dim=1, keepdim=True) # (u^2)/2, the drift of the second sde\n",
    "        return torch.cat([f, f_logqp], dim=1) # [batch_size, 2]\n",
    "\n",
    "    def g_aug(self, t, y):  # Diffusion for augmented dynamics with logqp term.\n",
    "        y = y[:, 0:1]\n",
    "        g = self.g(t, y)\n",
    "        g_logqp = torch.zeros_like(y) # the diffusion of the second sde\n",
    "        return torch.cat([g, g_logqp], dim=1) # [batch_size, 2]\n",
    "\n",
    "    def forward(self, ts, batch_size, eps=None):\n",
    "        # recognition process\n",
    "        eps = torch.randn(batch_size, 1).to(self.qy0_std) if eps is None else eps\n",
    "        y0 = self.qy0_mean + eps * self.qy0_std # randomly generate z0 from approx posterior distribution q(z|x)\n",
    "        \n",
    "        \n",
    "        qy0 = distributions.Normal(loc=self.qy0_mean, scale=self.qy0_std) # approx posterior distribution\n",
    "        py0 = distributions.Normal(loc=self.py0_mean, scale=self.py0_std) # prior distribution\n",
    "        #print(distributions.kl_divergence(qy0,py0).sum(dim=1))\n",
    "        logqp0 = distributions.kl_divergence(qy0, py0).sum(dim=1)  # KL(t=0). # kl divergence when t = 0\n",
    "\n",
    "        aug_y0 = torch.cat([y0, torch.zeros(batch_size, 1).to(y0)], dim=1)\n",
    "        aug_ys = sdeint_fn(\n",
    "            sde=self,\n",
    "            y0=aug_y0,\n",
    "            ts=ts, #[0,0.1,0.2, 0.3]\n",
    "            method=args[\"method\"],\n",
    "            dt=args[\"dt\"],\n",
    "            adaptive=args[\"adaptive\"],\n",
    "            rtol=args[\"rtol\"],\n",
    "            atol=args[\"atol\"],\n",
    "            names={'drift': 'f_aug', 'diffusion': 'g_aug'}\n",
    "        )\n",
    "        #[len(ts),batch_size,2]\n",
    "        ys, logqp_path = aug_ys[:, :, 0:1], aug_ys[-1, :, 1] \n",
    "        # the first column of the last dimension is the sample dynamic\n",
    "        # the second column of the last dimension is the kl divergence\n",
    "        logqp = (logqp0 + logqp_path).mean(dim=0)  # KL(t=0) + KL(path). # calculate the kl divergence\n",
    "        return ys, logqp\n",
    "\n",
    "    def sample_p(self, ts, batch_size, eps=None, bm=None):\n",
    "        '''\n",
    "            latent variable samples from prior distribution p(z), and their SDE dynamics\n",
    "        '''\n",
    "        eps = torch.randn(batch_size, 1).to(self.py0_mean) if eps is None else eps\n",
    "        y0 = self.py0_mean + eps * self.py0_std # [batch_size, 1]: [1024, 1]\n",
    "        \n",
    "        yt = sdeint_fn(self, y0, ts, bm=bm, method='srk', dt=args[\"dt\"], names={'drift': 'h'}) # [len(ts), batch_size, 1]: [300, 1024,1]\n",
    "        return yt\n",
    "\n",
    "    def sample_q(self, ts, batch_size, eps=None, bm=None):\n",
    "        '''\n",
    "            latent variable samples from approx posterior distribution q(z|x), and their SDE dynamics\n",
    "        '''\n",
    "        eps = torch.randn(batch_size, 1).to(self.qy0_mean) if eps is None else eps\n",
    "        y0 = self.qy0_mean + eps * self.qy0_std # [batch_size, 1]: [1024, 1]\n",
    "        return sdeint_fn(self, y0, ts, bm=bm, method='srk', dt=args[\"dt\"]) # [len(ts), batch_size, 1]: [300, 1024, 1]\n",
    "\n",
    "    # self.py_std = \n",
    "    @property # declare it as a property, then we can access it through self.py0_std and specify setter and getter\n",
    "    def py0_std(self): # the standard deviation of the prior distribution p(z)\n",
    "        return torch.exp(.5 * self.py0_logvar)\n",
    "\n",
    "    @property\n",
    "    def qy0_std(self): # the standard deviation of the approx posterior distribution q(z|x)\n",
    "        return torch.exp(.5 * self.qy0_logvar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61fcd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_segmented_cosine_data():\n",
    "    ts_ = np.concatenate((np.linspace(0.3, 0.8, 10), np.linspace(1.2, 1.5, 10)), axis=0) # create segmented time series\n",
    "    ts_ext_ = np.array([0.] + list(ts_) + [2.0]) # add out-of-data time point\n",
    "    ts_vis_ = np.linspace(0., 2.0, 300) # regular time series used for visualization\n",
    "    ys_ = np.cos(ts_ * (2. * math.pi))[:, None] # get the segmented cosine data\n",
    "\n",
    "    ts = torch.tensor(ts_).float()\n",
    "    ts_ext = torch.tensor(ts_ext_).float()\n",
    "    ts_vis = torch.tensor(ts_vis_).float()\n",
    "    ys = torch.tensor(ys_).float().to(device)\n",
    "    return Data(ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_)\n",
    "\n",
    "\n",
    "def make_irregular_sine_data():\n",
    "    ts_ = np.sort(np.random.uniform(low=0.4, high=1.6, size=16)) # create irregular time series\n",
    "    ts_ext_ = np.array([0.] + list(ts_) + [2.0]) # add out-of-data time point\n",
    "    ts_vis_ = np.linspace(0., 2.0, 300) # regular time series used for visualization\n",
    "    ys_ = np.sin(ts_ * (2. * math.pi))[:, None] * 0.8 # get the irregular sine data\n",
    "\n",
    "    ts = torch.tensor(ts_).float()\n",
    "    ts_ext = torch.tensor(ts_ext_).float()\n",
    "    ts_vis = torch.tensor(ts_vis_).float()\n",
    "    ys = torch.tensor(ys_).float().to(device)\n",
    "    return Data(ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_)\n",
    "\n",
    "\n",
    "def make_data():\n",
    "    data_constructor = {\n",
    "        'segmented_cosine': make_segmented_cosine_data,\n",
    "        'irregular_sine': make_irregular_sine_data\n",
    "    }[args[\"data\"]]\n",
    "    return data_constructor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6809b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Dataset.\n",
    "    ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_ = make_data()\n",
    "\n",
    "    # Plotting parameters.\n",
    "    vis_batch_size = 1024 # the batch_size used to visaulize\n",
    "    ylims = (-1.75, 1.75) # set up the ylim of the figure\n",
    "    \n",
    "    alphas = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55]\n",
    "    percentiles = [0.999, 0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    \n",
    "    vis_idx = np.random.permutation(vis_batch_size) # shuffle the numbers from 1-vis_batch_size\n",
    "    # From https://colorbrewer2.org/.\n",
    "\n",
    "    sample_colors = ('#fc4e2a', '#e31a1c', '#bd0026') \n",
    "    fill_color = '#fd8d3c'\n",
    "    \n",
    "    mean_color = '#800026'\n",
    "    \n",
    "    num_samples = len(sample_colors)\n",
    "    \n",
    "    eps = torch.randn(vis_batch_size, 1).to(device)  # samples from normal distribution\n",
    "    \n",
    "    bm = torchsde.BrownianInterval(\n",
    "        t0=ts_vis[0],\n",
    "        t1=ts_vis[-1],\n",
    "        size=(vis_batch_size, 1),\n",
    "        device=device,\n",
    "        levy_area_approximation='space-time'\n",
    "    )  # We need space-time Levy area to use the SRK solver, fix the brownian motion allows us to generate the sde dynamic fexedly\n",
    "\n",
    "    # Model.\n",
    "    model = LatentSDE().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=.999)\n",
    "    kl_scheduler = LinearScheduler(iters=args[\"kl_anneal_iters\"])\n",
    "\n",
    "    logpy_metric = EMAMetric()\n",
    "    kl_metric = EMAMetric()\n",
    "    loss_metric = EMAMetric()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        zs = model.sample_p(ts=ts_vis, batch_size=vis_batch_size, eps=eps, bm=bm).squeeze() # sde dynamic sampled from prior distribution\n",
    "        ts_vis_, zs_ = ts_vis.cpu().numpy(), zs.cpu().numpy()  # convert them to numpy\n",
    "        # print(zs.size()) # [len(ts_vis), batch_size]: [300, 1024]\n",
    "        zs_ = np.sort(zs_, axis=1) # sort each row\n",
    "\n",
    "        img_dir = os.path.join('./img/' 'prior.png')\n",
    "        plt.subplot(frameon=False)\n",
    "        for alpha, percentile in zip(alphas, percentiles):\n",
    "            idx = int((1 - percentile) / 2. * vis_batch_size) # 选择要考虑百分之多少的数据\n",
    "            zs_bot_ = zs_[:, idx] # 计算底线\n",
    "            zs_top_ = zs_[:, -idx] # 计算顶线\n",
    "            plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color) # 用来填充两条曲线之间的区域\n",
    "\n",
    "        # `zorder` determines who's on top; the larger the more at the top.\n",
    "        plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # scatter plot the original observed dynamic\n",
    "        plt.ylim(ylims) \n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$Y_t$')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(img_dir, dpi=args[\"dpi\"])\n",
    "        plt.close()\n",
    "        logging.info(f'Saved prior figure at: {img_dir}')\n",
    "    \n",
    "    for global_step in tqdm.tqdm(range(args[\"train_iters\"])):\n",
    "        # Plot and save.\n",
    "        \n",
    "        if global_step % args[\"pause_iters\"] == 0:\n",
    "            img_path = os.path.join('./img/', f'global_step_{global_step}.png')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                zs = model.sample_q(ts=ts_vis, batch_size=vis_batch_size, eps=eps, bm=bm).squeeze()\n",
    "                samples = zs[:, vis_idx] # samples是按照之前的permutation来排序的\n",
    "                ts_vis_, zs_, samples_ = ts_vis.cpu().numpy(), zs.cpu().numpy(), samples.cpu().numpy()\n",
    "                zs_ = np.sort(zs_, axis=1)\n",
    "                plt.subplot(frameon=False)\n",
    "                \n",
    "                # same as above, plot the percentiles\n",
    "                if True: #args.show_percentiles:\n",
    "                    for alpha, percentile in zip(alphas, percentiles):\n",
    "                        idx = int((1 - percentile) / 2. * vis_batch_size)\n",
    "                        zs_bot_, zs_top_ = zs_[:, idx], zs_[:, -idx]\n",
    "                        plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
    "                \n",
    "                # plot the mean of all the SDE dynamics\n",
    "                if True: #args.show_mean:\n",
    "                    plt.plot(ts_vis_, zs_.mean(axis=1), color=mean_color)\n",
    "                # plot the first three SDE dynamics, since we shuffle the samples already, so the first SDE dynamics are random\n",
    "                if True: #args.show_samples:\n",
    "                    for j in range(num_samples):\n",
    "                        plt.plot(ts_vis_, samples_[:, j], color=sample_colors[j], linewidth=1.0)\n",
    "                \n",
    "                if True: #args.show_arrows:\n",
    "                    num, dt = 12, 0.12\n",
    "                    t, y = torch.meshgrid(\n",
    "                        [torch.linspace(0.2, 1.8, num).to(device), torch.linspace(-1.5, 1.5, num).to(device)]\n",
    "                    )\n",
    "                    #print(t.size()) # [12,12]\n",
    "                    #print(y.size()) # [12, 12]\n",
    "                    t, y = t.reshape(-1, 1), y.reshape(-1, 1)\n",
    "                    '''\n",
    "                        ex:\n",
    "                        t = [[1],[1],[2],[2],[3],[3]]\n",
    "                        y = [[1],[2],[3],[1],[2],[3]]\n",
    "                    '''\n",
    "                    fty = model.f(t=t, y=y).reshape(num, num) # call f(t,y)\n",
    "                    dt = torch.zeros(num, num).fill_(dt).to(device)\n",
    "                    dy = fty * dt # calculate the gradients\n",
    "                    dt_, dy_, t_, y_ = dt.cpu().numpy(), dy.cpu().numpy(), t.cpu().numpy(), y.cpu().numpy()\n",
    "                    plt.quiver(t_, y_, dt_, dy_, alpha=0.3, edgecolors='k', width=0.0035, scale=50) #画箭头，画风场\n",
    "\n",
    "                if False: #args.hide_ticks:\n",
    "                    plt.xticks([], [])\n",
    "                    plt.yticks([], [])\n",
    "\n",
    "                plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # scatter plot the Data.\n",
    "                plt.ylim(ylims)\n",
    "                plt.xlabel('$t$')\n",
    "                plt.ylabel('$Y_t$')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(img_path, dpi=args[\"dpi\"])\n",
    "                plt.close()\n",
    "                logging.info(f'Saved figure at: {img_path}')\n",
    "\n",
    "    \n",
    "        \n",
    "        # Train.\n",
    "        optimizer.zero_grad()\n",
    "        zs, kl = model(ts=ts_ext, batch_size=args[\"batch_size\"]) \n",
    "        # print(zs.size()) # [len(ts_ext),batch_size,1]: [22,100,1] \n",
    "        zs = zs.squeeze() # [len(ts_ext), batch_size]: [22, 100]\n",
    "        print(zs.size())\n",
    "        zs = zs[1:-1]  # Drop first and last which are only used to penalize out-of-data region and spread uncertainty.\n",
    "        print(zs.size())\n",
    "        # select the likelihood function p(x|z)\n",
    "        # generation process\n",
    "        likelihood_constructor = {\"laplace\": distributions.Laplace, \"normal\": distributions.Normal}[args[\"likelihood\"]]\n",
    "        likelihood = likelihood_constructor(loc=zs, scale=args[\"scale\"]) #f(x) = p(x|z)\n",
    "        print(zs.size())\n",
    "        print(ys.size())\n",
    "        \n",
    "        logpy = likelihood.log_prob(ys).sum(dim=0).mean(dim=0) # calculate the log likelihood p(x|z)\n",
    "\n",
    "        loss = -logpy + kl * kl_scheduler.val # we want to maximize the ELBO\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        kl_scheduler.step()\n",
    "\n",
    "        logpy_metric.step(logpy)\n",
    "        kl_metric.step(kl)\n",
    "        loss_metric.step(loss)\n",
    "\n",
    "        logging.info(\n",
    "            f'global_step: {global_step}, '\n",
    "            f'logpy: {logpy_metric.val:.3f}, '\n",
    "            f'kl: {kl_metric.val:.3f}, '\n",
    "            f'loss: {loss_metric.val:.3f}'\n",
    "        )\n",
    "    torch.save(\n",
    "        {'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'kl_scheduler': kl_scheduler},\n",
    "        os.path.join('./', f'global_step_{global_step}.ckpt')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fa3f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved prior figure at: ./img/prior.png\n",
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]INFO:root:Saved figure at: ./img/global_step_0.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 100])\n",
      "torch.Size([20, 100])\n",
      "torch.Size([20, 100])\n",
      "torch.Size([20, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3604/3576362640.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0msdeint_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorchsde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msdeint_adjoint\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"adjoint\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorchsde\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msdeint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3604/542404195.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlogpy\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkl\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkl_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m \u001b[1;31m# we want to maximize the ELBO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "manual_seed(0)\n",
    "\n",
    "if args[\"debug\"]:\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "ckpt_dir = os.path.join('./', 'ckpts')\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "sdeint_fn = torchsde.sdeint_adjoint if args[\"adjoint\"] else torchsde.sdeint\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632f6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bf36e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
