{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "451594fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from typing import Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import distributions, nn, optim\n",
    "\n",
    "import torchsde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "338520f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the gpu is available or not, if yes, use gpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "023f728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"likelihood\": \"laplace\", # specify the likelihood distribution,p(x|z)\n",
    "    \"adjoint\": True, # specify whether to use adjoint sensitivty method to backward the sde\n",
    "    \"debug\": True, # specify whether to debug or not\n",
    "    \"data\": \"segmented_cosine\",# specify which data to use\n",
    "    \"kl_anneal_iters\": 100, # number of the iterations of the annealing kl divergence schedule\n",
    "    \"train_iters\": 500, # number of iterations to train the model\n",
    "    \"batch_size\": 100, \n",
    "    \"adaptive\": False, # whether use adaptive solver or not\n",
    "    \"method\": \"euler\", # the method of sde solver\n",
    "    \"dt\": 1e-2, # the parameter dt of sde solver\n",
    "    \"rtol\": 1e-3, # the parameter rtol of sde solver\n",
    "    \"atol\": 1e-3, # the atol of sde solver\n",
    "    \"scale\": 0.05, # the scale, of the likelihood distribution\n",
    "    \"dpi\": 500, # dpi of images\n",
    "    \"pause_iters\": 50 # the interval to evaluate the model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f9aba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w/ underscore -> numpy; w/o underscore -> torch.\n",
    "'''\n",
    "    ts: original time series; can be segmented or irregular\n",
    "    ts_ext: with extended time outisde the time series, use to generate latent outside to penalize out-of-data region and spread uncertainty\n",
    "    ts_vis: regular time series used to plot the data\n",
    "    ys: the observed dynamic, same size as ts\n",
    "'''\n",
    "\n",
    "Data = namedtuple('Data', ['ts_', 'ts_ext_', 'ts_vis_', 'ts', 'ts_ext', 'ts_vis', 'ys', 'ys_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8fcfe81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScheduler(object):\n",
    "    '''\n",
    "        output a value follows linear schedule from maxval/iters to maxval, with 'iters' steps\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "        iters = 100,\n",
    "        maxval = 1,\n",
    "        1/100 = 0.001, 0.002, .... 1\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, iters, maxval=1.0):\n",
    "        self._iters = max(1, iters)\n",
    "        self._val = maxval / self._iters\n",
    "        self._maxval = maxval\n",
    "\n",
    "    def step(self):\n",
    "        self._val = min(self._maxval, self._val + self._maxval / self._iters)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4b80998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMAMetric(object):\n",
    "    '''\n",
    "        Exponential moving average, used to calculate the average\n",
    "    '''\n",
    "    def __init__(self, gamma: Optional[float] = .99):\n",
    "        super(EMAMetric, self).__init__()\n",
    "        self._val = 0.\n",
    "        self._gamma = gamma\n",
    "\n",
    "    def step(self, x: Union[torch.Tensor, np.ndarray]):\n",
    "        x = x.detach().cpu().numpy() if torch.is_tensor(x) else x\n",
    "        self._val = self._gamma * self._val + (1 - self._gamma) * x\n",
    "        return self._val\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1e1bbc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_seed(seed: int):\n",
    "    '''\n",
    "        set the random seed, make sure the result keeps the same for each call\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f5ae816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stable_division(a, b, epsilon=1e-7): #a/b\n",
    "    '''\n",
    "        change all elements x in b s.t -epsilon < x < epsilon to epsilon, to make sure the division is stable, won't cause a really large number\n",
    "    '''\n",
    "    \n",
    "    b = torch.where(b.abs().detach() > epsilon, b, torch.full_like(b, fill_value=epsilon) * b.sign())\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6930276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentSDE(torchsde.SDEIto):\n",
    "\n",
    "    def __init__(self, theta=1.0, mu=0.0, sigma=0.5):\n",
    "        super(LatentSDE, self).__init__(noise_type=\"diagonal\")\n",
    "        logvar = math.log(sigma ** 2 / (2. * theta)) # calculate the log variance\n",
    "\n",
    "        # Prior drift.\n",
    "        self.register_buffer(\"theta\", torch.tensor([[theta]])) # prior parameters, register 成buffer, 参数不会进行更新\n",
    "        self.register_buffer(\"mu\", torch.tensor([[mu]]))\n",
    "        self.register_buffer(\"sigma\", torch.tensor([[sigma]]))\n",
    "\n",
    "        # p(z0).\n",
    "        self.register_buffer(\"py0_mean\", torch.tensor([[mu]])) # setup the prior distribution\n",
    "        self.register_buffer(\"py0_logvar\", torch.tensor([[logvar]]))\n",
    "\n",
    "        # Approximate posterior drift: Takes in 2 positional encodings and the state. f(t,y)\n",
    "        self.net = nn.Sequential( #h\\Phi()\n",
    "            nn.Linear(3, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialization trick from Glow. \n",
    "        self.net[-1].weight.data.fill_(0.)\n",
    "        self.net[-1].bias.data.fill_(0.)\n",
    "\n",
    "        # q(y0). the initial value of the approx posterior distribution\n",
    "        self.qy0_mean = nn.Parameter(torch.tensor([[mu]]), requires_grad=True) \n",
    "        self.qy0_logvar = nn.Parameter(torch.tensor([[logvar]]), requires_grad=True)\n",
    "\n",
    "    def f(self, t, y):  # Approximate posterior drift. f(t, y) = t # h\\Phi\n",
    "        if t.dim() == 0:\n",
    "            t = torch.full_like(y, fill_value=t)\n",
    "        # Positional encoding in transformers for time-inhomogeneous posterior.\n",
    "        return self.net(torch.cat((torch.sin(t), torch.cos(t), y), dim=-1))\n",
    "\n",
    "    def g(self, t, y):  # Shared diffusion. g(t,y) = sigma\n",
    "        return self.sigma.repeat(y.size(0), 1)\n",
    "\n",
    "    def h(self, t, y):  # Prior drift. h(t,y) = theta * (mu-y) # h\\Theta\n",
    "        return self.theta * (self.mu - y) # need to figure out\n",
    "\n",
    "    def f_aug(self, t, y):  # Drift for augmented dynamics with logqp term.\n",
    "        '''\n",
    "             y has two columns, the first column is y0, the one we want to generate the SDE dynamic\n",
    "             the second column is 0, used to generate the sampling paths from the posterior process, and used to estimate the kl divergence\n",
    "        '''\n",
    "        y = y[:, 0:1] # get the first column of y, that is to get y0 # z0\n",
    "        f, g, h = self.f(t, y), self.g(t, y), self.h(t, y) # calculate f, g, h\n",
    "        u = _stable_division(f - h, g) # u(z,t) = (f-h)/g\n",
    "        f_logqp = .5 * (u ** 2).sum(dim=1, keepdim=True) # (u^2)/2, the drift of the second sde\n",
    "        return torch.cat([f, f_logqp], dim=1) # [batch_size, 2]\n",
    "\n",
    "    def g_aug(self, t, y):  # Diffusion for augmented dynamics with logqp term.\n",
    "        y = y[:, 0:1]\n",
    "        g = self.g(t, y)\n",
    "        g_logqp = torch.zeros_like(y) # the diffusion of the second sde\n",
    "        return torch.cat([g, g_logqp], dim=1) # [batch_size, 2]\n",
    "\n",
    "    def forward(self, ts, batch_size, eps=None):\n",
    "        # recognition process\n",
    "        eps = torch.randn(batch_size, 1).to(self.qy0_std) if eps is None else eps\n",
    "        y0 = self.qy0_mean + eps * self.qy0_std # randomly generate z0 from approx posterior distribution q(z|x)\n",
    "        \n",
    "        \n",
    "        qy0 = distributions.Normal(loc=self.qy0_mean, scale=self.qy0_std) # approx posterior distribution\n",
    "        py0 = distributions.Normal(loc=self.py0_mean, scale=self.py0_std) # prior distribution\n",
    "        \n",
    "        logqp0 = distributions.kl_divergence(qy0, py0).sum(dim=1)  # KL(t=0). # kl divergence when t = 0\n",
    "\n",
    "        aug_y0 = torch.cat([y0, torch.zeros(batch_size, 1).to(y0)], dim=1)\n",
    "        aug_ys = sdeint_fn(\n",
    "            sde=self,\n",
    "            y0=aug_y0,\n",
    "            ts=ts, #[0,0.1,0.2, 0.3]\n",
    "            method=args[\"method\"],\n",
    "            dt=args[\"dt\"],\n",
    "            adaptive=args[\"adaptive\"],\n",
    "            rtol=args[\"rtol\"],\n",
    "            atol=args[\"atol\"],\n",
    "            names={'drift': 'f_aug', 'diffusion': 'g_aug'}\n",
    "        )\n",
    "        #[len(ts),batch_size,2]\n",
    "        ys, logqp_path = aug_ys[:, :, 0:1], aug_ys[-1, :, 1] \n",
    "        # the first column of the last dimension is the sample dynamic\n",
    "        # the second column of the last dimension is the kl divergence\n",
    "        logqp = (logqp0 + logqp_path).mean(dim=0)  # KL(t=0) + KL(path). # calculate the kl divergence\n",
    "        return ys, logqp\n",
    "\n",
    "    def sample_p(self, ts, batch_size, eps=None, bm=None):\n",
    "        '''\n",
    "            latent variable samples from prior distribution p(z), and their SDE dynamics\n",
    "        '''\n",
    "        eps = torch.randn(batch_size, 1).to(self.py0_mean) if eps is None else eps\n",
    "        y0 = self.py0_mean + eps * self.py0_std # [batch_size, 1]: [1024, 1]\n",
    "        \n",
    "        yt = sdeint_fn(self, y0, ts, bm=bm, method='srk', dt=args[\"dt\"], names={'drift': 'h'}) # [len(ts), batch_size, 1]: [300, 1024,1]\n",
    "        return yt\n",
    "\n",
    "    def sample_q(self, ts, batch_size, eps=None, bm=None):\n",
    "        '''\n",
    "            latent variable samples from approx posterior distribution q(z|x), and their SDE dynamics\n",
    "        '''\n",
    "        eps = torch.randn(batch_size, 1).to(self.qy0_mean) if eps is None else eps\n",
    "        y0 = self.qy0_mean + eps * self.qy0_std # [batch_size, 1]: [1024, 1]\n",
    "        return sdeint_fn(self, y0, ts, bm=bm, method='srk', dt=args[\"dt\"]) # [len(ts), batch_size, 1]: [300, 1024, 1]\n",
    "\n",
    "    # self.py_std = \n",
    "    @property # declare it as a property, then we can access it through self.py0_std and specify setter and getter\n",
    "    def py0_std(self): # the standard deviation of the prior distribution p(z)\n",
    "        return torch.exp(.5 * self.py0_logvar)\n",
    "\n",
    "    @property\n",
    "    def qy0_std(self): # the standard deviation of the approx posterior distribution q(z|x)\n",
    "        return torch.exp(.5 * self.qy0_logvar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61fcd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_segmented_cosine_data():\n",
    "    ts_ = np.concatenate((np.linspace(0.3, 0.8, 10), np.linspace(1.2, 1.5, 10)), axis=0) # create segmented time series\n",
    "    ts_ext_ = np.array([0.] + list(ts_) + [2.0]) # add out-of-data time point\n",
    "    ts_vis_ = np.linspace(0., 2.0, 300) # regular time series used for visualization\n",
    "    ys_ = np.cos(ts_ * (2. * math.pi))[:, None] # get the segmented cosine data\n",
    "\n",
    "    ts = torch.tensor(ts_).float()\n",
    "    ts_ext = torch.tensor(ts_ext_).float()\n",
    "    ts_vis = torch.tensor(ts_vis_).float()\n",
    "    ys = torch.tensor(ys_).float().to(device)\n",
    "    return Data(ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_)\n",
    "\n",
    "\n",
    "def make_irregular_sine_data():\n",
    "    ts_ = np.sort(np.random.uniform(low=0.4, high=1.6, size=16)) # create irregular time series\n",
    "    ts_ext_ = np.array([0.] + list(ts_) + [2.0]) # add out-of-data time point\n",
    "    ts_vis_ = np.linspace(0., 2.0, 300) # regular time series used for visualization\n",
    "    ys_ = np.sin(ts_ * (2. * math.pi))[:, None] * 0.8 # get the irregular sine data\n",
    "\n",
    "    ts = torch.tensor(ts_).float()\n",
    "    ts_ext = torch.tensor(ts_ext_).float()\n",
    "    ts_vis = torch.tensor(ts_vis_).float()\n",
    "    ys = torch.tensor(ys_).float().to(device)\n",
    "    return Data(ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_)\n",
    "\n",
    "\n",
    "def make_data():\n",
    "    data_constructor = {\n",
    "        'segmented_cosine': make_segmented_cosine_data,\n",
    "        'irregular_sine': make_irregular_sine_data\n",
    "    }[args[\"data\"]]\n",
    "    return data_constructor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c6809b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Dataset.\n",
    "    ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_ = make_data()\n",
    "\n",
    "    # Plotting parameters.\n",
    "    vis_batch_size = 1024 # the batch_size used to visaulize\n",
    "    ylims = (-1.2, 1.2) # set up the ylim of the figure\n",
    "    \n",
    "    alphas = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55]\n",
    "    percentiles = [0.999, 0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    \n",
    "    vis_idx = np.random.permutation(vis_batch_size) # shuffle the numbers from 1-vis_batch_size\n",
    "    # From https://colorbrewer2.org/.\n",
    "\n",
    "    sample_colors = ('#fc4e2a', '#e31a1c', '#bd0026') \n",
    "    fill_color = '#fd8d3c'\n",
    "    \n",
    "    mean_color = '#800026'\n",
    "    \n",
    "    num_samples = len(sample_colors)\n",
    "    \n",
    "    eps = torch.randn(vis_batch_size, 1).to(device)  # samples from normal distribution\n",
    "    \n",
    "    bm = torchsde.BrownianInterval(\n",
    "        t0=ts_vis[0],\n",
    "        t1=ts_vis[-1],\n",
    "        size=(vis_batch_size, 1),\n",
    "        device=device,\n",
    "        levy_area_approximation='space-time'\n",
    "    )  # We need space-time Levy area to use the SRK solver, fix the brownian motion allows us to generate the sde dynamic fexedly\n",
    "\n",
    "    # Model.\n",
    "    model = LatentSDE().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=.999)\n",
    "    kl_scheduler = LinearScheduler(iters=args[\"kl_anneal_iters\"])\n",
    "\n",
    "    logpy_metric = EMAMetric()\n",
    "    kl_metric = EMAMetric()\n",
    "    loss_metric = EMAMetric()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        zs = model.sample_p(ts=ts_vis, batch_size=vis_batch_size, eps=eps, bm=bm).squeeze() # sde dynamic sampled from prior distribution\n",
    "        ts_vis_, zs_ = ts_vis.cpu().numpy(), zs.cpu().numpy()  # convert them to numpy\n",
    "        # print(zs.size()) # [len(ts_vis), batch_size]: [300, 1024]\n",
    "        zs_ = np.sort(zs_, axis=1) # sort each row\n",
    "\n",
    "        img_dir = os.path.join('./img/' 'prior.png')\n",
    "        plt.subplot(frameon=False)\n",
    "        for alpha, percentile in zip(alphas, percentiles):\n",
    "            idx = int((1 - percentile) / 2. * vis_batch_size) # 选择要考虑百分之多少的数据\n",
    "            zs_bot_ = zs_[:, idx] # 计算底线\n",
    "            zs_top_ = zs_[:, -idx] # 计算顶线\n",
    "            plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color) # 用来填充两条曲线之间的区域\n",
    "\n",
    "        # `zorder` determines who's on top; the larger the more at the top.\n",
    "        plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # scatter plot the original observed dynamic\n",
    "        plt.ylim(ylims) \n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$Y_t$')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(img_dir, dpi=args[\"dpi\"])\n",
    "        plt.close()\n",
    "        logging.info(f'Saved prior figure at: {img_dir}')\n",
    "    \n",
    "    for global_step in tqdm.tqdm(range(args[\"train_iters\"])):\n",
    "        # Plot and save.\n",
    "        \n",
    "        if global_step % args[\"pause_iters\"] == 0:\n",
    "            img_path = os.path.join('./img/', f'global_step_{global_step}.png')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                zs = model.sample_q(ts=ts_vis, batch_size=vis_batch_size, eps=eps, bm=bm).squeeze()\n",
    "                samples = zs[:, vis_idx] # samples是按照之前的permutation来排序的\n",
    "                ts_vis_, zs_, samples_ = ts_vis.cpu().numpy(), zs.cpu().numpy(), samples.cpu().numpy()\n",
    "                zs_ = np.sort(zs_, axis=1)\n",
    "                plt.subplot(frameon=False)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # same as above, plot the percentiles\n",
    "                if True: #args.show_percentiles:\n",
    "                    for alpha, percentile in zip(alphas, percentiles):\n",
    "                        idx = int((1 - percentile) / 2. * vis_batch_size)\n",
    "                        zs_bot_, zs_top_ = zs_[:, idx], zs_[:, -idx]\n",
    "                        plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
    "                \n",
    "                # plot the mean of all the SDE dynamics\n",
    "                if True: #args.show_mean:\n",
    "                    plt.plot(ts_vis_, zs_.mean(axis=1), color=mean_color)\n",
    "                # plot the first three SDE dynamics, since we shuffle the samples already, so the first SDE dynamics are random\n",
    "                if True: #args.show_samples:\n",
    "                    for j in range(num_samples):\n",
    "                        plt.plot(ts_vis_, samples_[:, j], color=sample_colors[j], linewidth=1.0)\n",
    "                \n",
    "                if True: #args.show_arrows:\n",
    "                    num, dt = 12, 0.12\n",
    "                    t, y = torch.meshgrid(\n",
    "                        [torch.linspace(0.2, 1.8, num).to(device), torch.linspace(-1.5, 1.5, num).to(device)]\n",
    "                    )\n",
    "                    #print(t.size()) # [12,12]\n",
    "                    #print(y.size()) # [12, 12]\n",
    "                    t, y = t.reshape(-1, 1), y.reshape(-1, 1)\n",
    "                    '''\n",
    "                        ex:\n",
    "                        t = [[1],[1],[2],[2],[3],[3]]\n",
    "                        y = [[1],[2],[3],[1],[2],[3]]\n",
    "                    '''\n",
    "                    fty = model.f(t=t, y=y).reshape(num, num) # call f(t,y)\n",
    "                    dt = torch.zeros(num, num).fill_(dt).to(device)\n",
    "                    dy = fty * dt # calculate the gradients\n",
    "                    dt_, dy_, t_, y_ = dt.cpu().numpy(), dy.cpu().numpy(), t.cpu().numpy(), y.cpu().numpy()\n",
    "                    plt.quiver(t_, y_, dt_, dy_, alpha=0.3, edgecolors='k', width=0.0035, scale=50) #画箭头，画风场\n",
    "\n",
    "                if False: #args.hide_ticks:\n",
    "                    plt.xticks([], [])\n",
    "                    plt.yticks([], [])\n",
    "\n",
    "                plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # scatter plot the Data.\n",
    "                \n",
    "                \n",
    "                \n",
    "                zs = model.sample_q(ts=ts, batch_size=args[\"batch_size\"]).squeeze()\n",
    "                likelihood_constructor = {\"laplace\": distributions.Laplace, \"normal\": distributions.Normal}[args[\"likelihood\"]]\n",
    "                likelihood = likelihood_constructor(loc=zs, scale=args[\"scale\"]) #f(x) = p(x|z)\n",
    "        \n",
    "                samples = likelihood.sample()\n",
    "                samples = samples.mean(axis=1)\n",
    "                plt.plot(ts_,samples)\n",
    "                \n",
    "                plt.ylim(ylims)\n",
    "                plt.xlabel('$t$')\n",
    "                plt.ylabel('$Y_t$')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(img_path, dpi=args[\"dpi\"])\n",
    "                plt.close()\n",
    "                logging.info(f'Saved figure at: {img_path}')\n",
    "    \n",
    "        \n",
    "        # Train.\n",
    "        optimizer.zero_grad()\n",
    "        zs, kl = model(ts=ts_ext, batch_size=args[\"batch_size\"]) \n",
    "        # print(zs.size()) # [len(ts_ext),batch_size,1]: [22,100,1] \n",
    "        zs = zs.squeeze() # [len(ts_ext), batch_size]: [22, 100]\n",
    "        zs = zs[1:-1]  # Drop first and last which are only used to penalize out-of-data region and spread uncertainty.\n",
    "\n",
    "        # select the likelihood function p(x|z)\n",
    "        # generation process\n",
    "        likelihood_constructor = {\"laplace\": distributions.Laplace, \"normal\": distributions.Normal}[args[\"likelihood\"]]\n",
    "        likelihood = likelihood_constructor(loc=zs, scale=args[\"scale\"]) #f(x) = p(x|z)\n",
    "        \n",
    "        logpy = likelihood.log_prob(ys).sum(dim=0).mean(dim=0) # calculate the log likelihood p(x|z)\n",
    "\n",
    "        loss = -logpy + kl * kl_scheduler.val # we want to maximize the ELBO\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        kl_scheduler.step()\n",
    "\n",
    "        logpy_metric.step(logpy)\n",
    "        kl_metric.step(kl)\n",
    "        loss_metric.step(loss)\n",
    "\n",
    "        logging.info(\n",
    "            f'global_step: {global_step}, '\n",
    "            f'logpy: {logpy_metric.val:.3f}, '\n",
    "            f'kl: {kl_metric.val:.3f}, '\n",
    "            f'loss: {loss_metric.val:.3f}'\n",
    "        )\n",
    "    torch.save(\n",
    "        {'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'kl_scheduler': kl_scheduler},\n",
    "        os.path.join('./', f'global_step_{global_step}.ckpt')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0fa3f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved prior figure at: ./img/prior.png\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]INFO:root:Saved figure at: ./img/global_step_0.png\n",
      "INFO:root:global_step: 0, logpy: -2.300, kl: 0.013, loss: 2.300\n",
      "  0%|          | 1/500 [00:05<47:25,  5.70s/it]INFO:root:global_step: 1, logpy: -4.199, kl: 0.038, loss: 4.199\n",
      "  0%|          | 2/500 [00:06<35:27,  4.27s/it]INFO:root:global_step: 2, logpy: -7.072, kl: 0.197, loss: 7.077\n",
      "  1%|          | 3/500 [00:07<27:08,  3.28s/it]INFO:root:global_step: 3, logpy: -8.258, kl: 0.222, loss: 8.264\n",
      "  1%|          | 4/500 [00:08<21:19,  2.58s/it]INFO:root:global_step: 4, logpy: -9.639, kl: 0.231, loss: 9.645\n",
      "  1%|          | 5/500 [00:09<17:32,  2.13s/it]INFO:root:global_step: 5, logpy: -11.065, kl: 0.242, loss: 11.073\n",
      "  1%|          | 6/500 [00:10<14:37,  1.78s/it]INFO:root:global_step: 6, logpy: -12.372, kl: 0.263, loss: 12.381\n",
      "  1%|▏         | 7/500 [00:11<12:36,  1.53s/it]INFO:root:global_step: 7, logpy: -13.485, kl: 0.291, loss: 13.496\n",
      "  2%|▏         | 8/500 [00:12<11:13,  1.37s/it]INFO:root:global_step: 8, logpy: -14.452, kl: 0.336, loss: 14.468\n",
      "  2%|▏         | 9/500 [00:13<10:16,  1.26s/it]INFO:root:global_step: 9, logpy: -15.368, kl: 0.397, loss: 15.390\n",
      "  2%|▏         | 10/500 [00:14<09:29,  1.16s/it]INFO:root:global_step: 10, logpy: -16.276, kl: 0.473, loss: 16.306\n",
      "  2%|▏         | 11/500 [00:15<09:02,  1.11s/it]INFO:root:global_step: 11, logpy: -17.233, kl: 0.567, loss: 17.276\n",
      "  2%|▏         | 12/500 [00:16<08:44,  1.07s/it]INFO:root:global_step: 12, logpy: -18.194, kl: 0.672, loss: 18.251\n",
      "  3%|▎         | 13/500 [00:17<08:21,  1.03s/it]INFO:root:global_step: 13, logpy: -19.119, kl: 0.780, loss: 19.191\n",
      "  3%|▎         | 14/500 [00:18<08:05,  1.00it/s]INFO:root:global_step: 14, logpy: -19.992, kl: 0.898, loss: 20.082\n",
      "  3%|▎         | 15/500 [00:19<07:54,  1.02it/s]INFO:root:global_step: 15, logpy: -20.840, kl: 1.021, loss: 20.950\n",
      "  3%|▎         | 16/500 [00:20<07:46,  1.04it/s]INFO:root:global_step: 16, logpy: -21.665, kl: 1.151, loss: 21.798\n",
      "  3%|▎         | 17/500 [00:21<07:41,  1.05it/s]INFO:root:global_step: 17, logpy: -22.492, kl: 1.276, loss: 22.648\n",
      "  4%|▎         | 18/500 [00:22<07:35,  1.06it/s]INFO:root:global_step: 18, logpy: -23.296, kl: 1.405, loss: 23.478\n",
      "  4%|▍         | 19/500 [00:22<07:40,  1.04it/s]INFO:root:global_step: 19, logpy: -24.081, kl: 1.539, loss: 24.290\n",
      "  4%|▍         | 20/500 [00:23<07:34,  1.06it/s]INFO:root:global_step: 20, logpy: -24.887, kl: 1.679, loss: 25.126\n",
      "  4%|▍         | 21/500 [00:24<07:30,  1.06it/s]INFO:root:global_step: 21, logpy: -25.690, kl: 1.818, loss: 25.961\n",
      "  4%|▍         | 22/500 [00:25<07:27,  1.07it/s]INFO:root:global_step: 22, logpy: -26.483, kl: 1.963, loss: 26.789\n",
      "  5%|▍         | 23/500 [00:26<07:25,  1.07it/s]INFO:root:global_step: 23, logpy: -27.244, kl: 2.108, loss: 27.587\n",
      "  5%|▍         | 24/500 [00:27<07:23,  1.07it/s]INFO:root:global_step: 24, logpy: -28.000, kl: 2.259, loss: 28.382\n",
      "  5%|▌         | 25/500 [00:28<07:24,  1.07it/s]INFO:root:global_step: 25, logpy: -28.746, kl: 2.408, loss: 29.169\n",
      "  5%|▌         | 26/500 [00:29<07:30,  1.05it/s]INFO:root:global_step: 26, logpy: -29.484, kl: 2.552, loss: 29.948\n",
      "  5%|▌         | 27/500 [00:30<07:26,  1.06it/s]INFO:root:global_step: 27, logpy: -30.200, kl: 2.694, loss: 30.707\n",
      "  6%|▌         | 28/500 [00:31<07:23,  1.06it/s]INFO:root:global_step: 28, logpy: -30.904, kl: 2.836, loss: 31.454\n",
      "  6%|▌         | 29/500 [00:32<07:21,  1.07it/s]INFO:root:global_step: 29, logpy: -31.600, kl: 2.984, loss: 32.198\n",
      "  6%|▌         | 30/500 [00:33<07:19,  1.07it/s]INFO:root:global_step: 30, logpy: -32.293, kl: 3.112, loss: 32.933\n",
      "  6%|▌         | 31/500 [00:34<07:18,  1.07it/s]INFO:root:global_step: 31, logpy: -32.995, kl: 3.246, loss: 33.683\n",
      "  6%|▋         | 32/500 [00:35<07:17,  1.07it/s]INFO:root:global_step: 32, logpy: -33.684, kl: 3.372, loss: 34.416\n",
      "  7%|▋         | 33/500 [00:36<07:15,  1.07it/s]INFO:root:global_step: 33, logpy: -34.384, kl: 3.489, loss: 35.160\n",
      "  7%|▋         | 34/500 [00:37<07:22,  1.05it/s]INFO:root:global_step: 34, logpy: -35.056, kl: 3.595, loss: 35.874\n",
      "  7%|▋         | 35/500 [00:37<07:18,  1.06it/s]INFO:root:global_step: 35, logpy: -35.703, kl: 3.704, loss: 36.565\n",
      "  7%|▋         | 36/500 [00:38<07:14,  1.07it/s]INFO:root:global_step: 36, logpy: -36.343, kl: 3.801, loss: 37.246\n",
      "  7%|▋         | 37/500 [00:39<07:12,  1.07it/s]INFO:root:global_step: 37, logpy: -36.994, kl: 3.884, loss: 37.934\n",
      "  8%|▊         | 38/500 [00:40<07:09,  1.08it/s]INFO:root:global_step: 38, logpy: -37.654, kl: 3.968, loss: 38.633\n",
      "  8%|▊         | 39/500 [00:41<07:09,  1.07it/s]INFO:root:global_step: 39, logpy: -38.333, kl: 4.040, loss: 39.346\n",
      "  8%|▊         | 40/500 [00:42<07:07,  1.08it/s]INFO:root:global_step: 40, logpy: -38.966, kl: 4.107, loss: 40.013\n",
      "  8%|▊         | 41/500 [00:43<07:06,  1.08it/s]INFO:root:global_step: 41, logpy: -39.603, kl: 4.173, loss: 40.685\n",
      "  8%|▊         | 42/500 [00:44<07:12,  1.06it/s]INFO:root:global_step: 42, logpy: -40.235, kl: 4.228, loss: 41.347\n",
      "  9%|▊         | 43/500 [00:45<07:09,  1.06it/s]INFO:root:global_step: 43, logpy: -40.843, kl: 4.275, loss: 41.984\n",
      "  9%|▉         | 44/500 [00:46<07:06,  1.07it/s]INFO:root:global_step: 44, logpy: -41.452, kl: 4.321, loss: 42.621\n",
      "  9%|▉         | 45/500 [00:47<07:04,  1.07it/s]INFO:root:global_step: 45, logpy: -42.063, kl: 4.363, loss: 43.260\n",
      "  9%|▉         | 46/500 [00:48<07:02,  1.07it/s]INFO:root:global_step: 46, logpy: -42.658, kl: 4.404, loss: 43.883\n",
      "  9%|▉         | 47/500 [00:49<07:00,  1.08it/s]INFO:root:global_step: 47, logpy: -43.250, kl: 4.445, loss: 44.503\n",
      " 10%|▉         | 48/500 [00:50<06:59,  1.08it/s]INFO:root:global_step: 48, logpy: -43.865, kl: 4.485, loss: 45.147\n",
      " 10%|▉         | 49/500 [00:51<07:07,  1.06it/s]INFO:root:global_step: 49, logpy: -44.437, kl: 4.522, loss: 45.747\n",
      " 10%|█         | 50/500 [00:52<07:04,  1.06it/s]INFO:root:Saved figure at: ./img/global_step_50.png\n",
      "INFO:root:global_step: 50, logpy: -45.006, kl: 4.566, loss: 46.349\n",
      " 10%|█         | 51/500 [00:57<16:54,  2.26s/it]INFO:root:global_step: 51, logpy: -45.577, kl: 4.606, loss: 46.951\n",
      " 10%|█         | 52/500 [00:58<14:00,  1.88s/it]INFO:root:global_step: 52, logpy: -46.137, kl: 4.651, loss: 47.546\n",
      " 11%|█         | 53/500 [00:59<11:58,  1.61s/it]INFO:root:global_step: 53, logpy: -46.698, kl: 4.703, loss: 48.145\n",
      " 11%|█         | 54/500 [01:00<10:32,  1.42s/it]INFO:root:global_step: 54, logpy: -47.249, kl: 4.752, loss: 48.735\n",
      " 11%|█         | 55/500 [01:01<09:28,  1.28s/it]INFO:root:global_step: 55, logpy: -47.776, kl: 4.809, loss: 49.305\n",
      " 11%|█         | 56/500 [01:02<08:51,  1.20s/it]INFO:root:global_step: 56, logpy: -48.287, kl: 4.858, loss: 49.857\n",
      " 11%|█▏        | 57/500 [01:03<08:18,  1.13s/it]INFO:root:global_step: 57, logpy: -48.798, kl: 4.909, loss: 50.410\n",
      " 12%|█▏        | 58/500 [01:04<07:58,  1.08s/it]INFO:root:global_step: 58, logpy: -49.293, kl: 4.958, loss: 50.946\n",
      " 12%|█▏        | 59/500 [01:05<07:51,  1.07s/it]INFO:root:global_step: 59, logpy: -49.817, kl: 5.007, loss: 51.513\n",
      " 12%|█▏        | 60/500 [01:06<07:42,  1.05s/it]INFO:root:global_step: 60, logpy: -50.313, kl: 5.058, loss: 52.054\n",
      " 12%|█▏        | 61/500 [01:07<07:33,  1.03s/it]INFO:root:global_step: 61, logpy: -50.817, kl: 5.106, loss: 52.602\n",
      " 12%|█▏        | 62/500 [01:08<07:31,  1.03s/it]INFO:root:global_step: 62, logpy: -51.329, kl: 5.152, loss: 53.157\n",
      " 13%|█▎        | 63/500 [01:09<07:55,  1.09s/it]INFO:root:global_step: 63, logpy: -51.834, kl: 5.198, loss: 53.706\n",
      " 13%|█▎        | 64/500 [01:10<07:34,  1.04s/it]INFO:root:global_step: 64, logpy: -52.307, kl: 5.243, loss: 54.223\n",
      " 13%|█▎        | 65/500 [01:11<07:19,  1.01s/it]INFO:root:global_step: 65, logpy: -52.758, kl: 5.280, loss: 54.714\n",
      " 13%|█▎        | 66/500 [01:12<07:09,  1.01it/s]INFO:root:global_step: 66, logpy: -53.215, kl: 5.321, loss: 55.214\n",
      " 13%|█▎        | 67/500 [01:13<07:01,  1.03it/s]INFO:root:global_step: 67, logpy: -53.688, kl: 5.361, loss: 55.731\n",
      " 14%|█▎        | 68/500 [01:14<06:58,  1.03it/s]INFO:root:global_step: 68, logpy: -54.141, kl: 5.391, loss: 56.220\n",
      " 14%|█▍        | 69/500 [01:15<06:54,  1.04it/s]INFO:root:global_step: 69, logpy: -54.620, kl: 5.429, loss: 56.744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 70/500 [01:16<06:50,  1.05it/s]INFO:root:global_step: 70, logpy: -55.061, kl: 5.462, loss: 57.225\n",
      " 14%|█▍        | 71/500 [01:17<06:54,  1.03it/s]INFO:root:global_step: 71, logpy: -55.485, kl: 5.498, loss: 57.693\n",
      " 14%|█▍        | 72/500 [01:17<06:51,  1.04it/s]INFO:root:global_step: 72, logpy: -55.920, kl: 5.532, loss: 58.171\n",
      " 15%|█▍        | 73/500 [01:18<06:47,  1.05it/s]INFO:root:global_step: 73, logpy: -56.356, kl: 5.561, loss: 58.646\n",
      " 15%|█▍        | 74/500 [01:19<06:43,  1.06it/s]INFO:root:global_step: 74, logpy: -56.780, kl: 5.589, loss: 59.110\n",
      " 15%|█▌        | 75/500 [01:20<06:48,  1.04it/s]INFO:root:global_step: 75, logpy: -57.187, kl: 5.619, loss: 59.559\n",
      " 15%|█▌        | 76/500 [01:21<06:55,  1.02it/s]INFO:root:global_step: 76, logpy: -57.596, kl: 5.654, loss: 60.014\n",
      " 15%|█▌        | 77/500 [01:22<06:56,  1.01it/s]INFO:root:global_step: 77, logpy: -57.997, kl: 5.684, loss: 60.459\n",
      " 16%|█▌        | 78/500 [01:23<06:52,  1.02it/s]INFO:root:global_step: 78, logpy: -58.386, kl: 5.713, loss: 60.891\n",
      " 16%|█▌        | 79/500 [01:24<06:56,  1.01it/s]INFO:root:global_step: 79, logpy: -58.770, kl: 5.747, loss: 61.323\n",
      " 16%|█▌        | 80/500 [01:25<06:57,  1.01it/s]INFO:root:global_step: 80, logpy: -59.124, kl: 5.783, loss: 61.727\n",
      " 16%|█▌        | 81/500 [01:26<06:51,  1.02it/s]INFO:root:global_step: 81, logpy: -59.501, kl: 5.810, loss: 62.148\n",
      " 16%|█▋        | 82/500 [01:27<06:45,  1.03it/s]INFO:root:global_step: 82, logpy: -59.853, kl: 5.839, loss: 62.545\n",
      " 17%|█▋        | 83/500 [01:28<06:42,  1.04it/s]INFO:root:global_step: 83, logpy: -60.211, kl: 5.872, loss: 62.954\n",
      " 17%|█▋        | 84/500 [01:29<06:38,  1.04it/s]INFO:root:global_step: 84, logpy: -60.565, kl: 5.901, loss: 63.355\n",
      " 17%|█▋        | 85/500 [01:30<06:34,  1.05it/s]INFO:root:global_step: 85, logpy: -60.915, kl: 5.941, loss: 63.762\n",
      " 17%|█▋        | 86/500 [01:31<06:37,  1.04it/s]INFO:root:global_step: 86, logpy: -61.233, kl: 5.980, loss: 64.138\n",
      " 17%|█▋        | 87/500 [01:32<06:32,  1.05it/s]INFO:root:global_step: 87, logpy: -61.546, kl: 6.017, loss: 64.507\n",
      " 18%|█▊        | 88/500 [01:33<06:29,  1.06it/s]INFO:root:global_step: 88, logpy: -61.830, kl: 6.056, loss: 64.848\n",
      " 18%|█▊        | 89/500 [01:34<06:25,  1.07it/s]INFO:root:global_step: 89, logpy: -62.105, kl: 6.092, loss: 65.180\n",
      " 18%|█▊        | 90/500 [01:35<06:23,  1.07it/s]INFO:root:global_step: 90, logpy: -62.373, kl: 6.130, loss: 65.507\n",
      " 18%|█▊        | 91/500 [01:36<06:22,  1.07it/s]INFO:root:global_step: 91, logpy: -62.644, kl: 6.166, loss: 65.836\n",
      " 18%|█▊        | 92/500 [01:37<06:21,  1.07it/s]INFO:root:global_step: 92, logpy: -62.887, kl: 6.198, loss: 66.136\n",
      " 19%|█▊        | 93/500 [01:38<06:20,  1.07it/s]INFO:root:global_step: 93, logpy: -63.121, kl: 6.236, loss: 66.430\n",
      " 19%|█▉        | 94/500 [01:39<06:25,  1.05it/s]INFO:root:global_step: 94, logpy: -63.331, kl: 6.278, loss: 66.707\n",
      " 19%|█▉        | 95/500 [01:39<06:21,  1.06it/s]INFO:root:global_step: 95, logpy: -63.530, kl: 6.322, loss: 66.974\n",
      " 19%|█▉        | 96/500 [01:40<06:19,  1.06it/s]INFO:root:global_step: 96, logpy: -63.720, kl: 6.367, loss: 67.235\n",
      " 19%|█▉        | 97/500 [01:41<06:16,  1.07it/s]INFO:root:global_step: 97, logpy: -63.879, kl: 6.418, loss: 67.472\n",
      " 20%|█▉        | 98/500 [01:42<06:15,  1.07it/s]INFO:root:global_step: 98, logpy: -64.014, kl: 6.477, loss: 67.692\n",
      " 20%|█▉        | 99/500 [01:43<06:14,  1.07it/s]INFO:root:global_step: 99, logpy: -64.163, kl: 6.545, loss: 67.937\n",
      " 20%|██        | 100/500 [01:44<06:13,  1.07it/s]INFO:root:Saved figure at: ./img/global_step_100.png\n",
      "INFO:root:global_step: 100, logpy: -64.293, kl: 6.613, loss: 68.163\n",
      " 20%|██        | 101/500 [01:50<15:06,  2.27s/it]INFO:root:global_step: 101, logpy: -64.395, kl: 6.694, loss: 68.374\n",
      " 20%|██        | 102/500 [01:51<12:38,  1.91s/it]INFO:root:global_step: 102, logpy: -64.488, kl: 6.775, loss: 68.574\n",
      " 21%|██        | 103/500 [01:52<10:46,  1.63s/it]INFO:root:global_step: 103, logpy: -64.540, kl: 6.858, loss: 68.737\n",
      " 21%|██        | 104/500 [01:53<09:26,  1.43s/it]INFO:root:global_step: 104, logpy: -64.573, kl: 6.955, loss: 68.893\n",
      " 21%|██        | 105/500 [01:53<08:27,  1.29s/it]INFO:root:global_step: 105, logpy: -64.593, kl: 7.044, loss: 69.029\n",
      " 21%|██        | 106/500 [01:54<07:51,  1.20s/it]INFO:root:global_step: 106, logpy: -64.591, kl: 7.141, loss: 69.150\n",
      " 21%|██▏       | 107/500 [01:55<07:23,  1.13s/it]INFO:root:global_step: 107, logpy: -64.544, kl: 7.235, loss: 69.222\n",
      " 22%|██▏       | 108/500 [01:56<07:08,  1.09s/it]INFO:root:global_step: 108, logpy: -64.503, kl: 7.339, loss: 69.312\n",
      " 22%|██▏       | 109/500 [01:57<06:53,  1.06s/it]INFO:root:global_step: 109, logpy: -64.442, kl: 7.440, loss: 69.376\n",
      " 22%|██▏       | 110/500 [01:58<06:39,  1.03s/it]INFO:root:global_step: 110, logpy: -64.381, kl: 7.556, loss: 69.456\n",
      " 22%|██▏       | 111/500 [01:59<06:27,  1.00it/s]INFO:root:global_step: 111, logpy: -64.295, kl: 7.680, loss: 69.520\n",
      " 22%|██▏       | 112/500 [02:00<06:18,  1.02it/s]INFO:root:global_step: 112, logpy: -64.153, kl: 7.811, loss: 69.533\n",
      " 23%|██▎       | 113/500 [02:01<06:15,  1.03it/s]INFO:root:global_step: 113, logpy: -64.013, kl: 7.943, loss: 69.549\n",
      " 23%|██▎       | 114/500 [02:02<06:17,  1.02it/s]INFO:root:global_step: 114, logpy: -63.860, kl: 8.081, loss: 69.558\n",
      " 23%|██▎       | 115/500 [02:03<06:11,  1.04it/s]INFO:root:global_step: 115, logpy: -63.665, kl: 8.210, loss: 69.515\n",
      " 23%|██▎       | 116/500 [02:04<06:13,  1.03it/s]INFO:root:global_step: 116, logpy: -63.478, kl: 8.333, loss: 69.476\n",
      " 23%|██▎       | 117/500 [02:05<06:07,  1.04it/s]INFO:root:global_step: 117, logpy: -63.301, kl: 8.470, loss: 69.458\n",
      " 24%|██▎       | 118/500 [02:06<06:02,  1.05it/s]INFO:root:global_step: 118, logpy: -63.074, kl: 8.608, loss: 69.394\n",
      " 24%|██▍       | 119/500 [02:07<05:59,  1.06it/s]INFO:root:global_step: 119, logpy: -62.885, kl: 8.735, loss: 69.354\n",
      " 24%|██▍       | 120/500 [02:08<05:57,  1.06it/s]INFO:root:global_step: 120, logpy: -62.649, kl: 8.875, loss: 69.281\n",
      " 24%|██▍       | 121/500 [02:09<05:54,  1.07it/s]INFO:root:global_step: 121, logpy: -62.384, kl: 9.022, loss: 69.185\n",
      " 24%|██▍       | 122/500 [02:10<05:53,  1.07it/s]INFO:root:global_step: 122, logpy: -62.105, kl: 9.175, loss: 69.080\n",
      " 25%|██▍       | 123/500 [02:11<05:52,  1.07it/s]INFO:root:global_step: 123, logpy: -61.819, kl: 9.315, loss: 68.957\n",
      " 25%|██▍       | 124/500 [02:12<05:58,  1.05it/s]INFO:root:global_step: 124, logpy: -61.546, kl: 9.454, loss: 68.845\n",
      " 25%|██▌       | 125/500 [02:13<05:54,  1.06it/s]INFO:root:global_step: 125, logpy: -61.262, kl: 9.605, loss: 68.734\n",
      " 25%|██▌       | 126/500 [02:13<05:51,  1.06it/s]INFO:root:global_step: 126, logpy: -60.989, kl: 9.758, loss: 68.635\n",
      " 25%|██▌       | 127/500 [02:14<05:48,  1.07it/s]INFO:root:global_step: 127, logpy: -60.688, kl: 9.893, loss: 68.490\n",
      " 26%|██▌       | 128/500 [02:15<05:47,  1.07it/s]INFO:root:global_step: 128, logpy: -60.385, kl: 10.029, loss: 68.343\n",
      " 26%|██▌       | 129/500 [02:16<05:48,  1.07it/s]INFO:root:global_step: 129, logpy: -60.104, kl: 10.145, loss: 68.199\n",
      " 26%|██▌       | 130/500 [02:17<05:46,  1.07it/s]INFO:root:global_step: 130, logpy: -59.798, kl: 10.278, loss: 68.048\n",
      " 26%|██▌       | 131/500 [02:18<05:44,  1.07it/s]INFO:root:global_step: 131, logpy: -59.492, kl: 10.409, loss: 67.893\n",
      " 26%|██▋       | 132/500 [02:19<05:50,  1.05it/s]INFO:root:global_step: 132, logpy: -59.135, kl: 10.550, loss: 67.696\n",
      " 27%|██▋       | 133/500 [02:20<05:46,  1.06it/s]INFO:root:global_step: 133, logpy: -58.815, kl: 10.702, loss: 67.549\n",
      " 27%|██▋       | 134/500 [02:21<05:43,  1.07it/s]INFO:root:global_step: 134, logpy: -58.467, kl: 10.855, loss: 67.372\n",
      " 27%|██▋       | 135/500 [02:22<05:41,  1.07it/s]INFO:root:global_step: 135, logpy: -58.093, kl: 11.009, loss: 67.172\n",
      " 27%|██▋       | 136/500 [02:23<05:40,  1.07it/s]INFO:root:global_step: 136, logpy: -57.739, kl: 11.154, loss: 66.982\n",
      " 27%|██▋       | 137/500 [02:24<05:38,  1.07it/s]INFO:root:global_step: 137, logpy: -57.385, kl: 11.300, loss: 66.793\n",
      " 28%|██▊       | 138/500 [02:25<05:37,  1.07it/s]INFO:root:global_step: 138, logpy: -57.012, kl: 11.433, loss: 66.572\n",
      " 28%|██▊       | 139/500 [02:26<05:36,  1.07it/s]INFO:root:global_step: 139, logpy: -56.660, kl: 11.574, loss: 66.381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 140/500 [02:27<05:42,  1.05it/s]INFO:root:global_step: 140, logpy: -56.287, kl: 11.704, loss: 66.156\n",
      " 28%|██▊       | 141/500 [02:28<05:39,  1.06it/s]INFO:root:global_step: 141, logpy: -55.925, kl: 11.834, loss: 65.943\n",
      " 28%|██▊       | 142/500 [02:29<05:37,  1.06it/s]INFO:root:global_step: 142, logpy: -55.579, kl: 11.976, loss: 65.757\n",
      " 29%|██▊       | 143/500 [02:29<05:35,  1.06it/s]INFO:root:global_step: 143, logpy: -55.213, kl: 12.117, loss: 65.550\n",
      " 29%|██▉       | 144/500 [02:30<05:33,  1.07it/s]INFO:root:global_step: 144, logpy: -54.836, kl: 12.249, loss: 65.322\n",
      " 29%|██▉       | 145/500 [02:31<05:31,  1.07it/s]INFO:root:global_step: 145, logpy: -54.487, kl: 12.381, loss: 65.123\n",
      " 29%|██▉       | 146/500 [02:32<05:29,  1.07it/s]INFO:root:global_step: 146, logpy: -54.111, kl: 12.524, loss: 64.907\n",
      " 29%|██▉       | 147/500 [02:33<05:35,  1.05it/s]INFO:root:global_step: 147, logpy: -53.728, kl: 12.666, loss: 64.684\n",
      " 30%|██▉       | 148/500 [02:34<05:31,  1.06it/s]INFO:root:global_step: 148, logpy: -53.372, kl: 12.804, loss: 64.483\n",
      " 30%|██▉       | 149/500 [02:35<05:28,  1.07it/s]INFO:root:global_step: 149, logpy: -52.986, kl: 12.937, loss: 64.247\n",
      " 30%|███       | 150/500 [02:36<05:27,  1.07it/s]INFO:root:Saved figure at: ./img/global_step_150.png\n",
      "INFO:root:global_step: 150, logpy: -52.591, kl: 13.062, loss: 63.993\n",
      " 30%|███       | 151/500 [02:41<13:07,  2.26s/it]INFO:root:global_step: 151, logpy: -52.208, kl: 13.198, loss: 63.763\n",
      " 30%|███       | 152/500 [02:42<10:52,  1.87s/it]INFO:root:global_step: 152, logpy: -51.823, kl: 13.334, loss: 63.530\n",
      " 31%|███       | 153/500 [02:43<09:16,  1.60s/it]INFO:root:global_step: 153, logpy: -51.451, kl: 13.459, loss: 63.300\n",
      " 31%|███       | 154/500 [02:44<08:06,  1.40s/it]INFO:root:global_step: 154, logpy: -51.064, kl: 13.596, loss: 63.066\n",
      " 31%|███       | 155/500 [02:45<07:26,  1.29s/it]INFO:root:global_step: 155, logpy: -50.667, kl: 13.722, loss: 62.811\n",
      " 31%|███       | 156/500 [02:46<06:52,  1.20s/it]INFO:root:global_step: 156, logpy: -50.291, kl: 13.855, loss: 62.584\n",
      " 31%|███▏      | 157/500 [02:47<06:26,  1.13s/it]INFO:root:global_step: 157, logpy: -49.902, kl: 13.990, loss: 62.345\n",
      " 32%|███▏      | 158/500 [02:48<06:10,  1.08s/it]INFO:root:global_step: 158, logpy: -49.503, kl: 14.112, loss: 62.083\n",
      " 32%|███▏      | 159/500 [02:49<05:56,  1.05s/it]INFO:root:global_step: 159, logpy: -49.141, kl: 14.242, loss: 61.866\n",
      " 32%|███▏      | 160/500 [02:50<05:44,  1.01s/it]INFO:root:global_step: 160, logpy: -48.788, kl: 14.372, loss: 61.659\n",
      " 32%|███▏      | 161/500 [02:51<05:36,  1.01it/s]INFO:root:global_step: 161, logpy: -48.408, kl: 14.492, loss: 61.414\n",
      " 32%|███▏      | 162/500 [02:52<05:35,  1.01it/s]INFO:root:global_step: 162, logpy: -48.037, kl: 14.618, loss: 61.184\n",
      " 33%|███▎      | 163/500 [02:53<05:28,  1.03it/s]INFO:root:global_step: 163, logpy: -47.655, kl: 14.743, loss: 60.941\n",
      " 33%|███▎      | 164/500 [02:54<05:22,  1.04it/s]INFO:root:global_step: 164, logpy: -47.283, kl: 14.862, loss: 60.704\n",
      " 33%|███▎      | 165/500 [02:55<05:18,  1.05it/s]INFO:root:global_step: 165, logpy: -46.899, kl: 14.981, loss: 60.452\n",
      " 33%|███▎      | 166/500 [02:56<05:15,  1.06it/s]INFO:root:global_step: 166, logpy: -46.535, kl: 15.112, loss: 60.234\n",
      " 33%|███▎      | 167/500 [02:57<05:13,  1.06it/s]INFO:root:global_step: 167, logpy: -46.147, kl: 15.237, loss: 59.985\n",
      " 34%|███▎      | 168/500 [02:58<05:11,  1.07it/s]INFO:root:global_step: 168, logpy: -45.744, kl: 15.363, loss: 59.723\n",
      " 34%|███▍      | 169/500 [02:59<05:09,  1.07it/s]INFO:root:global_step: 169, logpy: -45.348, kl: 15.484, loss: 59.461\n",
      " 34%|███▍      | 170/500 [03:00<05:13,  1.05it/s]INFO:root:global_step: 170, logpy: -44.958, kl: 15.612, loss: 59.212\n",
      " 34%|███▍      | 171/500 [03:00<05:10,  1.06it/s]INFO:root:global_step: 171, logpy: -44.583, kl: 15.728, loss: 58.967\n",
      " 34%|███▍      | 172/500 [03:01<05:07,  1.07it/s]INFO:root:global_step: 172, logpy: -44.203, kl: 15.840, loss: 58.712\n",
      " 35%|███▍      | 173/500 [03:02<05:05,  1.07it/s]INFO:root:global_step: 173, logpy: -43.832, kl: 15.962, loss: 58.478\n",
      " 35%|███▍      | 174/500 [03:03<05:03,  1.07it/s]INFO:root:global_step: 174, logpy: -43.462, kl: 16.078, loss: 58.237\n",
      " 35%|███▌      | 175/500 [03:04<05:04,  1.07it/s]INFO:root:global_step: 175, logpy: -43.084, kl: 16.206, loss: 57.999\n",
      " 35%|███▌      | 176/500 [03:05<05:05,  1.06it/s]INFO:root:global_step: 176, logpy: -42.705, kl: 16.318, loss: 57.746\n",
      " 35%|███▌      | 177/500 [03:06<05:08,  1.05it/s]INFO:root:global_step: 177, logpy: -42.326, kl: 16.443, loss: 57.504\n",
      " 36%|███▌      | 178/500 [03:07<05:17,  1.01it/s]INFO:root:global_step: 178, logpy: -41.942, kl: 16.571, loss: 57.261\n",
      " 36%|███▌      | 179/500 [03:08<05:14,  1.02it/s]INFO:root:global_step: 179, logpy: -41.573, kl: 16.688, loss: 57.021\n",
      " 36%|███▌      | 180/500 [03:09<05:11,  1.03it/s]INFO:root:global_step: 180, logpy: -41.220, kl: 16.809, loss: 56.801\n",
      " 36%|███▌      | 181/500 [03:10<05:09,  1.03it/s]INFO:root:global_step: 181, logpy: -40.840, kl: 16.917, loss: 56.542\n",
      " 36%|███▋      | 182/500 [03:11<05:04,  1.04it/s]INFO:root:global_step: 182, logpy: -40.468, kl: 17.016, loss: 56.281\n",
      " 37%|███▋      | 183/500 [03:12<05:01,  1.05it/s]INFO:root:global_step: 183, logpy: -40.105, kl: 17.131, loss: 56.045\n",
      " 37%|███▋      | 184/500 [03:13<04:58,  1.06it/s]INFO:root:global_step: 184, logpy: -39.739, kl: 17.247, loss: 55.807\n",
      " 37%|███▋      | 185/500 [03:14<05:05,  1.03it/s]INFO:root:global_step: 185, logpy: -39.396, kl: 17.366, loss: 55.595\n",
      " 37%|███▋      | 186/500 [03:15<05:07,  1.02it/s]INFO:root:global_step: 186, logpy: -39.030, kl: 17.492, loss: 55.366\n",
      " 37%|███▋      | 187/500 [03:16<05:03,  1.03it/s]INFO:root:global_step: 187, logpy: -38.683, kl: 17.600, loss: 55.139\n",
      " 38%|███▊      | 188/500 [03:17<04:59,  1.04it/s]INFO:root:global_step: 188, logpy: -38.328, kl: 17.713, loss: 54.908\n",
      " 38%|███▊      | 189/500 [03:18<04:55,  1.05it/s]INFO:root:global_step: 189, logpy: -37.971, kl: 17.822, loss: 54.672\n",
      " 38%|███▊      | 190/500 [03:19<04:52,  1.06it/s]INFO:root:global_step: 190, logpy: -37.615, kl: 17.928, loss: 54.433\n",
      " 38%|███▊      | 191/500 [03:20<04:50,  1.06it/s]INFO:root:global_step: 191, logpy: -37.241, kl: 18.043, loss: 54.185\n",
      " 38%|███▊      | 192/500 [03:20<04:50,  1.06it/s]INFO:root:global_step: 192, logpy: -36.891, kl: 18.155, loss: 53.958\n",
      " 39%|███▊      | 193/500 [03:21<04:53,  1.05it/s]INFO:root:global_step: 193, logpy: -36.536, kl: 18.256, loss: 53.715\n",
      " 39%|███▉      | 194/500 [03:22<04:49,  1.06it/s]INFO:root:global_step: 194, logpy: -36.181, kl: 18.360, loss: 53.474\n",
      " 39%|███▉      | 195/500 [03:23<04:46,  1.06it/s]INFO:root:global_step: 195, logpy: -35.832, kl: 18.466, loss: 53.243\n",
      " 39%|███▉      | 196/500 [03:24<04:44,  1.07it/s]INFO:root:global_step: 196, logpy: -35.484, kl: 18.578, loss: 53.016\n",
      " 39%|███▉      | 197/500 [03:25<04:42,  1.07it/s]INFO:root:global_step: 197, logpy: -35.120, kl: 18.682, loss: 52.767\n",
      " 40%|███▉      | 198/500 [03:26<04:40,  1.08it/s]INFO:root:global_step: 198, logpy: -34.743, kl: 18.794, loss: 52.513\n",
      " 40%|███▉      | 199/500 [03:27<04:39,  1.08it/s]INFO:root:global_step: 199, logpy: -34.389, kl: 18.909, loss: 52.283\n",
      " 40%|████      | 200/500 [03:28<04:44,  1.05it/s]INFO:root:Saved figure at: ./img/global_step_200.png\n",
      "INFO:root:global_step: 200, logpy: -34.038, kl: 19.023, loss: 52.057\n",
      " 40%|████      | 201/500 [03:33<11:18,  2.27s/it]INFO:root:global_step: 201, logpy: -33.705, kl: 19.128, loss: 51.839\n",
      " 40%|████      | 202/500 [03:34<09:19,  1.88s/it]INFO:root:global_step: 202, logpy: -33.348, kl: 19.235, loss: 51.599\n",
      " 41%|████      | 203/500 [03:35<07:57,  1.61s/it]INFO:root:global_step: 203, logpy: -32.993, kl: 19.327, loss: 51.346\n",
      " 41%|████      | 204/500 [03:36<07:00,  1.42s/it]INFO:root:global_step: 204, logpy: -32.651, kl: 19.435, loss: 51.122\n",
      " 41%|████      | 205/500 [03:37<06:16,  1.28s/it]INFO:root:global_step: 205, logpy: -32.299, kl: 19.535, loss: 50.879\n",
      " 41%|████      | 206/500 [03:38<05:48,  1.19s/it]INFO:root:global_step: 206, logpy: -31.961, kl: 19.634, loss: 50.650\n",
      " 41%|████▏     | 207/500 [03:39<05:34,  1.14s/it]INFO:root:global_step: 207, logpy: -31.625, kl: 19.731, loss: 50.420\n",
      " 42%|████▏     | 208/500 [03:40<05:18,  1.09s/it]INFO:root:global_step: 208, logpy: -31.284, kl: 19.831, loss: 50.188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 209/500 [03:41<05:06,  1.05s/it]INFO:root:global_step: 209, logpy: -30.970, kl: 19.937, loss: 49.990\n",
      " 42%|████▏     | 210/500 [03:42<04:57,  1.02s/it]INFO:root:global_step: 210, logpy: -30.644, kl: 20.048, loss: 49.784\n",
      " 42%|████▏     | 211/500 [03:43<04:48,  1.00it/s]INFO:root:global_step: 211, logpy: -30.321, kl: 20.152, loss: 49.574\n",
      " 42%|████▏     | 212/500 [03:44<04:43,  1.01it/s]INFO:root:global_step: 212, logpy: -29.983, kl: 20.256, loss: 49.349\n",
      " 43%|████▎     | 213/500 [03:45<04:38,  1.03it/s]INFO:root:global_step: 213, logpy: -29.662, kl: 20.358, loss: 49.139\n",
      " 43%|████▎     | 214/500 [03:46<04:34,  1.04it/s]INFO:root:global_step: 214, logpy: -29.328, kl: 20.464, loss: 48.920\n",
      " 43%|████▎     | 215/500 [03:47<04:30,  1.05it/s]INFO:root:global_step: 215, logpy: -29.025, kl: 20.551, loss: 48.713\n",
      " 43%|████▎     | 216/500 [03:48<04:32,  1.04it/s]INFO:root:global_step: 216, logpy: -28.709, kl: 20.656, loss: 48.510\n",
      " 43%|████▎     | 217/500 [03:49<04:29,  1.05it/s]INFO:root:global_step: 217, logpy: -28.381, kl: 20.760, loss: 48.295\n",
      " 44%|████▎     | 218/500 [03:50<04:26,  1.06it/s]INFO:root:global_step: 218, logpy: -28.058, kl: 20.859, loss: 48.080\n",
      " 44%|████▍     | 219/500 [03:51<04:24,  1.06it/s]INFO:root:global_step: 219, logpy: -27.734, kl: 20.962, loss: 47.867\n",
      " 44%|████▍     | 220/500 [03:52<04:22,  1.07it/s]INFO:root:global_step: 220, logpy: -27.412, kl: 21.056, loss: 47.647\n",
      " 44%|████▍     | 221/500 [03:52<04:21,  1.07it/s]INFO:root:global_step: 221, logpy: -27.107, kl: 21.139, loss: 47.433\n",
      " 44%|████▍     | 222/500 [03:53<04:19,  1.07it/s]INFO:root:global_step: 222, logpy: -26.806, kl: 21.235, loss: 47.236\n",
      " 45%|████▍     | 223/500 [03:54<04:23,  1.05it/s]INFO:root:global_step: 223, logpy: -26.494, kl: 21.348, loss: 47.045\n",
      " 45%|████▍     | 224/500 [03:55<04:20,  1.06it/s]INFO:root:global_step: 224, logpy: -26.190, kl: 21.457, loss: 46.858\n",
      " 45%|████▌     | 225/500 [03:56<04:18,  1.06it/s]INFO:root:global_step: 225, logpy: -25.899, kl: 21.555, loss: 46.673\n",
      " 45%|████▌     | 226/500 [03:57<04:16,  1.07it/s]INFO:root:global_step: 226, logpy: -25.599, kl: 21.658, loss: 46.484\n",
      " 45%|████▌     | 227/500 [03:58<04:14,  1.07it/s]INFO:root:global_step: 227, logpy: -25.310, kl: 21.741, loss: 46.286\n",
      " 46%|████▌     | 228/500 [03:59<04:14,  1.07it/s]INFO:root:global_step: 228, logpy: -25.022, kl: 21.827, loss: 46.091\n",
      " 46%|████▌     | 229/500 [04:00<04:13,  1.07it/s]INFO:root:global_step: 229, logpy: -24.717, kl: 21.928, loss: 45.895\n",
      " 46%|████▌     | 230/500 [04:01<04:13,  1.07it/s]INFO:root:global_step: 230, logpy: -24.415, kl: 22.035, loss: 45.707\n",
      " 46%|████▌     | 231/500 [04:02<04:16,  1.05it/s]INFO:root:global_step: 231, logpy: -24.088, kl: 22.138, loss: 45.491\n",
      " 46%|████▋     | 232/500 [04:03<04:13,  1.06it/s]INFO:root:global_step: 232, logpy: -23.792, kl: 22.245, loss: 45.309\n",
      " 47%|████▋     | 233/500 [04:04<04:11,  1.06it/s]INFO:root:global_step: 233, logpy: -23.495, kl: 22.345, loss: 45.119\n",
      " 47%|████▋     | 234/500 [04:05<04:09,  1.07it/s]INFO:root:global_step: 234, logpy: -23.177, kl: 22.452, loss: 44.915\n",
      " 47%|████▋     | 235/500 [04:06<04:08,  1.07it/s]INFO:root:global_step: 235, logpy: -22.896, kl: 22.553, loss: 44.743\n",
      " 47%|████▋     | 236/500 [04:07<04:06,  1.07it/s]INFO:root:global_step: 236, logpy: -22.607, kl: 22.645, loss: 44.552\n",
      " 47%|████▋     | 237/500 [04:08<04:05,  1.07it/s]INFO:root:global_step: 237, logpy: -22.320, kl: 22.723, loss: 44.350\n",
      " 48%|████▊     | 238/500 [04:08<04:04,  1.07it/s]INFO:root:global_step: 238, logpy: -22.049, kl: 22.809, loss: 44.172\n",
      " 48%|████▊     | 239/500 [04:09<04:08,  1.05it/s]INFO:root:global_step: 239, logpy: -21.761, kl: 22.892, loss: 43.974\n",
      " 48%|████▊     | 240/500 [04:10<04:05,  1.06it/s]INFO:root:global_step: 240, logpy: -21.466, kl: 22.991, loss: 43.785\n",
      " 48%|████▊     | 241/500 [04:11<04:03,  1.06it/s]INFO:root:global_step: 241, logpy: -21.192, kl: 23.099, loss: 43.626\n",
      " 48%|████▊     | 242/500 [04:12<04:02,  1.07it/s]INFO:root:global_step: 242, logpy: -20.898, kl: 23.192, loss: 43.431\n",
      " 49%|████▊     | 243/500 [04:13<04:00,  1.07it/s]INFO:root:global_step: 243, logpy: -20.624, kl: 23.286, loss: 43.258\n",
      " 49%|████▉     | 244/500 [04:14<03:59,  1.07it/s]INFO:root:global_step: 244, logpy: -20.346, kl: 23.366, loss: 43.067\n",
      " 49%|████▉     | 245/500 [04:15<03:58,  1.07it/s]INFO:root:global_step: 245, logpy: -20.070, kl: 23.453, loss: 42.884\n",
      " 49%|████▉     | 246/500 [04:16<03:56,  1.07it/s]INFO:root:global_step: 246, logpy: -19.794, kl: 23.538, loss: 42.700\n",
      " 49%|████▉     | 247/500 [04:17<04:00,  1.05it/s]INFO:root:global_step: 247, logpy: -19.532, kl: 23.624, loss: 42.530\n",
      " 50%|████▉     | 248/500 [04:18<03:57,  1.06it/s]INFO:root:global_step: 248, logpy: -19.252, kl: 23.711, loss: 42.344\n",
      " 50%|████▉     | 249/500 [04:19<03:55,  1.07it/s]INFO:root:global_step: 249, logpy: -18.980, kl: 23.811, loss: 42.177\n",
      " 50%|█████     | 250/500 [04:20<03:53,  1.07it/s]INFO:root:Saved figure at: ./img/global_step_250.png\n",
      "INFO:root:global_step: 250, logpy: -18.701, kl: 23.910, loss: 42.004\n",
      " 50%|█████     | 251/500 [04:25<09:20,  2.25s/it]INFO:root:global_step: 251, logpy: -18.434, kl: 24.001, loss: 41.834\n",
      " 50%|█████     | 252/500 [04:26<07:44,  1.87s/it]INFO:root:global_step: 252, logpy: -18.163, kl: 24.093, loss: 41.661\n",
      " 51%|█████     | 253/500 [04:27<06:35,  1.60s/it]INFO:root:global_step: 253, logpy: -17.907, kl: 24.182, loss: 41.500\n",
      " 51%|█████     | 254/500 [04:28<06:01,  1.47s/it]INFO:root:global_step: 254, logpy: -17.656, kl: 24.253, loss: 41.326\n",
      " 51%|█████     | 255/500 [04:29<05:44,  1.41s/it]INFO:root:global_step: 255, logpy: -17.397, kl: 24.334, loss: 41.153\n",
      " 51%|█████     | 256/500 [04:31<05:22,  1.32s/it]INFO:root:global_step: 256, logpy: -17.136, kl: 24.424, loss: 40.988\n",
      " 51%|█████▏    | 257/500 [04:32<04:58,  1.23s/it]INFO:root:global_step: 257, logpy: -16.863, kl: 24.512, loss: 40.809\n",
      " 52%|█████▏    | 258/500 [04:33<04:39,  1.15s/it]INFO:root:global_step: 258, logpy: -16.600, kl: 24.610, loss: 40.650\n",
      " 52%|█████▏    | 259/500 [04:34<04:24,  1.10s/it]INFO:root:global_step: 259, logpy: -16.344, kl: 24.692, loss: 40.481\n",
      " 52%|█████▏    | 260/500 [04:35<04:16,  1.07s/it]INFO:root:global_step: 260, logpy: -16.094, kl: 24.772, loss: 40.316\n",
      " 52%|█████▏    | 261/500 [04:35<04:08,  1.04s/it]INFO:root:global_step: 261, logpy: -15.838, kl: 24.867, loss: 40.161\n",
      " 52%|█████▏    | 262/500 [04:37<04:13,  1.07s/it]INFO:root:global_step: 262, logpy: -15.591, kl: 24.954, loss: 40.007\n",
      " 53%|█████▎    | 263/500 [04:38<04:17,  1.09s/it]INFO:root:global_step: 263, logpy: -15.357, kl: 25.044, loss: 39.868\n",
      " 53%|█████▎    | 264/500 [04:39<04:08,  1.05s/it]INFO:root:global_step: 264, logpy: -15.093, kl: 25.119, loss: 39.684\n",
      " 53%|█████▎    | 265/500 [04:40<04:02,  1.03s/it]INFO:root:global_step: 265, logpy: -14.854, kl: 25.206, loss: 39.538\n",
      " 53%|█████▎    | 266/500 [04:41<04:03,  1.04s/it]INFO:root:global_step: 266, logpy: -14.593, kl: 25.281, loss: 39.356\n",
      " 53%|█████▎    | 267/500 [04:42<04:04,  1.05s/it]INFO:root:global_step: 267, logpy: -14.346, kl: 25.368, loss: 39.202\n",
      " 54%|█████▎    | 268/500 [04:43<04:00,  1.03s/it]INFO:root:global_step: 268, logpy: -14.105, kl: 25.463, loss: 39.060\n",
      " 54%|█████▍    | 269/500 [04:44<03:52,  1.00s/it]INFO:root:global_step: 269, logpy: -13.855, kl: 25.544, loss: 38.898\n",
      " 54%|█████▍    | 270/500 [04:45<03:46,  1.02it/s]INFO:root:global_step: 270, logpy: -13.612, kl: 25.625, loss: 38.740\n",
      " 54%|█████▍    | 271/500 [04:46<03:46,  1.01it/s]INFO:root:global_step: 271, logpy: -13.378, kl: 25.699, loss: 38.586\n",
      " 54%|█████▍    | 272/500 [04:47<03:41,  1.03it/s]INFO:root:global_step: 272, logpy: -13.139, kl: 25.776, loss: 38.428\n",
      " 55%|█████▍    | 273/500 [04:48<03:37,  1.04it/s]INFO:root:global_step: 273, logpy: -12.902, kl: 25.864, loss: 38.283\n",
      " 55%|█████▍    | 274/500 [04:48<03:34,  1.05it/s]INFO:root:global_step: 274, logpy: -12.667, kl: 25.943, loss: 38.132\n",
      " 55%|█████▌    | 275/500 [04:49<03:32,  1.06it/s]INFO:root:global_step: 275, logpy: -12.429, kl: 26.022, loss: 37.978\n",
      " 55%|█████▌    | 276/500 [04:50<03:30,  1.06it/s]INFO:root:global_step: 276, logpy: -12.187, kl: 26.103, loss: 37.822\n",
      " 55%|█████▌    | 277/500 [04:51<03:28,  1.07it/s]INFO:root:global_step: 277, logpy: -11.965, kl: 26.181, loss: 37.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 278/500 [04:52<03:28,  1.07it/s]INFO:root:global_step: 278, logpy: -11.742, kl: 26.258, loss: 37.542\n",
      " 56%|█████▌    | 279/500 [04:53<03:26,  1.07it/s]INFO:root:global_step: 279, logpy: -11.504, kl: 26.334, loss: 37.385\n",
      " 56%|█████▌    | 280/500 [04:54<03:29,  1.05it/s]INFO:root:global_step: 280, logpy: -11.282, kl: 26.410, loss: 37.243\n",
      " 56%|█████▌    | 281/500 [04:55<03:27,  1.06it/s]INFO:root:global_step: 281, logpy: -11.058, kl: 26.474, loss: 37.087\n",
      " 56%|█████▋    | 282/500 [04:56<03:25,  1.06it/s]INFO:root:global_step: 282, logpy: -10.843, kl: 26.551, loss: 36.954\n",
      " 57%|█████▋    | 283/500 [04:57<03:23,  1.07it/s]INFO:root:global_step: 283, logpy: -10.659, kl: 26.623, loss: 36.847\n",
      " 57%|█████▋    | 284/500 [04:58<03:22,  1.07it/s]INFO:root:global_step: 284, logpy: -10.444, kl: 26.692, loss: 36.704\n",
      " 57%|█████▋    | 285/500 [04:59<03:21,  1.07it/s]INFO:root:global_step: 285, logpy: -10.228, kl: 26.783, loss: 36.584\n",
      " 57%|█████▋    | 286/500 [05:00<03:20,  1.07it/s]INFO:root:global_step: 286, logpy: -10.010, kl: 26.872, loss: 36.459\n",
      " 57%|█████▋    | 287/500 [05:01<03:23,  1.05it/s]INFO:root:global_step: 287, logpy: -9.797, kl: 26.951, loss: 36.330\n",
      " 58%|█████▊    | 288/500 [05:02<03:20,  1.06it/s]INFO:root:global_step: 288, logpy: -9.594, kl: 27.034, loss: 36.214\n",
      " 58%|█████▊    | 289/500 [05:03<03:18,  1.06it/s]INFO:root:global_step: 289, logpy: -9.383, kl: 27.095, loss: 36.067\n",
      " 58%|█████▊    | 290/500 [05:04<03:17,  1.06it/s]INFO:root:global_step: 290, logpy: -9.174, kl: 27.167, loss: 35.935\n",
      " 58%|█████▊    | 291/500 [05:04<03:15,  1.07it/s]INFO:root:global_step: 291, logpy: -8.962, kl: 27.242, loss: 35.802\n",
      " 58%|█████▊    | 292/500 [05:05<03:14,  1.07it/s]INFO:root:global_step: 292, logpy: -8.761, kl: 27.323, loss: 35.685\n",
      " 59%|█████▊    | 293/500 [05:06<03:13,  1.07it/s]INFO:root:global_step: 293, logpy: -8.554, kl: 27.392, loss: 35.552\n",
      " 59%|█████▉    | 294/500 [05:07<03:12,  1.07it/s]INFO:root:global_step: 294, logpy: -8.366, kl: 27.451, loss: 35.427\n",
      " 59%|█████▉    | 295/500 [05:08<03:11,  1.07it/s]INFO:root:global_step: 295, logpy: -8.180, kl: 27.522, loss: 35.316\n",
      " 59%|█████▉    | 296/500 [05:09<03:14,  1.05it/s]INFO:root:global_step: 296, logpy: -7.987, kl: 27.593, loss: 35.198\n",
      " 59%|█████▉    | 297/500 [05:10<03:12,  1.06it/s]INFO:root:global_step: 297, logpy: -7.795, kl: 27.677, loss: 35.093\n",
      " 60%|█████▉    | 298/500 [05:11<03:10,  1.06it/s]INFO:root:global_step: 298, logpy: -7.598, kl: 27.763, loss: 34.986\n",
      " 60%|█████▉    | 299/500 [05:12<03:08,  1.07it/s]INFO:root:global_step: 299, logpy: -7.405, kl: 27.844, loss: 34.877\n",
      " 60%|██████    | 300/500 [05:13<03:07,  1.07it/s]INFO:root:Saved figure at: ./img/global_step_300.png\n",
      "INFO:root:global_step: 300, logpy: -7.210, kl: 27.905, loss: 34.748\n",
      " 60%|██████    | 301/500 [05:18<07:26,  2.25s/it]INFO:root:global_step: 301, logpy: -7.021, kl: 27.955, loss: 34.612\n",
      " 60%|██████    | 302/500 [05:19<06:08,  1.86s/it]INFO:root:global_step: 302, logpy: -6.849, kl: 28.025, loss: 34.513\n",
      " 61%|██████    | 303/500 [05:20<05:14,  1.60s/it]INFO:root:global_step: 303, logpy: -6.667, kl: 28.108, loss: 34.418\n",
      " 61%|██████    | 304/500 [05:21<04:39,  1.43s/it]INFO:root:global_step: 304, logpy: -6.480, kl: 28.187, loss: 34.314\n",
      " 61%|██████    | 305/500 [05:22<04:10,  1.29s/it]INFO:root:global_step: 305, logpy: -6.292, kl: 28.260, loss: 34.202\n",
      " 61%|██████    | 306/500 [05:23<03:51,  1.20s/it]INFO:root:global_step: 306, logpy: -6.106, kl: 28.332, loss: 34.091\n",
      " 61%|██████▏   | 307/500 [05:24<03:40,  1.14s/it]INFO:root:global_step: 307, logpy: -5.940, kl: 28.391, loss: 33.988\n",
      " 62%|██████▏   | 308/500 [05:25<03:30,  1.09s/it]INFO:root:global_step: 308, logpy: -5.768, kl: 28.441, loss: 33.870\n",
      " 62%|██████▏   | 309/500 [05:26<03:30,  1.10s/it]INFO:root:global_step: 309, logpy: -5.572, kl: 28.499, loss: 33.735\n",
      " 62%|██████▏   | 310/500 [05:27<03:30,  1.11s/it]INFO:root:global_step: 310, logpy: -5.405, kl: 28.566, loss: 33.638\n",
      " 62%|██████▏   | 311/500 [05:28<03:22,  1.07s/it]INFO:root:global_step: 311, logpy: -5.227, kl: 28.646, loss: 33.544\n",
      " 62%|██████▏   | 312/500 [05:29<03:20,  1.07s/it]INFO:root:global_step: 312, logpy: -5.041, kl: 28.712, loss: 33.427\n",
      " 63%|██████▎   | 313/500 [05:30<03:19,  1.07s/it]INFO:root:global_step: 313, logpy: -4.865, kl: 28.768, loss: 33.310\n",
      " 63%|██████▎   | 314/500 [05:32<03:20,  1.08s/it]INFO:root:global_step: 314, logpy: -4.686, kl: 28.832, loss: 33.199\n",
      " 63%|██████▎   | 315/500 [05:33<03:12,  1.04s/it]INFO:root:global_step: 315, logpy: -4.513, kl: 28.894, loss: 33.091\n",
      " 63%|██████▎   | 316/500 [05:34<03:08,  1.02s/it]INFO:root:global_step: 316, logpy: -4.338, kl: 28.965, loss: 32.990\n",
      " 63%|██████▎   | 317/500 [05:35<03:06,  1.02s/it]INFO:root:global_step: 317, logpy: -4.154, kl: 29.026, loss: 32.870\n",
      " 64%|██████▎   | 318/500 [05:36<03:14,  1.07s/it]INFO:root:global_step: 318, logpy: -3.990, kl: 29.092, loss: 32.775\n",
      " 64%|██████▍   | 319/500 [05:37<03:08,  1.04s/it]INFO:root:global_step: 319, logpy: -3.826, kl: 29.154, loss: 32.676\n",
      " 64%|██████▍   | 320/500 [05:38<03:08,  1.05s/it]INFO:root:global_step: 320, logpy: -3.653, kl: 29.193, loss: 32.545\n",
      " 64%|██████▍   | 321/500 [05:39<03:03,  1.02s/it]INFO:root:global_step: 321, logpy: -3.497, kl: 29.255, loss: 32.454\n",
      " 64%|██████▍   | 322/500 [05:40<02:58,  1.00s/it]INFO:root:global_step: 322, logpy: -3.323, kl: 29.310, loss: 32.339\n",
      " 65%|██████▍   | 323/500 [05:41<02:53,  1.02it/s]INFO:root:global_step: 323, logpy: -3.158, kl: 29.371, loss: 32.238\n",
      " 65%|██████▍   | 324/500 [05:42<02:50,  1.03it/s]INFO:root:global_step: 324, logpy: -2.998, kl: 29.440, loss: 32.149\n",
      " 65%|██████▌   | 325/500 [05:42<02:47,  1.05it/s]INFO:root:global_step: 325, logpy: -2.833, kl: 29.505, loss: 32.053\n",
      " 65%|██████▌   | 326/500 [05:43<02:45,  1.05it/s]INFO:root:global_step: 326, logpy: -2.674, kl: 29.564, loss: 31.955\n",
      " 65%|██████▌   | 327/500 [05:44<02:43,  1.06it/s]INFO:root:global_step: 327, logpy: -2.513, kl: 29.620, loss: 31.852\n",
      " 66%|██████▌   | 328/500 [05:45<02:41,  1.06it/s]INFO:root:global_step: 328, logpy: -2.366, kl: 29.685, loss: 31.774\n",
      " 66%|██████▌   | 329/500 [05:46<02:43,  1.05it/s]INFO:root:global_step: 329, logpy: -2.219, kl: 29.740, loss: 31.684\n",
      " 66%|██████▌   | 330/500 [05:47<02:40,  1.06it/s]INFO:root:global_step: 330, logpy: -2.064, kl: 29.787, loss: 31.579\n",
      " 66%|██████▌   | 331/500 [05:48<02:38,  1.07it/s]INFO:root:global_step: 331, logpy: -1.912, kl: 29.836, loss: 31.479\n",
      " 66%|██████▋   | 332/500 [05:49<02:37,  1.07it/s]INFO:root:global_step: 332, logpy: -1.754, kl: 29.884, loss: 31.372\n",
      " 67%|██████▋   | 333/500 [05:50<02:35,  1.07it/s]INFO:root:global_step: 333, logpy: -1.609, kl: 29.935, loss: 31.280\n",
      " 67%|██████▋   | 334/500 [05:51<02:34,  1.07it/s]INFO:root:global_step: 334, logpy: -1.462, kl: 29.995, loss: 31.196\n",
      " 67%|██████▋   | 335/500 [05:52<02:33,  1.07it/s]INFO:root:global_step: 335, logpy: -1.318, kl: 30.053, loss: 31.113\n",
      " 67%|██████▋   | 336/500 [05:53<02:32,  1.08it/s]INFO:root:global_step: 336, logpy: -1.161, kl: 30.120, loss: 31.025\n",
      " 67%|██████▋   | 337/500 [05:54<02:35,  1.05it/s]INFO:root:global_step: 337, logpy: -1.015, kl: 30.180, loss: 30.941\n",
      " 68%|██████▊   | 338/500 [05:55<02:33,  1.06it/s]INFO:root:global_step: 338, logpy: -0.855, kl: 30.234, loss: 30.838\n",
      " 68%|██████▊   | 339/500 [05:56<02:31,  1.06it/s]INFO:root:global_step: 339, logpy: -0.709, kl: 30.289, loss: 30.750\n",
      " 68%|██████▊   | 340/500 [05:57<02:30,  1.06it/s]INFO:root:global_step: 340, logpy: -0.576, kl: 30.326, loss: 30.655\n",
      " 68%|██████▊   | 341/500 [05:57<02:28,  1.07it/s]INFO:root:global_step: 341, logpy: -0.441, kl: 30.378, loss: 30.575\n",
      " 68%|██████▊   | 342/500 [05:58<02:27,  1.07it/s]INFO:root:global_step: 342, logpy: -0.307, kl: 30.419, loss: 30.485\n",
      " 69%|██████▊   | 343/500 [05:59<02:26,  1.07it/s]INFO:root:global_step: 343, logpy: -0.168, kl: 30.450, loss: 30.379\n",
      " 69%|██████▉   | 344/500 [06:00<02:24,  1.08it/s]INFO:root:global_step: 344, logpy: -0.037, kl: 30.500, loss: 30.300\n",
      " 69%|██████▉   | 345/500 [06:01<02:23,  1.08it/s]INFO:root:global_step: 345, logpy: 0.121, kl: 30.537, loss: 30.183\n",
      " 69%|██████▉   | 346/500 [06:02<02:27,  1.05it/s]INFO:root:global_step: 346, logpy: 0.263, kl: 30.584, loss: 30.090\n",
      " 69%|██████▉   | 347/500 [06:03<02:27,  1.04it/s]INFO:root:global_step: 347, logpy: 0.386, kl: 30.631, loss: 30.016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 348/500 [06:04<02:25,  1.04it/s]INFO:root:global_step: 348, logpy: 0.535, kl: 30.672, loss: 29.910\n",
      " 70%|██████▉   | 349/500 [06:05<02:25,  1.04it/s]INFO:root:global_step: 349, logpy: 0.671, kl: 30.721, loss: 29.825\n",
      " 70%|███████   | 350/500 [06:06<02:24,  1.03it/s]INFO:root:Saved figure at: ./img/global_step_350.png\n",
      "INFO:root:global_step: 350, logpy: 0.801, kl: 30.769, loss: 29.746\n",
      " 70%|███████   | 351/500 [06:12<05:49,  2.35s/it]INFO:root:global_step: 351, logpy: 0.932, kl: 30.816, loss: 29.664\n",
      " 70%|███████   | 352/500 [06:13<04:46,  1.93s/it]INFO:root:global_step: 352, logpy: 1.061, kl: 30.854, loss: 29.576\n",
      " 71%|███████   | 353/500 [06:14<04:02,  1.65s/it]INFO:root:global_step: 353, logpy: 1.195, kl: 30.897, loss: 29.486\n",
      " 71%|███████   | 354/500 [06:15<03:33,  1.46s/it]INFO:root:global_step: 354, logpy: 1.326, kl: 30.958, loss: 29.418\n",
      " 71%|███████   | 355/500 [06:16<03:09,  1.31s/it]INFO:root:global_step: 355, logpy: 1.461, kl: 30.991, loss: 29.318\n",
      " 71%|███████   | 356/500 [06:17<02:53,  1.21s/it]INFO:root:global_step: 356, logpy: 1.584, kl: 31.041, loss: 29.247\n",
      " 71%|███████▏  | 357/500 [06:18<02:43,  1.14s/it]INFO:root:global_step: 357, logpy: 1.704, kl: 31.086, loss: 29.175\n",
      " 72%|███████▏  | 358/500 [06:19<02:35,  1.09s/it]INFO:root:global_step: 358, logpy: 1.830, kl: 31.127, loss: 29.091\n",
      " 72%|███████▏  | 359/500 [06:19<02:28,  1.05s/it]INFO:root:global_step: 359, logpy: 1.936, kl: 31.167, loss: 29.028\n",
      " 72%|███████▏  | 360/500 [06:20<02:23,  1.03s/it]INFO:root:global_step: 360, logpy: 2.042, kl: 31.202, loss: 28.959\n",
      " 72%|███████▏  | 361/500 [06:21<02:19,  1.00s/it]INFO:root:global_step: 361, logpy: 2.157, kl: 31.232, loss: 28.876\n",
      " 72%|███████▏  | 362/500 [06:22<02:17,  1.00it/s]INFO:root:global_step: 362, logpy: 2.285, kl: 31.282, loss: 28.800\n",
      " 73%|███████▎  | 363/500 [06:23<02:14,  1.02it/s]INFO:root:global_step: 363, logpy: 2.394, kl: 31.338, loss: 28.748\n",
      " 73%|███████▎  | 364/500 [06:24<02:11,  1.04it/s]INFO:root:global_step: 364, logpy: 2.527, kl: 31.398, loss: 28.678\n",
      " 73%|███████▎  | 365/500 [06:25<02:08,  1.05it/s]INFO:root:global_step: 365, logpy: 2.640, kl: 31.444, loss: 28.613\n",
      " 73%|███████▎  | 366/500 [06:26<02:06,  1.06it/s]INFO:root:global_step: 366, logpy: 2.743, kl: 31.480, loss: 28.548\n",
      " 73%|███████▎  | 367/500 [06:27<02:04,  1.06it/s]INFO:root:global_step: 367, logpy: 2.847, kl: 31.502, loss: 28.467\n",
      " 74%|███████▎  | 368/500 [06:28<02:03,  1.07it/s]INFO:root:global_step: 368, logpy: 2.960, kl: 31.543, loss: 28.398\n",
      " 74%|███████▍  | 369/500 [06:29<02:03,  1.06it/s]INFO:root:global_step: 369, logpy: 3.079, kl: 31.588, loss: 28.325\n",
      " 74%|███████▍  | 370/500 [06:30<02:01,  1.07it/s]INFO:root:global_step: 370, logpy: 3.201, kl: 31.633, loss: 28.250\n",
      " 74%|███████▍  | 371/500 [06:31<02:03,  1.05it/s]INFO:root:global_step: 371, logpy: 3.312, kl: 31.661, loss: 28.169\n",
      " 74%|███████▍  | 372/500 [06:32<02:01,  1.06it/s]INFO:root:global_step: 372, logpy: 3.405, kl: 31.701, loss: 28.117\n",
      " 75%|███████▍  | 373/500 [06:33<01:59,  1.06it/s]INFO:root:global_step: 373, logpy: 3.506, kl: 31.732, loss: 28.049\n",
      " 75%|███████▍  | 374/500 [06:34<01:58,  1.07it/s]INFO:root:global_step: 374, logpy: 3.609, kl: 31.788, loss: 28.005\n",
      " 75%|███████▌  | 375/500 [06:35<01:57,  1.07it/s]INFO:root:global_step: 375, logpy: 3.710, kl: 31.817, loss: 27.934\n",
      " 75%|███████▌  | 376/500 [06:35<01:56,  1.07it/s]INFO:root:global_step: 376, logpy: 3.819, kl: 31.865, loss: 27.875\n",
      " 75%|███████▌  | 377/500 [06:36<01:54,  1.07it/s]INFO:root:global_step: 377, logpy: 3.930, kl: 31.904, loss: 27.805\n",
      " 76%|███████▌  | 378/500 [06:37<01:53,  1.07it/s]INFO:root:global_step: 378, logpy: 4.037, kl: 31.932, loss: 27.726\n",
      " 76%|███████▌  | 379/500 [06:38<01:52,  1.08it/s]INFO:root:global_step: 379, logpy: 4.137, kl: 31.963, loss: 27.659\n",
      " 76%|███████▌  | 380/500 [06:39<01:54,  1.05it/s]INFO:root:global_step: 380, logpy: 4.250, kl: 31.998, loss: 27.583\n",
      " 76%|███████▌  | 381/500 [06:40<01:52,  1.06it/s]INFO:root:global_step: 381, logpy: 4.361, kl: 32.034, loss: 27.511\n",
      " 76%|███████▋  | 382/500 [06:41<01:50,  1.07it/s]INFO:root:global_step: 382, logpy: 4.471, kl: 32.069, loss: 27.436\n",
      " 77%|███████▋  | 383/500 [06:42<01:49,  1.07it/s]INFO:root:global_step: 383, logpy: 4.566, kl: 32.108, loss: 27.383\n",
      " 77%|███████▋  | 384/500 [06:43<01:48,  1.07it/s]INFO:root:global_step: 384, logpy: 4.659, kl: 32.140, loss: 27.322\n",
      " 77%|███████▋  | 385/500 [06:44<01:47,  1.07it/s]INFO:root:global_step: 385, logpy: 4.759, kl: 32.171, loss: 27.255\n",
      " 77%|███████▋  | 386/500 [06:45<01:46,  1.07it/s]INFO:root:global_step: 386, logpy: 4.860, kl: 32.204, loss: 27.190\n",
      " 77%|███████▋  | 387/500 [06:46<01:45,  1.07it/s]INFO:root:global_step: 387, logpy: 4.962, kl: 32.230, loss: 27.115\n",
      " 78%|███████▊  | 388/500 [06:47<01:46,  1.05it/s]INFO:root:global_step: 388, logpy: 5.062, kl: 32.264, loss: 27.051\n",
      " 78%|███████▊  | 389/500 [06:48<01:45,  1.05it/s]INFO:root:global_step: 389, logpy: 5.167, kl: 32.291, loss: 26.974\n",
      " 78%|███████▊  | 390/500 [06:49<01:43,  1.06it/s]INFO:root:global_step: 390, logpy: 5.259, kl: 32.332, loss: 26.924\n",
      " 78%|███████▊  | 391/500 [06:50<01:42,  1.06it/s]INFO:root:global_step: 391, logpy: 5.359, kl: 32.375, loss: 26.869\n",
      " 78%|███████▊  | 392/500 [06:50<01:41,  1.07it/s]INFO:root:global_step: 392, logpy: 5.461, kl: 32.405, loss: 26.798\n",
      " 79%|███████▊  | 393/500 [06:51<01:39,  1.07it/s]INFO:root:global_step: 393, logpy: 5.556, kl: 32.436, loss: 26.736\n",
      " 79%|███████▉  | 394/500 [06:52<01:38,  1.08it/s]INFO:root:global_step: 394, logpy: 5.649, kl: 32.477, loss: 26.685\n",
      " 79%|███████▉  | 395/500 [06:53<01:37,  1.08it/s]INFO:root:global_step: 395, logpy: 5.750, kl: 32.517, loss: 26.626\n",
      " 79%|███████▉  | 396/500 [06:54<01:39,  1.05it/s]INFO:root:global_step: 396, logpy: 5.832, kl: 32.545, loss: 26.574\n",
      " 79%|███████▉  | 397/500 [06:55<01:37,  1.06it/s]INFO:root:global_step: 397, logpy: 5.924, kl: 32.582, loss: 26.519\n",
      " 80%|███████▉  | 398/500 [06:56<01:36,  1.06it/s]INFO:root:global_step: 398, logpy: 6.003, kl: 32.619, loss: 26.479\n",
      " 80%|███████▉  | 399/500 [06:57<01:34,  1.07it/s]INFO:root:global_step: 399, logpy: 6.095, kl: 32.665, loss: 26.434\n",
      " 80%|████████  | 400/500 [06:58<01:33,  1.07it/s]INFO:root:Saved figure at: ./img/global_step_400.png\n",
      "INFO:root:global_step: 400, logpy: 6.184, kl: 32.710, loss: 26.391\n",
      " 80%|████████  | 401/500 [07:03<03:42,  2.25s/it]INFO:root:global_step: 401, logpy: 6.285, kl: 32.733, loss: 26.315\n",
      " 80%|████████  | 402/500 [07:04<03:03,  1.87s/it]INFO:root:global_step: 402, logpy: 6.356, kl: 32.763, loss: 26.276\n",
      " 81%|████████  | 403/500 [07:05<02:34,  1.60s/it]INFO:root:global_step: 403, logpy: 6.428, kl: 32.796, loss: 26.237\n",
      " 81%|████████  | 404/500 [07:06<02:14,  1.41s/it]INFO:root:global_step: 404, logpy: 6.521, kl: 32.826, loss: 26.176\n",
      " 81%|████████  | 405/500 [07:07<02:02,  1.29s/it]INFO:root:global_step: 405, logpy: 6.599, kl: 32.853, loss: 26.126\n",
      " 81%|████████  | 406/500 [07:08<01:52,  1.19s/it]INFO:root:global_step: 406, logpy: 6.683, kl: 32.892, loss: 26.083\n",
      " 81%|████████▏ | 407/500 [07:09<01:44,  1.13s/it]INFO:root:global_step: 407, logpy: 6.757, kl: 32.932, loss: 26.050\n",
      " 82%|████████▏ | 408/500 [07:10<01:38,  1.07s/it]INFO:root:global_step: 408, logpy: 6.845, kl: 32.952, loss: 25.983\n",
      " 82%|████████▏ | 409/500 [07:11<01:33,  1.03s/it]INFO:root:global_step: 409, logpy: 6.929, kl: 32.986, loss: 25.934\n",
      " 82%|████████▏ | 410/500 [07:12<01:30,  1.00s/it]INFO:root:global_step: 410, logpy: 7.010, kl: 33.017, loss: 25.885\n",
      " 82%|████████▏ | 411/500 [07:13<01:27,  1.02it/s]INFO:root:global_step: 411, logpy: 7.106, kl: 33.043, loss: 25.817\n",
      " 82%|████████▏ | 412/500 [07:14<01:25,  1.03it/s]INFO:root:global_step: 412, logpy: 7.191, kl: 33.063, loss: 25.752\n",
      " 83%|████████▎ | 413/500 [07:15<01:26,  1.01it/s]INFO:root:global_step: 413, logpy: 7.255, kl: 33.098, loss: 25.725\n",
      " 83%|████████▎ | 414/500 [07:16<01:25,  1.01it/s]INFO:root:global_step: 414, logpy: 7.342, kl: 33.125, loss: 25.666\n",
      " 83%|████████▎ | 415/500 [07:17<01:22,  1.03it/s]INFO:root:global_step: 415, logpy: 7.417, kl: 33.150, loss: 25.617\n",
      " 83%|████████▎ | 416/500 [07:18<01:20,  1.04it/s]INFO:root:global_step: 416, logpy: 7.496, kl: 33.173, loss: 25.562\n",
      " 83%|████████▎ | 417/500 [07:19<01:19,  1.04it/s]INFO:root:global_step: 417, logpy: 7.574, kl: 33.203, loss: 25.516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 418/500 [07:20<01:18,  1.05it/s]INFO:root:global_step: 418, logpy: 7.650, kl: 33.231, loss: 25.469\n",
      " 84%|████████▍ | 419/500 [07:21<01:17,  1.05it/s]INFO:root:global_step: 419, logpy: 7.728, kl: 33.250, loss: 25.411\n",
      " 84%|████████▍ | 420/500 [07:22<01:15,  1.06it/s]INFO:root:global_step: 420, logpy: 7.810, kl: 33.278, loss: 25.358\n",
      " 84%|████████▍ | 421/500 [07:22<01:14,  1.06it/s]INFO:root:global_step: 421, logpy: 7.899, kl: 33.305, loss: 25.297\n",
      " 84%|████████▍ | 422/500 [07:23<01:14,  1.05it/s]INFO:root:global_step: 422, logpy: 7.984, kl: 33.338, loss: 25.246\n",
      " 85%|████████▍ | 423/500 [07:24<01:13,  1.05it/s]INFO:root:global_step: 423, logpy: 8.056, kl: 33.369, loss: 25.206\n",
      " 85%|████████▍ | 424/500 [07:25<01:11,  1.06it/s]INFO:root:global_step: 424, logpy: 8.128, kl: 33.383, loss: 25.149\n",
      " 85%|████████▌ | 425/500 [07:26<01:10,  1.07it/s]INFO:root:global_step: 425, logpy: 8.209, kl: 33.409, loss: 25.095\n",
      " 85%|████████▌ | 426/500 [07:27<01:09,  1.07it/s]INFO:root:global_step: 426, logpy: 8.288, kl: 33.418, loss: 25.026\n",
      " 85%|████████▌ | 427/500 [07:28<01:08,  1.07it/s]INFO:root:global_step: 427, logpy: 8.354, kl: 33.450, loss: 24.993\n",
      " 86%|████████▌ | 428/500 [07:29<01:07,  1.07it/s]INFO:root:global_step: 428, logpy: 8.414, kl: 33.472, loss: 24.956\n",
      " 86%|████████▌ | 429/500 [07:30<01:06,  1.07it/s]INFO:root:global_step: 429, logpy: 8.490, kl: 33.510, loss: 24.919\n",
      " 86%|████████▌ | 430/500 [07:31<01:05,  1.08it/s]INFO:root:global_step: 430, logpy: 8.570, kl: 33.551, loss: 24.882\n",
      " 86%|████████▌ | 431/500 [07:32<01:05,  1.05it/s]INFO:root:global_step: 431, logpy: 8.653, kl: 33.588, loss: 24.837\n",
      " 86%|████████▋ | 432/500 [07:33<01:04,  1.05it/s]INFO:root:global_step: 432, logpy: 8.735, kl: 33.614, loss: 24.781\n",
      " 87%|████████▋ | 433/500 [07:34<01:03,  1.06it/s]INFO:root:global_step: 433, logpy: 8.802, kl: 33.628, loss: 24.730\n",
      " 87%|████████▋ | 434/500 [07:35<01:02,  1.06it/s]INFO:root:global_step: 434, logpy: 8.841, kl: 33.653, loss: 24.717\n",
      " 87%|████████▋ | 435/500 [07:36<01:01,  1.06it/s]INFO:root:global_step: 435, logpy: 8.906, kl: 33.667, loss: 24.667\n",
      " 87%|████████▋ | 436/500 [07:37<01:00,  1.07it/s]INFO:root:global_step: 436, logpy: 8.967, kl: 33.688, loss: 24.628\n",
      " 87%|████████▋ | 437/500 [07:37<00:58,  1.07it/s]INFO:root:global_step: 437, logpy: 9.034, kl: 33.717, loss: 24.591\n",
      " 88%|████████▊ | 438/500 [07:38<00:57,  1.07it/s]INFO:root:global_step: 438, logpy: 9.103, kl: 33.760, loss: 24.565\n",
      " 88%|████████▊ | 439/500 [07:39<00:56,  1.07it/s]INFO:root:global_step: 439, logpy: 9.177, kl: 33.782, loss: 24.514\n",
      " 88%|████████▊ | 440/500 [07:40<00:56,  1.05it/s]INFO:root:global_step: 440, logpy: 9.234, kl: 33.794, loss: 24.471\n",
      " 88%|████████▊ | 441/500 [07:41<00:55,  1.06it/s]INFO:root:global_step: 441, logpy: 9.291, kl: 33.815, loss: 24.435\n",
      " 88%|████████▊ | 442/500 [07:42<00:54,  1.07it/s]INFO:root:global_step: 442, logpy: 9.365, kl: 33.842, loss: 24.389\n",
      " 89%|████████▊ | 443/500 [07:43<00:53,  1.07it/s]INFO:root:global_step: 443, logpy: 9.445, kl: 33.866, loss: 24.333\n",
      " 89%|████████▉ | 444/500 [07:44<00:52,  1.07it/s]INFO:root:global_step: 444, logpy: 9.513, kl: 33.883, loss: 24.284\n",
      " 89%|████████▉ | 445/500 [07:45<00:51,  1.07it/s]INFO:root:global_step: 445, logpy: 9.581, kl: 33.907, loss: 24.241\n",
      " 89%|████████▉ | 446/500 [07:46<00:50,  1.07it/s]INFO:root:global_step: 446, logpy: 9.637, kl: 33.947, loss: 24.225\n",
      " 89%|████████▉ | 447/500 [07:47<00:49,  1.08it/s]INFO:root:global_step: 447, logpy: 9.696, kl: 33.968, loss: 24.189\n",
      " 90%|████████▉ | 448/500 [07:48<00:49,  1.05it/s]INFO:root:global_step: 448, logpy: 9.757, kl: 33.990, loss: 24.150\n",
      " 90%|████████▉ | 449/500 [07:49<00:48,  1.06it/s]INFO:root:global_step: 449, logpy: 9.819, kl: 34.005, loss: 24.104\n",
      " 90%|█████████ | 450/500 [07:50<00:46,  1.06it/s]INFO:root:Saved figure at: ./img/global_step_450.png\n",
      "INFO:root:global_step: 450, logpy: 9.880, kl: 34.017, loss: 24.056\n",
      " 90%|█████████ | 451/500 [07:55<01:50,  2.25s/it]INFO:root:global_step: 451, logpy: 9.949, kl: 34.032, loss: 24.003\n",
      " 90%|█████████ | 452/500 [07:56<01:29,  1.87s/it]INFO:root:global_step: 452, logpy: 10.007, kl: 34.069, loss: 23.982\n",
      " 91%|█████████ | 453/500 [07:57<01:15,  1.60s/it]INFO:root:global_step: 453, logpy: 10.068, kl: 34.093, loss: 23.946\n",
      " 91%|█████████ | 454/500 [07:58<01:04,  1.41s/it]INFO:root:global_step: 454, logpy: 10.128, kl: 34.121, loss: 23.915\n",
      " 91%|█████████ | 455/500 [07:59<00:57,  1.27s/it]INFO:root:global_step: 455, logpy: 10.183, kl: 34.140, loss: 23.880\n",
      " 91%|█████████ | 456/500 [08:00<00:53,  1.21s/it]INFO:root:global_step: 456, logpy: 10.247, kl: 34.165, loss: 23.842\n",
      " 91%|█████████▏| 457/500 [08:01<00:48,  1.14s/it]INFO:root:global_step: 457, logpy: 10.304, kl: 34.177, loss: 23.797\n",
      " 92%|█████████▏| 458/500 [08:02<00:45,  1.09s/it]INFO:root:global_step: 458, logpy: 10.378, kl: 34.193, loss: 23.740\n",
      " 92%|█████████▏| 459/500 [08:03<00:43,  1.06s/it]INFO:root:global_step: 459, logpy: 10.442, kl: 34.219, loss: 23.703\n",
      " 92%|█████████▏| 460/500 [08:04<00:41,  1.03s/it]INFO:root:global_step: 460, logpy: 10.517, kl: 34.245, loss: 23.655\n",
      " 92%|█████████▏| 461/500 [08:05<00:39,  1.00s/it]INFO:root:global_step: 461, logpy: 10.583, kl: 34.280, loss: 23.624\n",
      " 92%|█████████▏| 462/500 [08:06<00:37,  1.02it/s]INFO:root:global_step: 462, logpy: 10.643, kl: 34.297, loss: 23.582\n",
      " 93%|█████████▎| 463/500 [08:07<00:36,  1.02it/s]INFO:root:global_step: 463, logpy: 10.705, kl: 34.317, loss: 23.540\n",
      " 93%|█████████▎| 464/500 [08:08<00:35,  1.03it/s]INFO:root:global_step: 464, logpy: 10.759, kl: 34.327, loss: 23.497\n",
      " 93%|█████████▎| 465/500 [08:09<00:34,  1.01it/s]INFO:root:global_step: 465, logpy: 10.814, kl: 34.346, loss: 23.462\n",
      " 93%|█████████▎| 466/500 [08:10<00:33,  1.02it/s]INFO:root:global_step: 466, logpy: 10.870, kl: 34.364, loss: 23.425\n",
      " 93%|█████████▎| 467/500 [08:11<00:32,  1.03it/s]INFO:root:global_step: 467, logpy: 10.933, kl: 34.386, loss: 23.385\n",
      " 94%|█████████▎| 468/500 [08:11<00:30,  1.04it/s]INFO:root:global_step: 468, logpy: 10.996, kl: 34.408, loss: 23.344\n",
      " 94%|█████████▍| 469/500 [08:12<00:29,  1.05it/s]INFO:root:global_step: 469, logpy: 11.051, kl: 34.434, loss: 23.315\n",
      " 94%|█████████▍| 470/500 [08:13<00:28,  1.05it/s]INFO:root:global_step: 470, logpy: 11.114, kl: 34.456, loss: 23.275\n",
      " 94%|█████████▍| 471/500 [08:14<00:27,  1.04it/s]INFO:root:global_step: 471, logpy: 11.177, kl: 34.480, loss: 23.237\n",
      " 94%|█████████▍| 472/500 [08:15<00:26,  1.05it/s]INFO:root:global_step: 472, logpy: 11.240, kl: 34.519, loss: 23.214\n",
      " 95%|█████████▍| 473/500 [08:16<00:25,  1.06it/s]INFO:root:global_step: 473, logpy: 11.295, kl: 34.540, loss: 23.181\n",
      " 95%|█████████▍| 474/500 [08:17<00:25,  1.04it/s]INFO:root:global_step: 474, logpy: 11.350, kl: 34.559, loss: 23.145\n",
      " 95%|█████████▌| 475/500 [08:18<00:23,  1.05it/s]INFO:root:global_step: 475, logpy: 11.406, kl: 34.583, loss: 23.114\n",
      " 95%|█████████▌| 476/500 [08:19<00:22,  1.05it/s]INFO:root:global_step: 476, logpy: 11.461, kl: 34.602, loss: 23.078\n",
      " 95%|█████████▌| 477/500 [08:20<00:21,  1.06it/s]INFO:root:global_step: 477, logpy: 11.509, kl: 34.619, loss: 23.048\n",
      " 96%|█████████▌| 478/500 [08:21<00:20,  1.06it/s]INFO:root:global_step: 478, logpy: 11.557, kl: 34.635, loss: 23.016\n",
      " 96%|█████████▌| 479/500 [08:22<00:19,  1.07it/s]INFO:root:global_step: 479, logpy: 11.625, kl: 34.650, loss: 22.964\n",
      " 96%|█████████▌| 480/500 [08:23<00:18,  1.07it/s]INFO:root:global_step: 480, logpy: 11.689, kl: 34.670, loss: 22.921\n",
      " 96%|█████████▌| 481/500 [08:24<00:17,  1.07it/s]INFO:root:global_step: 481, logpy: 11.737, kl: 34.701, loss: 22.904\n",
      " 96%|█████████▋| 482/500 [08:25<00:16,  1.07it/s]INFO:root:global_step: 482, logpy: 11.793, kl: 34.715, loss: 22.862\n",
      " 97%|█████████▋| 483/500 [08:26<00:16,  1.05it/s]INFO:root:global_step: 483, logpy: 11.840, kl: 34.734, loss: 22.836\n",
      " 97%|█████████▋| 484/500 [08:27<00:15,  1.06it/s]INFO:root:global_step: 484, logpy: 11.884, kl: 34.746, loss: 22.804\n",
      " 97%|█████████▋| 485/500 [08:27<00:14,  1.07it/s]INFO:root:global_step: 485, logpy: 11.930, kl: 34.753, loss: 22.766\n",
      " 97%|█████████▋| 486/500 [08:28<00:13,  1.07it/s]INFO:root:global_step: 486, logpy: 11.977, kl: 34.772, loss: 22.738\n",
      " 97%|█████████▋| 487/500 [08:29<00:12,  1.06it/s]INFO:root:global_step: 487, logpy: 12.025, kl: 34.786, loss: 22.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 488/500 [08:30<00:11,  1.07it/s]INFO:root:global_step: 488, logpy: 12.073, kl: 34.809, loss: 22.681\n",
      " 98%|█████████▊| 489/500 [08:31<00:10,  1.07it/s]INFO:root:global_step: 489, logpy: 12.121, kl: 34.823, loss: 22.647\n",
      " 98%|█████████▊| 490/500 [08:32<00:09,  1.07it/s]INFO:root:global_step: 490, logpy: 12.183, kl: 34.833, loss: 22.596\n",
      " 98%|█████████▊| 491/500 [08:33<00:08,  1.07it/s]INFO:root:global_step: 491, logpy: 12.236, kl: 34.837, loss: 22.547\n",
      " 98%|█████████▊| 492/500 [08:34<00:07,  1.05it/s]INFO:root:global_step: 492, logpy: 12.277, kl: 34.848, loss: 22.518\n",
      " 99%|█████████▊| 493/500 [08:35<00:06,  1.06it/s]INFO:root:global_step: 493, logpy: 12.329, kl: 34.871, loss: 22.489\n",
      " 99%|█████████▉| 494/500 [08:36<00:05,  1.07it/s]INFO:root:global_step: 494, logpy: 12.378, kl: 34.889, loss: 22.459\n",
      " 99%|█████████▉| 495/500 [08:37<00:04,  1.07it/s]INFO:root:global_step: 495, logpy: 12.422, kl: 34.923, loss: 22.449\n",
      " 99%|█████████▉| 496/500 [08:38<00:03,  1.07it/s]INFO:root:global_step: 496, logpy: 12.458, kl: 34.933, loss: 22.423\n",
      " 99%|█████████▉| 497/500 [08:39<00:02,  1.07it/s]INFO:root:global_step: 497, logpy: 12.505, kl: 34.944, loss: 22.388\n",
      "100%|█████████▉| 498/500 [08:40<00:01,  1.08it/s]INFO:root:global_step: 498, logpy: 12.546, kl: 34.958, loss: 22.361\n",
      "100%|█████████▉| 499/500 [08:41<00:00,  1.08it/s]INFO:root:global_step: 499, logpy: 12.585, kl: 34.970, loss: 22.335\n",
      "100%|██████████| 500/500 [08:42<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "manual_seed(0)\n",
    "\n",
    "if args[\"debug\"]:\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "ckpt_dir = os.path.join('./', 'ckpts')\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "sdeint_fn = torchsde.sdeint_adjoint if args[\"adjoint\"] else torchsde.sdeint\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bf36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8968a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
