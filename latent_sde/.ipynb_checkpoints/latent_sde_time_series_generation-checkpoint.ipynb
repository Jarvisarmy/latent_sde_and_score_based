{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4dd7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from typing import Optional, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import distributions, nn, optim\n",
    "\n",
    "import torchsde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0aeebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the gpu is available or not, if yes, use gpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2902c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the tuple for data, \n",
    "Data = namedtuple('Data', ['ts_', 'ts_ext_', 'ts_vis_', 'ts', 'ts_ext', 'ts_vis', 'ys', 'ys_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9198a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"train_iters\": 1000,\n",
    "    \"pause_iters\": 50,\n",
    "    \"hide_ticks\": False,\n",
    "    \"save_ckpt\":True,\n",
    "    \"likelihood\":\"laplace\",\n",
    "    \"scale\": 0.001,\n",
    "    \"adjoint\": True,\n",
    "    \"debug\": True,\n",
    "    \"seed\": 42,\n",
    "    'data':'segmented_cosine',\n",
    "    \"dt\": 1e-2,\n",
    "    \"batch_size\": 256,\n",
    "    \"method\": 'euler',\n",
    "    \"adaptive\": 'False',\n",
    "    \"rtol\": 1e-3,\n",
    "    \"atol\": 1e-3\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40420fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output values from 0 to maxval with iters steps\n",
    "class LinearScheduler(object):\n",
    "    def __init__(self, iters, maxval=1.0):\n",
    "        self._iters = max(1,iters)\n",
    "        self._val = maxval/self._iters\n",
    "        self._maxval = maxval\n",
    "    \n",
    "    def step(self):\n",
    "        self._val = min(self._maxval, self._val+self._maxval/self._iters)\n",
    "        \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93c09c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMAMetric(object):\n",
    "    def __init__(self, gamma: Optional[float]=0.99):\n",
    "        super(EMAMetric, self).__init__()\n",
    "        self._val=0\n",
    "        self._gamma = gamma\n",
    "    def step(self, x:Union[torch.Tensor, np.ndarray]):\n",
    "        x = x.detach().cpu().numpy() if torch.is_tensor(x) else x\n",
    "        self._val = self._gamma * self._val + (1-self._gamma)*x\n",
    "        return self._val\n",
    "    \n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a4ec86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "def manual_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98a2f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the dvision is stable\n",
    "def _stable_division(a,b,epsilon=1e-7):\n",
    "    b = torch.where(b.abs().detach() > epsilon, b, torch.full_like(b, fill_value=epsilon)*b.sign())\n",
    "    return a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522d3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentSDE(torchsde.SDEIto): # sde with ito calculus\n",
    "    def __init__(self, theta=1.0, mu=0.0, sigma=0.01):\n",
    "        super(LatentSDE, self).__init__(noise_type=\"diagonal\")\n",
    "        logvar = math.log(sigma ** 2/(2.*theta))\n",
    "        \n",
    "        # prior drift\n",
    "        self.register_buffer(\"theta\",torch.tensor([[theta]])) # prior parameters, register 成buffer, 参数不会进行更新\n",
    "        self.register_buffer(\"mu\", torch.tensor([[mu]]))\n",
    "        self.register_buffer(\"sigma\",torch.tensor([[sigma]]))\n",
    "        \n",
    "        # p(y0)\n",
    "        self.register_buffer(\"py0_mean\", torch.tensor([[mu]]))\n",
    "        self.register_buffer(\"py0_logvar\", torch.tensor([[logvar]]))\n",
    "        \n",
    "        # approximate posterior drift: Takes in 2 positional encodings and the state\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3,200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200,200),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(200,1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Initialization the parameters\n",
    "        self.net[-1].weight.data.fill_(0.) # 初始化最后一层的参数\n",
    "        self.net[-1].bias.data.fill_(0.)\n",
    "            \n",
    "        # q(y0)\n",
    "        self.qy0_mean = nn.Parameter(torch.tensor([[mu]]), requires_grad=True) # 创建parameters\n",
    "        self.qy0_logvar = nn.Parameter(torch.tensor([[logvar]]), requires_grad=True) # 创建parameters\n",
    "        #self.theta = nn.Parameter(torch.tensor([[theta]]),requires_grad=True)\n",
    "        #self.sigma = nn.Parameter(torch.tensor([[sigma]]),requires_grad=True)\n",
    "        \n",
    "        #self.theta = nn.Parameter(torch.tensor([[theta]]),requires_grad=True)\n",
    "            \n",
    "    def f(self, t, y):  # Approximate posterior drift.\n",
    "        if t.dim() == 0:\n",
    "            t = torch.full_like(y, fill_value=t) # create a tensor of t\n",
    "        # Positional encoding in transformers for time-inhomogeneous posterior.\n",
    "        return self.net(torch.cat((torch.sin(t), torch.cos(t), y), dim=-1))\n",
    "\n",
    "    def g(self, t, y):  # Shared diffusion.\n",
    "        return self.sigma.repeat(y.size(0), 1) # 重复复制, 创建一个size为[y.size[0],1]\n",
    "\n",
    "    def h(self, t, y):  # Prior drift.\n",
    "        return self.theta * (self.mu - y)\n",
    "\n",
    "    def f_aug(self, t, y):  # Drift for augmented dynamics with logqp term.\n",
    "        y = y[:, 0:1] # 提取第一列，保持列的形态\n",
    "        f, g, h = self.f(t, y), self.g(t, y), self.h(t, y)\n",
    "        u = _stable_division(f - h, g) # 计算u(z,t)\n",
    "        f_logqp = .5 * (u ** 2).sum(dim=1, keepdim=True) # 计算integral\n",
    "        return torch.cat([f, f_logqp], dim=1)\n",
    "\n",
    "    def g_aug(self, t, y):  # Diffusion for augmented dynamics with logqp term.\n",
    "        y = y[:, 0:1]\n",
    "        g = self.g(t, y)\n",
    "        g_logqp = torch.zeros_like(y)\n",
    "        return torch.cat([g, g_logqp], dim=1)\n",
    "\n",
    "    def forward(self, ts, batch_size, eps=None):\n",
    "        eps = torch.randn(batch_size, 1).to(self.qy0_std) if eps is None else eps\n",
    "        y0 = self.qy0_mean + eps * self.qy0_std # the latent variable\n",
    "        qy0 = distributions.Normal(loc=self.qy0_mean, scale=self.qy0_std) # approximate posterior distribution\n",
    "        py0 = distributions.Normal(loc=self.py0_mean, scale=self.py0_std) # prior distribution\n",
    "        logqp0 = distributions.kl_divergence(qy0, py0).sum(dim=1)  # KL(t=0). calculate the kl divergence\n",
    "        #print(y0.size()) # (256, 1)\n",
    "        aug_y0 = torch.cat([y0, torch.zeros(batch_size, 1).to(y0)], dim=1) # create the augmented initial value\n",
    "        #print(aug_y0.size()) # [256, 2]\n",
    "        aug_ys = sdeint_fn(\n",
    "            sde=self,\n",
    "            y0=aug_y0,\n",
    "            ts=ts,\n",
    "            method=args['method'],\n",
    "            dt=args['dt'],\n",
    "            adaptive=args['adaptive'],\n",
    "            rtol=args['rtol'],\n",
    "            atol=args['atol'],\n",
    "            names={'drift': 'f_aug', 'diffusion': 'g_aug'}\n",
    "        ) # call the sde solver to \n",
    "        # print(aug_ys.size()) # [22, 256, 2]\n",
    "        ys, logqp_path = aug_ys[:, :, 0:1], aug_ys[-1, :, 1] # get the integral of the u(z,t) at the last time\n",
    "        \n",
    "        logqp = (logqp0 + logqp_path).mean(dim=0)  # KL(t=0) + KL(path).\n",
    "        return ys, logqp\n",
    "\n",
    "    def sample_p(self, ts, batch_size, eps=None, bm=None):\n",
    "        eps = torch.randn(batch_size, 1).to(self.py0_mean) if eps is None else eps\n",
    "        y0 = self.py0_mean + eps * self.py0_std\n",
    "        return sdeint_fn(self, y0, ts, bm=bm, method=args['method'], dt=args['dt'], names={'drift': 'h'}) # prior sde\n",
    "\n",
    "    def sample_q(self, ts, batch_size, eps=None, bm=None):\n",
    "        eps = torch.randn(batch_size, 1).to(self.qy0_mean) if eps is None else eps\n",
    "        y0 = self.qy0_mean + eps * self.qy0_std\n",
    "        return sdeint_fn(self, y0, ts, bm=bm, method=args['method'], dt=args['dt']) # posterior sde\n",
    "\n",
    "    @property\n",
    "    def py0_std(self):\n",
    "        return torch.exp(.5 * self.py0_logvar)\n",
    "\n",
    "    @property\n",
    "    def qy0_std(self):\n",
    "        return torch.exp(.5 * self.qy0_logvar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80736fff",
   "metadata": {},
   "source": [
    "## 5. Simulation of time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd15aaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "T = 120\n",
    "#beta = 0.1694\n",
    "beta=1\n",
    "phi = 0.9\n",
    "sigma_v = 0.003342\n",
    "u_over_v = 1\n",
    "rho = -0.856\n",
    "mu = 0\n",
    "def make_time_series():\n",
    "    \n",
    "    \n",
    "    noise_mu = [0, 0]\n",
    "    sigma_u = u_over_v * sigma_v\n",
    "    cov_uv = rho*sigma_u*sigma_v\n",
    "    cov = [[sigma_u**2, cov_uv], [cov_uv, sigma_v**2]]\n",
    "    \n",
    "    shocks = np.random.multivariate_normal(noise_mu, cov, T) # 1st column is u; 2nd columne is v\n",
    "    z0 = np.random.normal(mu, sigma_u ** 2/(1-phi**2), 1)\n",
    "    r0 = shocks[0][0]\n",
    "\n",
    "    z = np.zeros(T)\n",
    "    r = np.zeros(T)\n",
    "    z[0] = z0\n",
    "    r[0] = r0\n",
    "\n",
    "    for idx_t in range(T-1):\n",
    "        z[idx_t+1] = phi*(z[idx_t]-mu) + shocks[idx_t+1][1] + mu\n",
    "        r[idx_t+1] = beta*z[idx_t+1] + shocks[idx_t+1][0]\n",
    "    return r, z\n",
    "r,z = make_time_series()\n",
    "plt.figure(figsize=(15,5))\n",
    "xvalues = np.array(range(T))\n",
    "plt.plot(xvalues, r, linestyle='-', color='g', label=\"observed $r_t$\")\n",
    "plt.plot(xvalues, z, linestyle=\"--\", color=\"r\", label=r\"hidden variable $\\beta * z_t$\", linewidth=3.0)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('%')\n",
    "plt.grid(True)\n",
    "plt.title(r\"Simulated $r_t$ and hidden $\\beta * z_t$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cf9fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    ts_ = np.linspace(0.1,3.0,120)\n",
    "    ts_ext_ = np.array([0.] + list(ts_) + [3.1])\n",
    "    ts_vis_ = np.linspace(0.1, 3.1, 121)\n",
    "    ys_ = r[:,None]\n",
    "    ts = torch.tensor(ts_).float()\n",
    "    ts_ext = torch.tensor(ts_ext_).float()\n",
    "    ts_vis = torch.tensor(ts_vis_).float()\n",
    "    ys = torch.tensor(ys_).float().to(device)\n",
    "    return Data(ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99a1fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Dataset\n",
    "    ts_, ts_ext_, ts_vis_, ts, ts_ext, ts_vis, ys, ys_ = make_data()\n",
    "    mu = torch.mean(ys)\n",
    "    sigma = torch.std(ys)\n",
    "    # plotting parameters\n",
    "    vis_batch_size = 1024\n",
    "    ylims = (-0.03, 0.03)\n",
    "    alphas = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55]\n",
    "    percentiles = [0.999, 0.99, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "    vis_idx = np.random.permutation(vis_batch_size)\n",
    "    sample_colors = ('#8c96c6', '#8c6bb1', '#810f7c')\n",
    "    fill_color = '#9ebcda'\n",
    "    mean_color = '#4d004b'\n",
    "    num_samples = len(sample_colors)\n",
    "    \n",
    "    eps = torch.randn(vis_batch_size, 1).to(device)\n",
    "    bm = torchsde.BrownianInterval(\n",
    "        t0=ts_vis[0],\n",
    "        t1=ts_vis[-1],\n",
    "        size=(vis_batch_size,1),\n",
    "        device=device,\n",
    "        levy_area_approximation=\"space-time\")\n",
    "    \n",
    "    # Model\n",
    "    model = LatentSDE(mu=mu,sigma=sigma).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    kl_scheduler = LinearScheduler(iters=500)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    logpy_metric = EMAMetric()\n",
    "    kl_metric = EMAMetric()\n",
    "    loss_metric = EMAMetric()\n",
    "    \n",
    "    \n",
    "    # show prior\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        zs = model.sample_p(ts=ts_vis, batch_size = vis_batch_size, eps = eps, bm=bm).squeeze()\n",
    "       \n",
    "        ts_vis_, zs_ = ts_vis.cpu().numpy(), zs.cpu().numpy()\n",
    "        #plt.scatter(ts_vis_, ys_, color='r', label=\"prior\")\n",
    "        zs_ = np.sort(zs_,axis=1)\n",
    "        img_dir = os.path.join('./img_generation/','prior.png')\n",
    "        plt.subplot(frameon=False)\n",
    "        for alpha, percentile in zip(alphas, percentiles):\n",
    "            idx = int((1 - percentile) / 2. * vis_batch_size)\n",
    "            zs_bot_ = zs_[:, idx]\n",
    "            zs_top_ = zs_[:, -idx]\n",
    "            plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
    "\n",
    "        # `zorder` determines who's on top; the larger the more at the top.\n",
    "        \n",
    "        plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # Data.\n",
    "        plt.plot(ts_, ys_, marker='x', zorder=3, color='k',label=\"observed $r_t$\")\n",
    "        plt.plot(ts_, z[:,None], color='g', linewidth=3.0, label=r\"hidden variable $z_t$\")\n",
    "        plt.ylim(ylims)\n",
    "        plt.xlabel('$t$')\n",
    "        plt.ylabel('$Y_t$')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "        plt.savefig(img_dir, dpi=300)\n",
    "        plt.close()\n",
    "        logging.info(f'Saved prior figure at: {img_dir}')\n",
    "    \n",
    "    \n",
    "    for global_step in tqdm.tqdm(range(args['train_iters'])):\n",
    "        \n",
    "        # Plot and save.\n",
    "        if global_step % args['pause_iters'] == 0:\n",
    "            img_path = os.path.join(\"./img_generation/\", f'global_step_{global_step}.png')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                zs = model.sample_q(ts=ts_vis, batch_size=vis_batch_size, eps=eps, bm=bm).squeeze()\n",
    "                samples = zs[:, vis_idx]\n",
    "                ts_vis_, zs_, samples_ = ts_vis.cpu().numpy(), zs.cpu().numpy(), samples.cpu().numpy()\n",
    "                zs_ = np.sort(zs_, axis=1)\n",
    "                plt.subplot(frameon=False)\n",
    "\n",
    "                if True: # args.show_percentiles:\n",
    "                    for alpha, percentile in zip(alphas, percentiles):\n",
    "                        idx = int((1 - percentile) / 2. * vis_batch_size)\n",
    "                        zs_bot_, zs_top_ = zs_[:, idx], zs_[:, -idx]\n",
    "                        plt.fill_between(ts_vis_, zs_bot_, zs_top_, alpha=alpha, color=fill_color)\n",
    "\n",
    "                if False: #args.show_mean:\n",
    "                    plt.plot(ts_vis_, zs_.mean(axis=1), color=mean_color)\n",
    "\n",
    "                if True: #args.show_samples:\n",
    "                    #for j in range(num_samples):\n",
    "                    #    plt.plot(ts_vis_, samples_[:, j], color=sample_colors[j], linewidth=1.0)\n",
    "                    # plt.plot(ts_, z[:,None], color='g', linewidth=3.0, label=r\"hidden variable $z_t$\")\n",
    "                    plt.plot(ts_vis_, samples_.mean(axis=1), marker='o', color='r',label=r'mean of latent variables')\n",
    "\n",
    "                if True: #args.show_arrows:\n",
    "                    num, dt = 3, 0.12\n",
    "                    t, y = torch.meshgrid(\n",
    "                        [torch.linspace(0, 3, num).to(device), torch.linspace(-0.3, 0.3, num).to(device)]\n",
    "                    )\n",
    "                    t, y = t.reshape(-1, 1), y.reshape(-1, 1)\n",
    "                    fty = model.f(t=t, y=y).reshape(num, num)\n",
    "                    dt = torch.zeros(num, num).fill_(dt).to(device)\n",
    "                    dy = fty * dt\n",
    "                    dt_, dy_, t_, y_ = dt.cpu().numpy(), dy.cpu().numpy(), t.cpu().numpy(), y.cpu().numpy()\n",
    "                    plt.quiver(t_, y_, dt_, dy_, alpha=0.3, edgecolors='k', width=0.0035, scale=50)\n",
    "\n",
    "                if False: #args.hide_ticks:\n",
    "                    plt.xticks([], [])\n",
    "                    plt.yticks([], [])\n",
    "\n",
    "                #plt.scatter(ts_, ys_, marker='x', zorder=3, color='k', s=35)  # Data.\n",
    "                plt.plot(ts_, ys_, linestyle=\"-\",color='g', zorder=3, label=\"observed $r_t$ \") # new added\n",
    "                \n",
    "                for j in range(num_samples):\n",
    "                    zs = samples[:,j:j+1]\n",
    "                    likelihood_constructor = {\"laplace\": distributions.Laplace, \"normal\": distributions.Normal}[args['likelihood']]\n",
    "                    likelihood = likelihood_constructor(loc=zs, scale=args['scale'])\n",
    "                    reconstruct = likelihood.sample()\n",
    "                    print(reconstruct.size())\n",
    "                    plt.plot(ts_vis_, reconstruct, color=sample_colors[j], linewidth=1.0,label=r'estimated observations')\n",
    "                \n",
    "\n",
    "                \n",
    "                plt.ylim(ylims)\n",
    "                plt.xlabel('$t$')\n",
    "                plt.ylabel('$Y_t$')\n",
    "                plt.tight_layout()\n",
    "                plt.legend()\n",
    "                plt.savefig(img_path, dpi=300)\n",
    "                plt.close()\n",
    "                logging.info(f'Saved figure at: {img_path}')\n",
    "\n",
    "                \n",
    "        \n",
    "        # Train.\n",
    "        optimizer.zero_grad() # zero the gradient\n",
    "        zs, kl = model(ts=ts_ext, batch_size=args['batch_size']) # pass through the model, ys, logqp\n",
    "        zs = zs.squeeze() # remove the dimensions of input of size 1\n",
    "        zs = zs[1:-1]  # Drop first and last which are only used to penalize out-of-data region and spread uncertainty.\n",
    "        likelihood_constructor = {\"laplace\": distributions.Laplace, \"normal\": distributions.Normal}[args['likelihood']]\n",
    "        likelihood = likelihood_constructor(loc=zs, scale=args['scale']) # create the laplace distribution\n",
    "        logpy = likelihood.log_prob(ys).sum(dim=0).mean(dim=0)\n",
    "        loss = -logpy + kl * kl_scheduler.val\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        kl_scheduler.step()\n",
    "\n",
    "        logpy_metric.step(logpy)\n",
    "        kl_metric.step(kl)\n",
    "        loss_metric.step(loss)\n",
    "\n",
    "        logging.info(\n",
    "            f'global_step: {global_step}, '\n",
    "            f'logpy: {logpy_metric.val:.3f}, '\n",
    "            f'kl: {kl_metric.val:.3f}, '\n",
    "            f'loss: {loss_metric.val:.3f}'\n",
    "        )\n",
    "    torch.save(\n",
    "        {'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'kl_scheduler': kl_scheduler},\n",
    "        os.path.join('./sim/', f'global_step_{global_step}.ckpt')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "74b55c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved prior figure at: ./img_generation/prior.png\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_0.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 0, logpy: -18.315, kl: 0.030, loss: 18.315\n",
      "  0%|          | 1/1000 [00:02<47:56,  2.88s/it]INFO:root:global_step: 1, logpy: -585.703, kl: 22.858, loss: 585.795\n",
      "  0%|          | 2/1000 [00:04<40:47,  2.45s/it]INFO:root:global_step: 2, logpy: -1171.891, kl: 50.262, loss: 1172.147\n",
      "  0%|          | 3/1000 [00:05<35:54,  2.16s/it]INFO:root:global_step: 3, logpy: -1357.197, kl: 52.687, loss: 1357.474\n",
      "  0%|          | 4/1000 [00:07<32:24,  1.95s/it]INFO:root:global_step: 4, logpy: -1396.050, kl: 52.503, loss: 1396.328\n",
      "  0%|          | 5/1000 [00:08<30:34,  1.84s/it]INFO:root:global_step: 5, logpy: -1679.736, kl: 59.525, loss: 1680.102\n",
      "  1%|          | 6/1000 [00:10<29:10,  1.76s/it]INFO:root:global_step: 6, logpy: -1693.857, kl: 59.009, loss: 1694.220\n",
      "  1%|          | 7/1000 [00:11<27:59,  1.69s/it]INFO:root:global_step: 7, logpy: -1700.471, kl: 58.572, loss: 1700.833\n",
      "  1%|          | 8/1000 [00:13<26:52,  1.63s/it]INFO:root:global_step: 8, logpy: -1741.059, kl: 58.223, loss: 1741.421\n",
      "  1%|          | 9/1000 [00:14<26:17,  1.59s/it]INFO:root:global_step: 9, logpy: -1753.613, kl: 57.740, loss: 1753.974\n",
      "  1%|          | 10/1000 [00:16<25:10,  1.53s/it]INFO:root:global_step: 10, logpy: -1760.657, kl: 57.291, loss: 1761.017\n",
      "  1%|          | 11/1000 [00:17<24:32,  1.49s/it]INFO:root:global_step: 11, logpy: -1752.690, kl: 56.778, loss: 1753.048\n",
      "  1%|          | 12/1000 [00:19<23:47,  1.44s/it]INFO:root:global_step: 12, logpy: -1759.942, kl: 56.339, loss: 1760.300\n",
      "  1%|▏         | 13/1000 [00:20<23:32,  1.43s/it]INFO:root:global_step: 13, logpy: -1759.934, kl: 55.859, loss: 1760.290\n",
      "  1%|▏         | 14/1000 [00:21<23:50,  1.45s/it]INFO:root:global_step: 14, logpy: -1760.473, kl: 55.381, loss: 1760.829\n",
      "  2%|▏         | 15/1000 [00:23<24:09,  1.47s/it]INFO:root:global_step: 15, logpy: -1754.727, kl: 54.907, loss: 1755.081\n",
      "  2%|▏         | 16/1000 [00:24<24:11,  1.47s/it]INFO:root:global_step: 16, logpy: -1743.816, kl: 54.425, loss: 1744.169\n",
      "  2%|▏         | 17/1000 [00:26<24:13,  1.48s/it]INFO:root:global_step: 17, logpy: -1742.164, kl: 53.995, loss: 1742.518\n",
      "  2%|▏         | 18/1000 [00:27<24:18,  1.49s/it]INFO:root:global_step: 18, logpy: -1731.306, kl: 53.521, loss: 1731.659\n",
      "  2%|▏         | 19/1000 [00:29<24:25,  1.49s/it]INFO:root:global_step: 19, logpy: -1729.755, kl: 53.064, loss: 1730.107\n",
      "  2%|▏         | 20/1000 [00:31<25:03,  1.53s/it]INFO:root:global_step: 20, logpy: -1725.127, kl: 52.600, loss: 1725.478\n",
      "  2%|▏         | 21/1000 [00:32<26:29,  1.62s/it]INFO:root:global_step: 21, logpy: -1720.357, kl: 52.111, loss: 1720.707\n",
      "  2%|▏         | 22/1000 [00:34<26:47,  1.64s/it]INFO:root:global_step: 22, logpy: -1709.881, kl: 51.624, loss: 1710.228\n",
      "  2%|▏         | 23/1000 [00:36<26:54,  1.65s/it]INFO:root:global_step: 23, logpy: -1705.763, kl: 51.184, loss: 1706.111\n",
      "  2%|▏         | 24/1000 [00:37<26:35,  1.64s/it]INFO:root:global_step: 24, logpy: -1699.013, kl: 50.764, loss: 1699.362\n",
      "  2%|▎         | 25/1000 [00:39<26:07,  1.61s/it]INFO:root:global_step: 25, logpy: -1692.735, kl: 50.350, loss: 1693.085\n",
      "  3%|▎         | 26/1000 [00:41<26:20,  1.62s/it]INFO:root:global_step: 26, logpy: -1686.478, kl: 49.911, loss: 1686.828\n",
      "  3%|▎         | 27/1000 [00:42<26:52,  1.66s/it]INFO:root:global_step: 27, logpy: -1674.131, kl: 49.450, loss: 1674.480\n",
      "  3%|▎         | 28/1000 [00:44<27:23,  1.69s/it]INFO:root:global_step: 28, logpy: -1666.859, kl: 49.009, loss: 1667.208\n",
      "  3%|▎         | 29/1000 [00:46<27:48,  1.72s/it]INFO:root:global_step: 29, logpy: -1652.438, kl: 48.561, loss: 1652.786\n",
      "  3%|▎         | 30/1000 [00:48<27:50,  1.72s/it]INFO:root:global_step: 30, logpy: -1644.560, kl: 48.148, loss: 1644.908\n",
      "  3%|▎         | 31/1000 [00:49<27:53,  1.73s/it]INFO:root:global_step: 31, logpy: -1634.108, kl: 47.749, loss: 1634.458\n",
      "  3%|▎         | 32/1000 [00:51<27:50,  1.73s/it]INFO:root:global_step: 32, logpy: -1623.257, kl: 47.338, loss: 1623.609\n",
      "  3%|▎         | 33/1000 [00:53<27:39,  1.72s/it]INFO:root:global_step: 33, logpy: -1611.988, kl: 46.915, loss: 1612.339\n",
      "  3%|▎         | 34/1000 [00:55<27:58,  1.74s/it]INFO:root:global_step: 34, logpy: -1600.586, kl: 46.489, loss: 1600.937\n",
      "  4%|▎         | 35/1000 [00:56<28:26,  1.77s/it]INFO:root:global_step: 35, logpy: -1589.443, kl: 46.071, loss: 1589.794\n",
      "  4%|▎         | 36/1000 [00:58<28:43,  1.79s/it]INFO:root:global_step: 36, logpy: -1576.756, kl: 45.669, loss: 1577.107\n",
      "  4%|▎         | 37/1000 [01:00<28:55,  1.80s/it]INFO:root:global_step: 37, logpy: -1565.161, kl: 45.284, loss: 1565.515\n",
      "  4%|▍         | 38/1000 [01:02<28:42,  1.79s/it]INFO:root:global_step: 38, logpy: -1552.537, kl: 44.908, loss: 1552.893\n",
      "  4%|▍         | 39/1000 [01:04<28:33,  1.78s/it]INFO:root:global_step: 39, logpy: -1540.233, kl: 44.526, loss: 1540.590\n",
      "  4%|▍         | 40/1000 [01:06<29:21,  1.84s/it]INFO:root:global_step: 40, logpy: -1527.575, kl: 44.137, loss: 1527.934\n",
      "  4%|▍         | 41/1000 [01:07<29:35,  1.85s/it]INFO:root:global_step: 41, logpy: -1515.446, kl: 43.753, loss: 1515.806\n",
      "  4%|▍         | 42/1000 [01:09<29:54,  1.87s/it]INFO:root:global_step: 42, logpy: -1501.808, kl: 43.376, loss: 1502.170\n",
      "  4%|▍         | 43/1000 [01:11<29:51,  1.87s/it]INFO:root:global_step: 43, logpy: -1489.420, kl: 43.014, loss: 1489.784\n",
      "  4%|▍         | 44/1000 [01:13<29:25,  1.85s/it]INFO:root:global_step: 44, logpy: -1476.109, kl: 42.654, loss: 1476.476\n",
      "  4%|▍         | 45/1000 [01:15<29:23,  1.85s/it]INFO:root:global_step: 45, logpy: -1463.395, kl: 42.288, loss: 1463.764\n",
      "  5%|▍         | 46/1000 [01:17<29:28,  1.85s/it]INFO:root:global_step: 46, logpy: -1450.003, kl: 41.921, loss: 1450.374\n",
      "  5%|▍         | 47/1000 [01:19<29:39,  1.87s/it]INFO:root:global_step: 47, logpy: -1437.437, kl: 41.556, loss: 1437.808\n",
      "  5%|▍         | 48/1000 [01:21<29:52,  1.88s/it]INFO:root:global_step: 48, logpy: -1424.192, kl: 41.199, loss: 1424.566\n",
      "  5%|▍         | 49/1000 [01:22<29:54,  1.89s/it]INFO:root:global_step: 49, logpy: -1411.649, kl: 40.852, loss: 1412.026\n",
      "  5%|▌         | 50/1000 [01:24<29:51,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_50.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 50, logpy: -1398.633, kl: 40.509, loss: 1399.013\n",
      "  5%|▌         | 51/1000 [01:27<35:37,  2.25s/it]INFO:root:global_step: 51, logpy: -1385.997, kl: 40.168, loss: 1386.380\n",
      "  5%|▌         | 52/1000 [01:29<34:03,  2.16s/it]INFO:root:global_step: 52, logpy: -1373.271, kl: 39.826, loss: 1373.656\n",
      "  5%|▌         | 53/1000 [01:31<33:47,  2.14s/it]INFO:root:global_step: 53, logpy: -1360.767, kl: 39.487, loss: 1361.155\n",
      "  5%|▌         | 54/1000 [01:34<33:24,  2.12s/it]INFO:root:global_step: 54, logpy: -1348.292, kl: 39.155, loss: 1348.682\n",
      "  6%|▌         | 55/1000 [01:36<32:38,  2.07s/it]INFO:root:global_step: 55, logpy: -1335.994, kl: 38.835, loss: 1336.388\n",
      "  6%|▌         | 56/1000 [01:37<31:41,  2.01s/it]INFO:root:global_step: 56, logpy: -1323.813, kl: 38.515, loss: 1324.211\n",
      "  6%|▌         | 57/1000 [01:39<31:23,  2.00s/it]INFO:root:global_step: 57, logpy: -1311.634, kl: 38.196, loss: 1312.036\n",
      "  6%|▌         | 58/1000 [01:41<31:08,  1.98s/it]INFO:root:global_step: 58, logpy: -1299.597, kl: 37.874, loss: 1300.002\n",
      "  6%|▌         | 59/1000 [01:43<31:46,  2.03s/it]INFO:root:global_step: 59, logpy: -1287.423, kl: 37.555, loss: 1287.832\n",
      "  6%|▌         | 60/1000 [01:46<32:21,  2.07s/it]INFO:root:global_step: 60, logpy: -1275.611, kl: 37.245, loss: 1276.023\n",
      "  6%|▌         | 61/1000 [01:48<32:16,  2.06s/it]INFO:root:global_step: 61, logpy: -1263.805, kl: 36.941, loss: 1264.221\n",
      "  6%|▌         | 62/1000 [01:50<31:35,  2.02s/it]INFO:root:global_step: 62, logpy: -1252.358, kl: 36.639, loss: 1252.779\n",
      "  6%|▋         | 63/1000 [01:52<31:25,  2.01s/it]INFO:root:global_step: 63, logpy: -1240.625, kl: 36.335, loss: 1241.049\n",
      "  6%|▋         | 64/1000 [01:54<31:40,  2.03s/it]INFO:root:global_step: 64, logpy: -1229.287, kl: 36.030, loss: 1229.715\n",
      "  6%|▋         | 65/1000 [01:56<31:39,  2.03s/it]INFO:root:global_step: 65, logpy: -1217.863, kl: 35.731, loss: 1218.295\n",
      "  7%|▋         | 66/1000 [01:58<31:48,  2.04s/it]INFO:root:global_step: 66, logpy: -1206.610, kl: 35.441, loss: 1207.047\n",
      "  7%|▋         | 67/1000 [02:00<31:37,  2.03s/it]INFO:root:global_step: 67, logpy: -1195.301, kl: 35.151, loss: 1195.742\n",
      "  7%|▋         | 68/1000 [02:02<31:26,  2.02s/it]INFO:root:global_step: 68, logpy: -1184.207, kl: 34.866, loss: 1184.653\n",
      "  7%|▋         | 69/1000 [02:04<31:24,  2.02s/it]INFO:root:global_step: 69, logpy: -1173.210, kl: 34.581, loss: 1173.660\n",
      "  7%|▋         | 70/1000 [02:06<32:03,  2.07s/it]INFO:root:global_step: 70, logpy: -1162.381, kl: 34.301, loss: 1162.836\n",
      "  7%|▋         | 71/1000 [02:08<32:14,  2.08s/it]INFO:root:global_step: 71, logpy: -1151.840, kl: 34.023, loss: 1152.300\n",
      "  7%|▋         | 72/1000 [02:10<32:22,  2.09s/it]INFO:root:global_step: 72, logpy: -1140.953, kl: 33.750, loss: 1141.418\n",
      "  7%|▋         | 73/1000 [02:12<32:08,  2.08s/it]INFO:root:global_step: 73, logpy: -1130.571, kl: 33.482, loss: 1131.041\n",
      "  7%|▋         | 74/1000 [02:14<32:08,  2.08s/it]INFO:root:global_step: 74, logpy: -1119.831, kl: 33.213, loss: 1120.307\n",
      "  8%|▊         | 75/1000 [02:16<32:05,  2.08s/it]INFO:root:global_step: 75, logpy: -1109.355, kl: 32.943, loss: 1109.836\n",
      "  8%|▊         | 76/1000 [02:18<31:44,  2.06s/it]INFO:root:global_step: 76, logpy: -1098.892, kl: 32.679, loss: 1099.378\n",
      "  8%|▊         | 77/1000 [02:20<31:42,  2.06s/it]INFO:root:global_step: 77, logpy: -1088.659, kl: 32.419, loss: 1089.150\n",
      "  8%|▊         | 78/1000 [02:23<31:35,  2.06s/it]INFO:root:global_step: 78, logpy: -1078.491, kl: 32.164, loss: 1078.988\n",
      "  8%|▊         | 79/1000 [02:25<31:19,  2.04s/it]INFO:root:global_step: 79, logpy: -1068.310, kl: 31.912, loss: 1068.813\n",
      "  8%|▊         | 80/1000 [02:27<31:20,  2.04s/it]INFO:root:global_step: 80, logpy: -1058.213, kl: 31.655, loss: 1058.722\n",
      "  8%|▊         | 81/1000 [02:29<31:22,  2.05s/it]INFO:root:global_step: 81, logpy: -1048.140, kl: 31.403, loss: 1048.654\n",
      "  8%|▊         | 82/1000 [02:31<31:17,  2.04s/it]INFO:root:global_step: 82, logpy: -1038.365, kl: 31.157, loss: 1038.885\n",
      "  8%|▊         | 83/1000 [02:33<31:21,  2.05s/it]INFO:root:global_step: 83, logpy: -1028.455, kl: 30.913, loss: 1028.982\n",
      "  8%|▊         | 84/1000 [02:35<31:12,  2.04s/it]INFO:root:global_step: 84, logpy: -1018.753, kl: 30.674, loss: 1019.286\n",
      "  8%|▊         | 85/1000 [02:37<31:28,  2.06s/it]INFO:root:global_step: 85, logpy: -1008.935, kl: 30.432, loss: 1009.474\n",
      "  9%|▊         | 86/1000 [02:39<31:25,  2.06s/it]INFO:root:global_step: 86, logpy: -999.411, kl: 30.195, loss: 999.956\n",
      "  9%|▊         | 87/1000 [02:41<31:26,  2.07s/it]INFO:root:global_step: 87, logpy: -989.863, kl: 29.960, loss: 990.414\n",
      "  9%|▉         | 88/1000 [02:43<31:20,  2.06s/it]INFO:root:global_step: 88, logpy: -980.405, kl: 29.731, loss: 980.964\n",
      "  9%|▉         | 89/1000 [02:45<31:03,  2.05s/it]INFO:root:global_step: 89, logpy: -971.044, kl: 29.503, loss: 971.609\n",
      "  9%|▉         | 90/1000 [02:47<30:59,  2.04s/it]INFO:root:global_step: 90, logpy: -961.650, kl: 29.278, loss: 962.222\n",
      "  9%|▉         | 91/1000 [02:49<31:03,  2.05s/it]INFO:root:global_step: 91, logpy: -952.422, kl: 29.051, loss: 953.001\n",
      "  9%|▉         | 92/1000 [02:51<31:07,  2.06s/it]INFO:root:global_step: 92, logpy: -943.251, kl: 28.829, loss: 943.837\n",
      "  9%|▉         | 93/1000 [02:53<31:08,  2.06s/it]INFO:root:global_step: 93, logpy: -934.159, kl: 28.608, loss: 934.751\n",
      "  9%|▉         | 94/1000 [02:55<31:18,  2.07s/it]INFO:root:global_step: 94, logpy: -925.044, kl: 28.389, loss: 925.643\n",
      " 10%|▉         | 95/1000 [02:58<31:22,  2.08s/it]INFO:root:global_step: 95, logpy: -916.096, kl: 28.171, loss: 916.702\n",
      " 10%|▉         | 96/1000 [03:00<31:23,  2.08s/it]INFO:root:global_step: 96, logpy: -907.286, kl: 27.954, loss: 907.898\n",
      " 10%|▉         | 97/1000 [03:02<31:33,  2.10s/it]INFO:root:global_step: 97, logpy: -898.433, kl: 27.743, loss: 899.053\n",
      " 10%|▉         | 98/1000 [03:04<31:36,  2.10s/it]INFO:root:global_step: 98, logpy: -889.655, kl: 27.536, loss: 890.283\n",
      " 10%|▉         | 99/1000 [03:06<31:47,  2.12s/it]INFO:root:global_step: 99, logpy: -880.992, kl: 27.333, loss: 881.628\n",
      " 10%|█         | 100/1000 [03:08<31:44,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_100.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 100, logpy: -872.371, kl: 27.126, loss: 873.014\n",
      " 10%|█         | 101/1000 [03:11<37:21,  2.49s/it]INFO:root:global_step: 101, logpy: -863.803, kl: 26.919, loss: 864.452\n",
      " 10%|█         | 102/1000 [03:14<35:51,  2.40s/it]INFO:root:global_step: 102, logpy: -855.393, kl: 26.720, loss: 856.050\n",
      " 10%|█         | 103/1000 [03:16<34:43,  2.32s/it]INFO:root:global_step: 103, logpy: -847.124, kl: 26.523, loss: 847.790\n",
      " 10%|█         | 104/1000 [03:18<33:54,  2.27s/it]INFO:root:global_step: 104, logpy: -838.889, kl: 26.327, loss: 839.563\n",
      " 10%|█         | 105/1000 [03:20<33:17,  2.23s/it]INFO:root:global_step: 105, logpy: -830.771, kl: 26.133, loss: 831.452\n",
      " 11%|█         | 106/1000 [03:22<32:47,  2.20s/it]INFO:root:global_step: 106, logpy: -822.571, kl: 25.937, loss: 823.259\n",
      " 11%|█         | 107/1000 [03:24<32:32,  2.19s/it]INFO:root:global_step: 107, logpy: -814.546, kl: 25.745, loss: 815.242\n",
      " 11%|█         | 108/1000 [03:27<32:25,  2.18s/it]INFO:root:global_step: 108, logpy: -806.623, kl: 25.558, loss: 807.327\n",
      " 11%|█         | 109/1000 [03:29<32:23,  2.18s/it]INFO:root:global_step: 109, logpy: -798.766, kl: 25.369, loss: 799.478\n",
      " 11%|█         | 110/1000 [03:31<32:21,  2.18s/it]INFO:root:global_step: 110, logpy: -790.913, kl: 25.180, loss: 791.633\n",
      " 11%|█         | 111/1000 [03:33<32:53,  2.22s/it]INFO:root:global_step: 111, logpy: -783.207, kl: 24.994, loss: 783.934\n",
      " 11%|█         | 112/1000 [03:35<32:49,  2.22s/it]INFO:root:global_step: 112, logpy: -775.486, kl: 24.814, loss: 776.222\n",
      " 11%|█▏        | 113/1000 [03:38<32:38,  2.21s/it]INFO:root:global_step: 113, logpy: -767.872, kl: 24.638, loss: 768.617\n",
      " 11%|█▏        | 114/1000 [03:40<32:31,  2.20s/it]INFO:root:global_step: 114, logpy: -760.272, kl: 24.460, loss: 761.025\n",
      " 12%|█▏        | 115/1000 [03:42<32:51,  2.23s/it]INFO:root:global_step: 115, logpy: -752.864, kl: 24.281, loss: 753.624\n",
      " 12%|█▏        | 116/1000 [03:44<32:57,  2.24s/it]INFO:root:global_step: 116, logpy: -745.432, kl: 24.105, loss: 746.201\n",
      " 12%|█▏        | 117/1000 [03:47<32:52,  2.23s/it]INFO:root:global_step: 117, logpy: -738.036, kl: 23.934, loss: 738.813\n",
      " 12%|█▏        | 118/1000 [03:49<32:44,  2.23s/it]INFO:root:global_step: 118, logpy: -730.777, kl: 23.769, loss: 731.565\n",
      " 12%|█▏        | 119/1000 [03:51<32:32,  2.22s/it]INFO:root:global_step: 119, logpy: -723.477, kl: 23.603, loss: 724.273\n",
      " 12%|█▏        | 120/1000 [03:53<32:35,  2.22s/it]INFO:root:global_step: 120, logpy: -716.234, kl: 23.437, loss: 717.040\n",
      " 12%|█▏        | 121/1000 [03:56<32:54,  2.25s/it]INFO:root:global_step: 121, logpy: -709.067, kl: 23.272, loss: 709.881\n",
      " 12%|█▏        | 122/1000 [03:58<33:22,  2.28s/it]INFO:root:global_step: 122, logpy: -701.968, kl: 23.114, loss: 702.792\n",
      " 12%|█▏        | 123/1000 [04:00<33:24,  2.29s/it]INFO:root:global_step: 123, logpy: -695.028, kl: 22.956, loss: 695.863\n",
      " 12%|█▏        | 124/1000 [04:02<33:26,  2.29s/it]INFO:root:global_step: 124, logpy: -687.988, kl: 22.796, loss: 688.832\n",
      " 12%|█▎        | 125/1000 [04:05<33:17,  2.28s/it]INFO:root:global_step: 125, logpy: -681.081, kl: 22.637, loss: 681.933\n",
      " 13%|█▎        | 126/1000 [04:07<33:09,  2.28s/it]INFO:root:global_step: 126, logpy: -674.234, kl: 22.479, loss: 675.096\n",
      " 13%|█▎        | 127/1000 [04:09<33:03,  2.27s/it]INFO:root:global_step: 127, logpy: -667.437, kl: 22.325, loss: 668.308\n",
      " 13%|█▎        | 128/1000 [04:12<33:12,  2.29s/it]INFO:root:global_step: 128, logpy: -660.721, kl: 22.172, loss: 661.602\n",
      " 13%|█▎        | 129/1000 [04:14<33:15,  2.29s/it]INFO:root:global_step: 129, logpy: -654.007, kl: 22.020, loss: 654.897\n",
      " 13%|█▎        | 130/1000 [04:16<33:28,  2.31s/it]INFO:root:global_step: 130, logpy: -647.335, kl: 21.868, loss: 648.234\n",
      " 13%|█▎        | 131/1000 [04:19<33:35,  2.32s/it]INFO:root:global_step: 131, logpy: -640.736, kl: 21.717, loss: 641.644\n",
      " 13%|█▎        | 132/1000 [04:21<33:26,  2.31s/it]INFO:root:global_step: 132, logpy: -634.214, kl: 21.569, loss: 635.131\n",
      " 13%|█▎        | 133/1000 [04:23<33:21,  2.31s/it]INFO:root:global_step: 133, logpy: -627.828, kl: 21.424, loss: 628.755\n",
      " 13%|█▎        | 134/1000 [04:25<33:08,  2.30s/it]INFO:root:global_step: 134, logpy: -621.400, kl: 21.277, loss: 622.336\n",
      " 14%|█▎        | 135/1000 [04:28<33:16,  2.31s/it]INFO:root:global_step: 135, logpy: -614.967, kl: 21.136, loss: 615.913\n",
      " 14%|█▎        | 136/1000 [04:30<33:32,  2.33s/it]INFO:root:global_step: 136, logpy: -608.669, kl: 20.997, loss: 609.625\n",
      " 14%|█▎        | 137/1000 [04:32<33:19,  2.32s/it]INFO:root:global_step: 137, logpy: -602.398, kl: 20.857, loss: 603.364\n",
      " 14%|█▍        | 138/1000 [04:35<33:19,  2.32s/it]INFO:root:global_step: 138, logpy: -596.236, kl: 20.723, loss: 597.213\n",
      " 14%|█▍        | 139/1000 [04:37<33:19,  2.32s/it]INFO:root:global_step: 139, logpy: -590.075, kl: 20.587, loss: 591.062\n",
      " 14%|█▍        | 140/1000 [04:39<33:28,  2.34s/it]INFO:root:global_step: 140, logpy: -584.072, kl: 20.452, loss: 585.070\n",
      " 14%|█▍        | 141/1000 [04:42<33:26,  2.34s/it]INFO:root:global_step: 141, logpy: -577.958, kl: 20.317, loss: 578.965\n",
      " 14%|█▍        | 142/1000 [04:44<33:22,  2.33s/it]INFO:root:global_step: 142, logpy: -571.985, kl: 20.187, loss: 573.003\n",
      " 14%|█▍        | 143/1000 [04:46<33:15,  2.33s/it]INFO:root:global_step: 143, logpy: -566.048, kl: 20.057, loss: 567.077\n",
      " 14%|█▍        | 144/1000 [04:49<33:17,  2.33s/it]INFO:root:global_step: 144, logpy: -560.116, kl: 19.928, loss: 561.155\n",
      " 14%|█▍        | 145/1000 [04:51<33:25,  2.35s/it]INFO:root:global_step: 145, logpy: -554.309, kl: 19.800, loss: 555.358\n",
      " 15%|█▍        | 146/1000 [04:54<33:27,  2.35s/it]INFO:root:global_step: 146, logpy: -548.498, kl: 19.673, loss: 549.558\n",
      " 15%|█▍        | 147/1000 [04:56<34:00,  2.39s/it]INFO:root:global_step: 147, logpy: -542.813, kl: 19.549, loss: 543.883\n",
      " 15%|█▍        | 148/1000 [04:58<33:43,  2.38s/it]INFO:root:global_step: 148, logpy: -537.159, kl: 19.425, loss: 538.240\n",
      " 15%|█▍        | 149/1000 [05:01<33:32,  2.37s/it]INFO:root:global_step: 149, logpy: -531.509, kl: 19.303, loss: 532.601\n",
      " 15%|█▌        | 150/1000 [05:03<33:31,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_150.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 150, logpy: -525.957, kl: 19.181, loss: 527.060\n",
      " 15%|█▌        | 151/1000 [05:07<38:48,  2.74s/it]INFO:root:global_step: 151, logpy: -520.451, kl: 19.064, loss: 521.566\n",
      " 15%|█▌        | 152/1000 [05:09<37:40,  2.67s/it]INFO:root:global_step: 152, logpy: -514.930, kl: 18.947, loss: 516.056\n",
      " 15%|█▌        | 153/1000 [05:12<36:27,  2.58s/it]INFO:root:global_step: 153, logpy: -509.459, kl: 18.831, loss: 510.596\n",
      " 15%|█▌        | 154/1000 [05:14<35:30,  2.52s/it]INFO:root:global_step: 154, logpy: -504.124, kl: 18.718, loss: 505.273\n",
      " 16%|█▌        | 155/1000 [05:16<35:09,  2.50s/it]INFO:root:global_step: 155, logpy: -498.752, kl: 18.604, loss: 499.912\n",
      " 16%|█▌        | 156/1000 [05:19<35:11,  2.50s/it]INFO:root:global_step: 156, logpy: -493.482, kl: 18.491, loss: 494.654\n",
      " 16%|█▌        | 157/1000 [05:22<35:41,  2.54s/it]INFO:root:global_step: 157, logpy: -488.183, kl: 18.379, loss: 489.365\n",
      " 16%|█▌        | 158/1000 [05:24<35:15,  2.51s/it]INFO:root:global_step: 158, logpy: -483.014, kl: 18.271, loss: 484.209\n",
      " 16%|█▌        | 159/1000 [05:26<34:38,  2.47s/it]INFO:root:global_step: 159, logpy: -477.830, kl: 18.161, loss: 479.036\n",
      " 16%|█▌        | 160/1000 [05:29<34:15,  2.45s/it]INFO:root:global_step: 160, logpy: -472.732, kl: 18.051, loss: 473.950\n",
      " 16%|█▌        | 161/1000 [05:31<34:17,  2.45s/it]INFO:root:global_step: 161, logpy: -467.684, kl: 17.941, loss: 468.913\n",
      " 16%|█▌        | 162/1000 [05:34<34:09,  2.45s/it]INFO:root:global_step: 162, logpy: -462.698, kl: 17.836, loss: 463.938\n",
      " 16%|█▋        | 163/1000 [05:36<34:32,  2.48s/it]INFO:root:global_step: 163, logpy: -457.772, kl: 17.735, loss: 459.025\n",
      " 16%|█▋        | 164/1000 [05:39<34:07,  2.45s/it]INFO:root:global_step: 164, logpy: -452.872, kl: 17.634, loss: 454.138\n",
      " 16%|█▋        | 165/1000 [05:41<33:54,  2.44s/it]INFO:root:global_step: 165, logpy: -448.005, kl: 17.533, loss: 449.283\n",
      " 17%|█▋        | 166/1000 [05:44<34:28,  2.48s/it]INFO:root:global_step: 166, logpy: -443.214, kl: 17.435, loss: 444.505\n",
      " 17%|█▋        | 167/1000 [05:46<35:02,  2.52s/it]INFO:root:global_step: 167, logpy: -438.372, kl: 17.336, loss: 439.676\n",
      " 17%|█▋        | 168/1000 [05:49<35:26,  2.56s/it]INFO:root:global_step: 168, logpy: -433.645, kl: 17.239, loss: 434.961\n",
      " 17%|█▋        | 169/1000 [05:51<34:47,  2.51s/it]INFO:root:global_step: 169, logpy: -428.972, kl: 17.144, loss: 430.302\n",
      " 17%|█▋        | 170/1000 [05:54<34:23,  2.49s/it]INFO:root:global_step: 170, logpy: -424.360, kl: 17.046, loss: 425.701\n",
      " 17%|█▋        | 171/1000 [05:56<34:10,  2.47s/it]INFO:root:global_step: 171, logpy: -419.648, kl: 16.950, loss: 421.002\n",
      " 17%|█▋        | 172/1000 [05:59<34:21,  2.49s/it]INFO:root:global_step: 172, logpy: -415.020, kl: 16.856, loss: 416.386\n",
      " 17%|█▋        | 173/1000 [06:01<34:07,  2.48s/it]INFO:root:global_step: 173, logpy: -410.520, kl: 16.766, loss: 411.899\n",
      " 17%|█▋        | 174/1000 [06:03<33:48,  2.46s/it]INFO:root:global_step: 174, logpy: -405.936, kl: 16.673, loss: 407.328\n",
      " 18%|█▊        | 175/1000 [06:06<33:49,  2.46s/it]INFO:root:global_step: 175, logpy: -401.477, kl: 16.580, loss: 402.882\n",
      " 18%|█▊        | 176/1000 [06:08<33:55,  2.47s/it]INFO:root:global_step: 176, logpy: -397.045, kl: 16.488, loss: 398.462\n",
      " 18%|█▊        | 177/1000 [06:11<33:58,  2.48s/it]INFO:root:global_step: 177, logpy: -392.635, kl: 16.400, loss: 394.064\n",
      " 18%|█▊        | 178/1000 [06:13<33:52,  2.47s/it]INFO:root:global_step: 178, logpy: -388.254, kl: 16.314, loss: 389.697\n",
      " 18%|█▊        | 179/1000 [06:16<34:12,  2.50s/it]INFO:root:global_step: 179, logpy: -383.922, kl: 16.229, loss: 385.378\n",
      " 18%|█▊        | 180/1000 [06:18<33:59,  2.49s/it]INFO:root:global_step: 180, logpy: -379.629, kl: 16.144, loss: 381.099\n",
      " 18%|█▊        | 181/1000 [06:21<33:50,  2.48s/it]INFO:root:global_step: 181, logpy: -375.422, kl: 16.060, loss: 376.906\n",
      " 18%|█▊        | 182/1000 [06:23<33:55,  2.49s/it]INFO:root:global_step: 182, logpy: -371.188, kl: 15.977, loss: 372.685\n",
      " 18%|█▊        | 183/1000 [06:26<33:48,  2.48s/it]INFO:root:global_step: 183, logpy: -367.003, kl: 15.895, loss: 368.514\n",
      " 18%|█▊        | 184/1000 [06:28<33:41,  2.48s/it]INFO:root:global_step: 184, logpy: -362.920, kl: 15.815, loss: 364.445\n",
      " 18%|█▊        | 185/1000 [06:31<33:34,  2.47s/it]INFO:root:global_step: 185, logpy: -358.851, kl: 15.736, loss: 360.390\n",
      " 19%|█▊        | 186/1000 [06:33<33:37,  2.48s/it]INFO:root:global_step: 186, logpy: -354.773, kl: 15.658, loss: 356.327\n",
      " 19%|█▊        | 187/1000 [06:36<33:35,  2.48s/it]INFO:root:global_step: 187, logpy: -350.804, kl: 15.581, loss: 352.372\n",
      " 19%|█▉        | 188/1000 [06:38<33:30,  2.48s/it]INFO:root:global_step: 188, logpy: -346.774, kl: 15.504, loss: 348.356\n",
      " 19%|█▉        | 189/1000 [06:41<33:32,  2.48s/it]INFO:root:global_step: 189, logpy: -342.822, kl: 15.429, loss: 344.419\n",
      " 19%|█▉        | 190/1000 [06:43<33:50,  2.51s/it]INFO:root:global_step: 190, logpy: -338.956, kl: 15.355, loss: 340.568\n",
      " 19%|█▉        | 191/1000 [06:46<33:51,  2.51s/it]INFO:root:global_step: 191, logpy: -334.960, kl: 15.282, loss: 336.586\n",
      " 19%|█▉        | 192/1000 [06:48<33:47,  2.51s/it]INFO:root:global_step: 192, logpy: -331.051, kl: 15.210, loss: 332.692\n",
      " 19%|█▉        | 193/1000 [06:51<33:43,  2.51s/it]INFO:root:global_step: 193, logpy: -327.227, kl: 15.137, loss: 328.883\n",
      " 19%|█▉        | 194/1000 [06:53<33:46,  2.51s/it]INFO:root:global_step: 194, logpy: -323.404, kl: 15.065, loss: 325.074\n",
      " 20%|█▉        | 195/1000 [06:56<33:45,  2.52s/it]INFO:root:global_step: 195, logpy: -319.658, kl: 14.994, loss: 321.343\n",
      " 20%|█▉        | 196/1000 [06:58<33:37,  2.51s/it]INFO:root:global_step: 196, logpy: -315.981, kl: 14.928, loss: 317.682\n",
      " 20%|█▉        | 197/1000 [07:01<33:42,  2.52s/it]INFO:root:global_step: 197, logpy: -312.213, kl: 14.861, loss: 313.930\n",
      " 20%|█▉        | 198/1000 [07:03<33:32,  2.51s/it]INFO:root:global_step: 198, logpy: -308.558, kl: 14.794, loss: 310.290\n",
      " 20%|█▉        | 199/1000 [07:06<33:58,  2.54s/it]INFO:root:global_step: 199, logpy: -305.012, kl: 14.727, loss: 306.758\n",
      " 20%|██        | 200/1000 [07:09<33:52,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_200.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 200, logpy: -301.454, kl: 14.662, loss: 303.217\n",
      " 20%|██        | 201/1000 [07:12<39:23,  2.96s/it]INFO:root:global_step: 201, logpy: -297.845, kl: 14.597, loss: 299.623\n",
      " 20%|██        | 202/1000 [07:15<37:45,  2.84s/it]INFO:root:global_step: 202, logpy: -294.333, kl: 14.533, loss: 296.126\n",
      " 20%|██        | 203/1000 [07:18<36:32,  2.75s/it]INFO:root:global_step: 203, logpy: -290.791, kl: 14.469, loss: 292.599\n",
      " 20%|██        | 204/1000 [07:20<35:48,  2.70s/it]INFO:root:global_step: 204, logpy: -287.305, kl: 14.407, loss: 289.130\n",
      " 20%|██        | 205/1000 [07:23<34:58,  2.64s/it]INFO:root:global_step: 205, logpy: -283.846, kl: 14.347, loss: 285.686\n",
      " 21%|██        | 206/1000 [07:25<34:34,  2.61s/it]INFO:root:global_step: 206, logpy: -280.428, kl: 14.285, loss: 282.284\n",
      " 21%|██        | 207/1000 [07:28<34:28,  2.61s/it]INFO:root:global_step: 207, logpy: -277.036, kl: 14.225, loss: 278.908\n",
      " 21%|██        | 208/1000 [07:30<34:31,  2.62s/it]INFO:root:global_step: 208, logpy: -273.730, kl: 14.166, loss: 275.617\n",
      " 21%|██        | 209/1000 [07:33<34:43,  2.63s/it]INFO:root:global_step: 209, logpy: -270.367, kl: 14.106, loss: 272.270\n",
      " 21%|██        | 210/1000 [07:36<34:30,  2.62s/it]INFO:root:global_step: 210, logpy: -267.054, kl: 14.050, loss: 268.974\n",
      " 21%|██        | 211/1000 [07:38<34:29,  2.62s/it]INFO:root:global_step: 211, logpy: -263.718, kl: 13.996, loss: 265.655\n",
      " 21%|██        | 212/1000 [07:41<34:08,  2.60s/it]INFO:root:global_step: 212, logpy: -260.475, kl: 13.939, loss: 262.429\n",
      " 21%|██▏       | 213/1000 [07:43<34:11,  2.61s/it]INFO:root:global_step: 213, logpy: -257.258, kl: 13.886, loss: 259.229\n",
      " 21%|██▏       | 214/1000 [07:46<34:01,  2.60s/it]INFO:root:global_step: 214, logpy: -254.030, kl: 13.835, loss: 256.019\n",
      " 22%|██▏       | 215/1000 [07:49<34:00,  2.60s/it]INFO:root:global_step: 215, logpy: -250.866, kl: 13.783, loss: 252.872\n",
      " 22%|██▏       | 216/1000 [07:51<34:07,  2.61s/it]INFO:root:global_step: 216, logpy: -247.731, kl: 13.731, loss: 249.754\n",
      " 22%|██▏       | 217/1000 [07:54<33:57,  2.60s/it]INFO:root:global_step: 217, logpy: -244.689, kl: 13.681, loss: 246.731\n",
      " 22%|██▏       | 218/1000 [07:56<33:52,  2.60s/it]INFO:root:global_step: 218, logpy: -241.609, kl: 13.630, loss: 243.667\n",
      " 22%|██▏       | 219/1000 [07:59<33:42,  2.59s/it]INFO:root:global_step: 219, logpy: -238.633, kl: 13.584, loss: 240.710\n",
      " 22%|██▏       | 220/1000 [08:02<33:29,  2.58s/it]INFO:root:global_step: 220, logpy: -235.611, kl: 13.537, loss: 237.707\n",
      " 22%|██▏       | 221/1000 [08:04<33:18,  2.57s/it]INFO:root:global_step: 221, logpy: -232.613, kl: 13.489, loss: 234.727\n",
      " 22%|██▏       | 222/1000 [08:07<33:47,  2.61s/it]INFO:root:global_step: 222, logpy: -229.652, kl: 13.440, loss: 231.783\n",
      " 22%|██▏       | 223/1000 [08:10<33:59,  2.63s/it]INFO:root:global_step: 223, logpy: -226.668, kl: 13.392, loss: 228.817\n",
      " 22%|██▏       | 224/1000 [08:12<33:51,  2.62s/it]INFO:root:global_step: 224, logpy: -223.796, kl: 13.349, loss: 225.964\n",
      " 22%|██▎       | 225/1000 [08:15<33:47,  2.62s/it]INFO:root:global_step: 225, logpy: -220.878, kl: 13.304, loss: 223.064\n",
      " 23%|██▎       | 226/1000 [08:17<33:39,  2.61s/it]INFO:root:global_step: 226, logpy: -218.004, kl: 13.260, loss: 220.209\n",
      " 23%|██▎       | 227/1000 [08:20<33:35,  2.61s/it]INFO:root:global_step: 227, logpy: -215.167, kl: 13.217, loss: 217.391\n",
      " 23%|██▎       | 228/1000 [08:23<33:33,  2.61s/it]INFO:root:global_step: 228, logpy: -212.366, kl: 13.175, loss: 214.609\n",
      " 23%|██▎       | 229/1000 [08:25<33:23,  2.60s/it]INFO:root:global_step: 229, logpy: -209.569, kl: 13.133, loss: 211.830\n",
      " 23%|██▎       | 230/1000 [08:28<33:16,  2.59s/it]INFO:root:global_step: 230, logpy: -206.794, kl: 13.090, loss: 209.074\n",
      " 23%|██▎       | 231/1000 [08:30<33:14,  2.59s/it]INFO:root:global_step: 231, logpy: -204.002, kl: 13.048, loss: 206.300\n",
      " 23%|██▎       | 232/1000 [08:33<33:25,  2.61s/it]INFO:root:global_step: 232, logpy: -201.302, kl: 13.009, loss: 203.620\n",
      " 23%|██▎       | 233/1000 [08:36<33:37,  2.63s/it]INFO:root:global_step: 233, logpy: -198.685, kl: 12.970, loss: 201.023\n",
      " 23%|██▎       | 234/1000 [08:38<33:31,  2.63s/it]INFO:root:global_step: 234, logpy: -195.989, kl: 12.934, loss: 198.348\n",
      " 24%|██▎       | 235/1000 [08:41<33:30,  2.63s/it]INFO:root:global_step: 235, logpy: -193.298, kl: 12.894, loss: 195.674\n",
      " 24%|██▎       | 236/1000 [08:43<33:10,  2.61s/it]INFO:root:global_step: 236, logpy: -190.683, kl: 12.856, loss: 193.079\n",
      " 24%|██▎       | 237/1000 [08:46<33:07,  2.60s/it]INFO:root:global_step: 237, logpy: -188.004, kl: 12.821, loss: 190.421\n",
      " 24%|██▍       | 238/1000 [08:49<32:57,  2.60s/it]INFO:root:global_step: 238, logpy: -185.460, kl: 12.789, loss: 187.899\n",
      " 24%|██▍       | 239/1000 [08:51<32:47,  2.58s/it]INFO:root:global_step: 239, logpy: -182.853, kl: 12.752, loss: 185.310\n",
      " 24%|██▍       | 240/1000 [08:54<32:42,  2.58s/it]INFO:root:global_step: 240, logpy: -180.287, kl: 12.716, loss: 182.764\n",
      " 24%|██▍       | 241/1000 [08:56<32:46,  2.59s/it]INFO:root:global_step: 241, logpy: -177.812, kl: 12.683, loss: 180.310\n",
      " 24%|██▍       | 242/1000 [08:59<32:47,  2.60s/it]INFO:root:global_step: 242, logpy: -175.277, kl: 12.651, loss: 177.796\n",
      " 24%|██▍       | 243/1000 [09:02<32:48,  2.60s/it]INFO:root:global_step: 243, logpy: -172.825, kl: 12.618, loss: 175.365\n",
      " 24%|██▍       | 244/1000 [09:04<33:02,  2.62s/it]INFO:root:global_step: 244, logpy: -170.345, kl: 12.584, loss: 172.904\n",
      " 24%|██▍       | 245/1000 [09:07<33:11,  2.64s/it]INFO:root:global_step: 245, logpy: -167.861, kl: 12.551, loss: 170.441\n",
      " 25%|██▍       | 246/1000 [09:10<33:08,  2.64s/it]INFO:root:global_step: 246, logpy: -165.522, kl: 12.519, loss: 168.121\n",
      " 25%|██▍       | 247/1000 [09:12<33:13,  2.65s/it]INFO:root:global_step: 247, logpy: -163.195, kl: 12.489, loss: 165.816\n",
      " 25%|██▍       | 248/1000 [09:15<33:13,  2.65s/it]INFO:root:global_step: 248, logpy: -160.808, kl: 12.458, loss: 163.450\n",
      " 25%|██▍       | 249/1000 [09:17<33:09,  2.65s/it]INFO:root:global_step: 249, logpy: -158.551, kl: 12.429, loss: 161.214\n",
      " 25%|██▌       | 250/1000 [09:20<33:20,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_250.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 250, logpy: -156.313, kl: 12.401, loss: 158.998\n",
      " 25%|██▌       | 251/1000 [09:24<37:48,  3.03s/it]INFO:root:global_step: 251, logpy: -153.947, kl: 12.372, loss: 156.653\n",
      " 25%|██▌       | 252/1000 [09:27<36:41,  2.94s/it]INFO:root:global_step: 252, logpy: -151.788, kl: 12.345, loss: 154.515\n",
      " 25%|██▌       | 253/1000 [09:30<35:40,  2.87s/it]INFO:root:global_step: 253, logpy: -149.580, kl: 12.317, loss: 152.329\n",
      " 25%|██▌       | 254/1000 [09:32<35:45,  2.88s/it]INFO:root:global_step: 254, logpy: -147.270, kl: 12.290, loss: 150.040\n",
      " 26%|██▌       | 255/1000 [09:35<35:18,  2.84s/it]INFO:root:global_step: 255, logpy: -145.143, kl: 12.263, loss: 147.935\n",
      " 26%|██▌       | 256/1000 [09:38<34:38,  2.79s/it]INFO:root:global_step: 256, logpy: -142.955, kl: 12.237, loss: 145.768\n",
      " 26%|██▌       | 257/1000 [09:41<34:16,  2.77s/it]INFO:root:global_step: 257, logpy: -140.729, kl: 12.210, loss: 143.564\n",
      " 26%|██▌       | 258/1000 [09:43<34:23,  2.78s/it]INFO:root:global_step: 258, logpy: -138.533, kl: 12.188, loss: 141.391\n",
      " 26%|██▌       | 259/1000 [09:46<34:30,  2.79s/it]INFO:root:global_step: 259, logpy: -136.359, kl: 12.164, loss: 139.240\n",
      " 26%|██▌       | 260/1000 [09:49<34:06,  2.77s/it]INFO:root:global_step: 260, logpy: -134.176, kl: 12.135, loss: 137.076\n",
      " 26%|██▌       | 261/1000 [09:52<33:46,  2.74s/it]INFO:root:global_step: 261, logpy: -132.041, kl: 12.111, loss: 134.963\n",
      " 26%|██▌       | 262/1000 [09:54<33:33,  2.73s/it]INFO:root:global_step: 262, logpy: -129.944, kl: 12.090, loss: 132.889\n",
      " 26%|██▋       | 263/1000 [09:57<33:18,  2.71s/it]INFO:root:global_step: 263, logpy: -127.831, kl: 12.065, loss: 130.798\n",
      " 26%|██▋       | 264/1000 [10:00<33:17,  2.71s/it]INFO:root:global_step: 264, logpy: -125.767, kl: 12.044, loss: 128.756\n",
      " 26%|██▋       | 265/1000 [10:03<33:41,  2.75s/it]INFO:root:global_step: 265, logpy: -123.692, kl: 12.022, loss: 126.704\n",
      " 27%|██▋       | 266/1000 [10:05<34:03,  2.78s/it]INFO:root:global_step: 266, logpy: -121.582, kl: 12.001, loss: 124.617\n",
      " 27%|██▋       | 267/1000 [10:08<33:56,  2.78s/it]INFO:root:global_step: 267, logpy: -119.528, kl: 11.981, loss: 122.586\n",
      " 27%|██▋       | 268/1000 [10:11<33:40,  2.76s/it]INFO:root:global_step: 268, logpy: -117.547, kl: 11.962, loss: 120.629\n",
      " 27%|██▋       | 269/1000 [10:14<33:26,  2.74s/it]INFO:root:global_step: 269, logpy: -115.576, kl: 11.946, loss: 118.683\n",
      " 27%|██▋       | 270/1000 [10:16<33:07,  2.72s/it]INFO:root:global_step: 270, logpy: -113.553, kl: 11.925, loss: 116.682\n",
      " 27%|██▋       | 271/1000 [10:19<33:10,  2.73s/it]INFO:root:global_step: 271, logpy: -111.580, kl: 11.906, loss: 114.733\n",
      " 27%|██▋       | 272/1000 [10:22<33:06,  2.73s/it]INFO:root:global_step: 272, logpy: -109.648, kl: 11.888, loss: 112.825\n",
      " 27%|██▋       | 273/1000 [10:24<32:55,  2.72s/it]INFO:root:global_step: 273, logpy: -107.687, kl: 11.874, loss: 110.889\n",
      " 27%|██▋       | 274/1000 [10:27<32:47,  2.71s/it]INFO:root:global_step: 274, logpy: -105.772, kl: 11.856, loss: 108.997\n",
      " 28%|██▊       | 275/1000 [10:30<32:44,  2.71s/it]INFO:root:global_step: 275, logpy: -103.881, kl: 11.838, loss: 107.129\n",
      " 28%|██▊       | 276/1000 [10:33<33:02,  2.74s/it]INFO:root:global_step: 276, logpy: -102.005, kl: 11.820, loss: 105.277\n",
      " 28%|██▊       | 277/1000 [10:35<32:44,  2.72s/it]INFO:root:global_step: 277, logpy: -100.160, kl: 11.808, loss: 103.458\n",
      " 28%|██▊       | 278/1000 [10:38<32:42,  2.72s/it]INFO:root:global_step: 278, logpy: -98.364, kl: 11.792, loss: 101.686\n",
      " 28%|██▊       | 279/1000 [10:41<32:34,  2.71s/it]INFO:root:global_step: 279, logpy: -96.480, kl: 11.778, loss: 99.827\n",
      " 28%|██▊       | 280/1000 [10:43<32:33,  2.71s/it]INFO:root:global_step: 280, logpy: -94.657, kl: 11.766, loss: 98.030\n",
      " 28%|██▊       | 281/1000 [10:46<32:40,  2.73s/it]INFO:root:global_step: 281, logpy: -92.876, kl: 11.751, loss: 96.274\n",
      " 28%|██▊       | 282/1000 [10:49<32:26,  2.71s/it]INFO:root:global_step: 282, logpy: -91.115, kl: 11.741, loss: 94.539\n",
      " 28%|██▊       | 283/1000 [10:52<32:21,  2.71s/it]INFO:root:global_step: 283, logpy: -89.340, kl: 11.728, loss: 92.789\n",
      " 28%|██▊       | 284/1000 [10:54<32:29,  2.72s/it]INFO:root:global_step: 284, logpy: -87.596, kl: 11.715, loss: 91.070\n",
      " 28%|██▊       | 285/1000 [10:57<32:25,  2.72s/it]INFO:root:global_step: 285, logpy: -85.814, kl: 11.703, loss: 89.314\n",
      " 29%|██▊       | 286/1000 [11:00<32:26,  2.73s/it]INFO:root:global_step: 286, logpy: -84.135, kl: 11.690, loss: 87.659\n",
      " 29%|██▊       | 287/1000 [11:03<32:39,  2.75s/it]INFO:root:global_step: 287, logpy: -82.465, kl: 11.679, loss: 86.015\n",
      " 29%|██▉       | 288/1000 [11:05<32:30,  2.74s/it]INFO:root:global_step: 288, logpy: -80.738, kl: 11.667, loss: 84.313\n",
      " 29%|██▉       | 289/1000 [11:08<32:25,  2.74s/it]INFO:root:global_step: 289, logpy: -79.107, kl: 11.656, loss: 82.707\n",
      " 29%|██▉       | 290/1000 [11:11<32:25,  2.74s/it]INFO:root:global_step: 290, logpy: -77.473, kl: 11.642, loss: 81.098\n",
      " 29%|██▉       | 291/1000 [11:14<32:26,  2.75s/it]INFO:root:global_step: 291, logpy: -75.796, kl: 11.633, loss: 79.447\n",
      " 29%|██▉       | 292/1000 [11:16<32:20,  2.74s/it]INFO:root:global_step: 292, logpy: -74.221, kl: 11.623, loss: 77.898\n",
      " 29%|██▉       | 293/1000 [11:19<32:14,  2.74s/it]INFO:root:global_step: 293, logpy: -72.592, kl: 11.612, loss: 76.294\n",
      " 29%|██▉       | 294/1000 [11:22<32:11,  2.74s/it]INFO:root:global_step: 294, logpy: -71.009, kl: 11.605, loss: 74.738\n",
      " 30%|██▉       | 295/1000 [11:24<32:08,  2.74s/it]INFO:root:global_step: 295, logpy: -69.563, kl: 11.602, loss: 73.321\n",
      " 30%|██▉       | 296/1000 [11:27<32:01,  2.73s/it]INFO:root:global_step: 296, logpy: -68.007, kl: 11.595, loss: 71.793\n",
      " 30%|██▉       | 297/1000 [11:30<32:13,  2.75s/it]INFO:root:global_step: 297, logpy: -66.409, kl: 11.586, loss: 70.221\n",
      " 30%|██▉       | 298/1000 [11:33<32:21,  2.77s/it]INFO:root:global_step: 298, logpy: -64.916, kl: 11.584, loss: 68.757\n",
      " 30%|██▉       | 299/1000 [11:36<32:15,  2.76s/it]INFO:root:global_step: 299, logpy: -63.432, kl: 11.577, loss: 67.300\n",
      " 30%|███       | 300/1000 [11:38<32:10,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_300.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 300, logpy: -61.894, kl: 11.571, loss: 65.790\n",
      " 30%|███       | 301/1000 [11:42<36:52,  3.16s/it]INFO:root:global_step: 301, logpy: -60.500, kl: 11.568, loss: 64.425\n",
      " 30%|███       | 302/1000 [11:45<35:23,  3.04s/it]INFO:root:global_step: 302, logpy: -58.991, kl: 11.563, loss: 62.944\n",
      " 30%|███       | 303/1000 [11:48<35:03,  3.02s/it]INFO:root:global_step: 303, logpy: -57.553, kl: 11.557, loss: 61.533\n",
      " 30%|███       | 304/1000 [11:51<34:39,  2.99s/it]INFO:root:global_step: 304, logpy: -56.113, kl: 11.554, loss: 60.122\n",
      " 30%|███       | 305/1000 [11:54<33:44,  2.91s/it]INFO:root:global_step: 305, logpy: -54.621, kl: 11.549, loss: 58.657\n",
      " 31%|███       | 306/1000 [11:57<33:12,  2.87s/it]INFO:root:global_step: 306, logpy: -53.175, kl: 11.543, loss: 57.239\n",
      " 31%|███       | 307/1000 [11:59<32:52,  2.85s/it]INFO:root:global_step: 307, logpy: -51.742, kl: 11.544, loss: 55.836\n",
      " 31%|███       | 308/1000 [12:02<32:23,  2.81s/it]INFO:root:global_step: 308, logpy: -50.285, kl: 11.539, loss: 54.407\n",
      " 31%|███       | 309/1000 [12:05<32:24,  2.81s/it]INFO:root:global_step: 309, logpy: -48.871, kl: 11.535, loss: 53.021\n",
      " 31%|███       | 310/1000 [12:08<32:19,  2.81s/it]INFO:root:global_step: 310, logpy: -47.426, kl: 11.534, loss: 51.605\n",
      " 31%|███       | 311/1000 [12:10<32:23,  2.82s/it]INFO:root:global_step: 311, logpy: -46.107, kl: 11.535, loss: 50.317\n",
      " 31%|███       | 312/1000 [12:13<32:44,  2.86s/it]INFO:root:global_step: 312, logpy: -44.845, kl: 11.536, loss: 49.085\n",
      " 31%|███▏      | 313/1000 [12:16<32:47,  2.86s/it]INFO:root:global_step: 313, logpy: -43.447, kl: 11.533, loss: 47.716\n",
      " 31%|███▏      | 314/1000 [12:19<32:42,  2.86s/it]INFO:root:global_step: 314, logpy: -42.139, kl: 11.532, loss: 46.437\n",
      " 32%|███▏      | 315/1000 [12:22<32:28,  2.84s/it]INFO:root:global_step: 315, logpy: -40.842, kl: 11.530, loss: 45.169\n",
      " 32%|███▏      | 316/1000 [12:25<32:08,  2.82s/it]INFO:root:global_step: 316, logpy: -39.501, kl: 11.532, loss: 43.859\n",
      " 32%|███▏      | 317/1000 [12:27<31:48,  2.79s/it]INFO:root:global_step: 317, logpy: -38.202, kl: 11.535, loss: 42.591\n",
      " 32%|███▏      | 318/1000 [12:30<31:55,  2.81s/it]INFO:root:global_step: 318, logpy: -36.909, kl: 11.535, loss: 41.328\n",
      " 32%|███▏      | 319/1000 [12:33<32:10,  2.84s/it]INFO:root:global_step: 319, logpy: -35.600, kl: 11.536, loss: 40.049\n",
      " 32%|███▏      | 320/1000 [12:36<32:14,  2.84s/it]INFO:root:global_step: 320, logpy: -34.298, kl: 11.534, loss: 38.776\n",
      " 32%|███▏      | 321/1000 [12:39<32:12,  2.85s/it]INFO:root:global_step: 321, logpy: -32.998, kl: 11.533, loss: 37.505\n",
      " 32%|███▏      | 322/1000 [12:42<31:51,  2.82s/it]INFO:root:global_step: 322, logpy: -31.679, kl: 11.537, loss: 36.218\n",
      " 32%|███▏      | 323/1000 [12:44<31:30,  2.79s/it]INFO:root:global_step: 323, logpy: -30.431, kl: 11.536, loss: 34.999\n",
      " 32%|███▏      | 324/1000 [12:47<31:30,  2.80s/it]INFO:root:global_step: 324, logpy: -29.179, kl: 11.536, loss: 33.776\n",
      " 32%|███▎      | 325/1000 [12:50<31:24,  2.79s/it]INFO:root:global_step: 325, logpy: -27.983, kl: 11.540, loss: 32.611\n",
      " 33%|███▎      | 326/1000 [12:53<31:13,  2.78s/it]INFO:root:global_step: 326, logpy: -26.769, kl: 11.539, loss: 31.426\n",
      " 33%|███▎      | 327/1000 [12:56<31:16,  2.79s/it]INFO:root:global_step: 327, logpy: -25.529, kl: 11.540, loss: 30.216\n",
      " 33%|███▎      | 328/1000 [12:58<31:08,  2.78s/it]INFO:root:global_step: 328, logpy: -24.352, kl: 11.542, loss: 29.069\n",
      " 33%|███▎      | 329/1000 [13:01<31:13,  2.79s/it]INFO:root:global_step: 329, logpy: -23.199, kl: 11.547, loss: 27.949\n",
      " 33%|███▎      | 330/1000 [13:04<31:04,  2.78s/it]INFO:root:global_step: 330, logpy: -22.033, kl: 11.548, loss: 26.812\n",
      " 33%|███▎      | 331/1000 [13:07<31:09,  2.79s/it]INFO:root:global_step: 331, logpy: -20.832, kl: 11.554, loss: 25.644\n",
      " 33%|███▎      | 332/1000 [13:10<31:15,  2.81s/it]INFO:root:global_step: 332, logpy: -19.646, kl: 11.556, loss: 24.488\n",
      " 33%|███▎      | 333/1000 [13:12<31:22,  2.82s/it]INFO:root:global_step: 333, logpy: -18.456, kl: 11.556, loss: 23.327\n",
      " 33%|███▎      | 334/1000 [13:15<31:12,  2.81s/it]INFO:root:global_step: 334, logpy: -17.322, kl: 11.563, loss: 22.226\n",
      " 34%|███▎      | 335/1000 [13:18<31:09,  2.81s/it]INFO:root:global_step: 335, logpy: -16.125, kl: 11.567, loss: 21.060\n",
      " 34%|███▎      | 336/1000 [13:21<31:03,  2.81s/it]INFO:root:global_step: 336, logpy: -14.933, kl: 11.569, loss: 19.899\n",
      " 34%|███▎      | 337/1000 [13:24<31:01,  2.81s/it]INFO:root:global_step: 337, logpy: -13.763, kl: 11.573, loss: 18.760\n",
      " 34%|███▍      | 338/1000 [13:26<31:08,  2.82s/it]INFO:root:global_step: 338, logpy: -12.637, kl: 11.579, loss: 17.666\n",
      " 34%|███▍      | 339/1000 [13:29<30:58,  2.81s/it]INFO:root:global_step: 339, logpy: -11.550, kl: 11.584, loss: 16.611\n",
      " 34%|███▍      | 340/1000 [13:32<30:55,  2.81s/it]INFO:root:global_step: 340, logpy: -10.401, kl: 11.588, loss: 15.494\n",
      " 34%|███▍      | 341/1000 [13:35<30:57,  2.82s/it]INFO:root:global_step: 341, logpy: -9.300, kl: 11.592, loss: 14.423\n",
      " 34%|███▍      | 342/1000 [13:38<30:45,  2.80s/it]INFO:root:global_step: 342, logpy: -8.230, kl: 11.599, loss: 13.387\n",
      " 34%|███▍      | 343/1000 [13:41<31:09,  2.85s/it]INFO:root:global_step: 343, logpy: -7.141, kl: 11.607, loss: 12.332\n",
      " 34%|███▍      | 344/1000 [13:44<31:24,  2.87s/it]INFO:root:global_step: 344, logpy: -6.056, kl: 11.610, loss: 11.276\n",
      " 34%|███▍      | 345/1000 [13:47<31:48,  2.91s/it]INFO:root:global_step: 345, logpy: -4.978, kl: 11.618, loss: 10.233\n",
      " 35%|███▍      | 346/1000 [13:49<31:46,  2.92s/it]INFO:root:global_step: 346, logpy: -3.922, kl: 11.627, loss: 9.211\n",
      " 35%|███▍      | 347/1000 [13:52<31:19,  2.88s/it]INFO:root:global_step: 347, logpy: -2.852, kl: 11.633, loss: 8.173\n",
      " 35%|███▍      | 348/1000 [13:55<31:14,  2.87s/it]INFO:root:global_step: 348, logpy: -1.804, kl: 11.638, loss: 7.157\n",
      " 35%|███▍      | 349/1000 [13:58<31:12,  2.88s/it]INFO:root:global_step: 349, logpy: -0.771, kl: 11.645, loss: 6.157\n",
      " 35%|███▌      | 350/1000 [14:01<31:06,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_350.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 350, logpy: 0.267, kl: 11.652, loss: 5.151\n",
      " 35%|███▌      | 351/1000 [14:05<35:14,  3.26s/it]INFO:root:global_step: 351, logpy: 1.297, kl: 11.657, loss: 4.153\n",
      " 35%|███▌      | 352/1000 [14:08<34:17,  3.18s/it]INFO:root:global_step: 352, logpy: 2.296, kl: 11.662, loss: 3.185\n",
      " 35%|███▌      | 353/1000 [14:11<33:43,  3.13s/it]INFO:root:global_step: 353, logpy: 3.272, kl: 11.669, loss: 2.241\n",
      " 35%|███▌      | 354/1000 [14:14<32:51,  3.05s/it]INFO:root:global_step: 354, logpy: 4.193, kl: 11.677, loss: 1.354\n",
      " 36%|███▌      | 355/1000 [14:17<32:18,  3.01s/it]INFO:root:global_step: 355, logpy: 5.124, kl: 11.687, loss: 0.457\n",
      " 36%|███▌      | 356/1000 [14:20<32:08,  2.99s/it]INFO:root:global_step: 356, logpy: 6.062, kl: 11.696, loss: -0.446\n",
      " 36%|███▌      | 357/1000 [14:23<31:39,  2.95s/it]INFO:root:global_step: 357, logpy: 6.950, kl: 11.707, loss: -1.299\n",
      " 36%|███▌      | 358/1000 [14:26<31:24,  2.93s/it]INFO:root:global_step: 358, logpy: 7.901, kl: 11.714, loss: -2.217\n",
      " 36%|███▌      | 359/1000 [14:28<31:16,  2.93s/it]INFO:root:global_step: 359, logpy: 8.827, kl: 11.721, loss: -3.111\n",
      " 36%|███▌      | 360/1000 [14:31<31:05,  2.92s/it]INFO:root:global_step: 360, logpy: 9.713, kl: 11.732, loss: -3.962\n",
      " 36%|███▌      | 361/1000 [14:34<30:54,  2.90s/it]INFO:root:global_step: 361, logpy: 10.641, kl: 11.741, loss: -4.856\n",
      " 36%|███▌      | 362/1000 [14:37<30:44,  2.89s/it]INFO:root:global_step: 362, logpy: 11.548, kl: 11.750, loss: -5.729\n",
      " 36%|███▋      | 363/1000 [14:40<30:54,  2.91s/it]INFO:root:global_step: 363, logpy: 12.409, kl: 11.759, loss: -6.556\n",
      " 36%|███▋      | 364/1000 [14:43<30:39,  2.89s/it]INFO:root:global_step: 364, logpy: 13.331, kl: 11.769, loss: -7.443\n",
      " 36%|███▋      | 365/1000 [14:46<30:28,  2.88s/it]INFO:root:global_step: 365, logpy: 14.226, kl: 11.779, loss: -8.304\n",
      " 37%|███▋      | 366/1000 [14:49<30:11,  2.86s/it]INFO:root:global_step: 366, logpy: 15.136, kl: 11.791, loss: -9.178\n",
      " 37%|███▋      | 367/1000 [14:51<30:12,  2.86s/it]INFO:root:global_step: 367, logpy: 16.049, kl: 11.801, loss: -10.056\n",
      " 37%|███▋      | 368/1000 [14:54<30:15,  2.87s/it]INFO:root:global_step: 368, logpy: 16.921, kl: 11.815, loss: -10.890\n",
      " 37%|███▋      | 369/1000 [14:57<30:16,  2.88s/it]INFO:root:global_step: 369, logpy: 17.829, kl: 11.824, loss: -11.765\n",
      " 37%|███▋      | 370/1000 [15:00<30:22,  2.89s/it]INFO:root:global_step: 370, logpy: 18.738, kl: 11.837, loss: -12.636\n",
      " 37%|███▋      | 371/1000 [15:03<30:19,  2.89s/it]INFO:root:global_step: 371, logpy: 19.595, kl: 11.849, loss: -13.457\n",
      " 37%|███▋      | 372/1000 [15:06<30:11,  2.88s/it]INFO:root:global_step: 372, logpy: 20.487, kl: 11.862, loss: -14.313\n",
      " 37%|███▋      | 373/1000 [15:09<30:05,  2.88s/it]INFO:root:global_step: 373, logpy: 21.296, kl: 11.876, loss: -15.085\n",
      " 37%|███▋      | 374/1000 [15:12<30:25,  2.92s/it]INFO:root:global_step: 374, logpy: 22.164, kl: 11.887, loss: -15.917\n",
      " 38%|███▊      | 375/1000 [15:15<30:14,  2.90s/it]INFO:root:global_step: 375, logpy: 23.044, kl: 11.901, loss: -16.760\n",
      " 38%|███▊      | 376/1000 [15:17<30:07,  2.90s/it]INFO:root:global_step: 376, logpy: 23.791, kl: 11.913, loss: -17.471\n",
      " 38%|███▊      | 377/1000 [15:20<30:04,  2.90s/it]INFO:root:global_step: 377, logpy: 24.609, kl: 11.926, loss: -18.253\n",
      " 38%|███▊      | 378/1000 [15:23<30:03,  2.90s/it]INFO:root:global_step: 378, logpy: 25.404, kl: 11.937, loss: -19.012\n",
      " 38%|███▊      | 379/1000 [15:26<30:03,  2.90s/it]INFO:root:global_step: 379, logpy: 26.173, kl: 11.948, loss: -19.746\n",
      " 38%|███▊      | 380/1000 [15:29<29:58,  2.90s/it]INFO:root:global_step: 380, logpy: 27.066, kl: 11.958, loss: -20.605\n",
      " 38%|███▊      | 381/1000 [15:32<29:55,  2.90s/it]INFO:root:global_step: 381, logpy: 27.859, kl: 11.969, loss: -21.362\n",
      " 38%|███▊      | 382/1000 [15:35<29:47,  2.89s/it]INFO:root:global_step: 382, logpy: 28.659, kl: 11.980, loss: -22.127\n",
      " 38%|███▊      | 383/1000 [15:38<29:37,  2.88s/it]INFO:root:global_step: 383, logpy: 29.480, kl: 11.990, loss: -22.913\n",
      " 38%|███▊      | 384/1000 [15:41<29:39,  2.89s/it]INFO:root:global_step: 384, logpy: 30.296, kl: 12.004, loss: -23.693\n",
      " 38%|███▊      | 385/1000 [15:44<30:14,  2.95s/it]INFO:root:global_step: 385, logpy: 31.105, kl: 12.018, loss: -24.464\n",
      " 39%|███▊      | 386/1000 [15:47<30:18,  2.96s/it]INFO:root:global_step: 386, logpy: 31.875, kl: 12.032, loss: -25.197\n",
      " 39%|███▊      | 387/1000 [15:50<30:31,  2.99s/it]INFO:root:global_step: 387, logpy: 32.638, kl: 12.048, loss: -25.921\n",
      " 39%|███▉      | 388/1000 [15:53<30:12,  2.96s/it]INFO:root:global_step: 388, logpy: 33.397, kl: 12.063, loss: -26.641\n",
      " 39%|███▉      | 389/1000 [15:56<30:34,  3.00s/it]INFO:root:global_step: 389, logpy: 34.166, kl: 12.074, loss: -27.376\n",
      " 39%|███▉      | 390/1000 [15:59<30:43,  3.02s/it]INFO:root:global_step: 390, logpy: 34.936, kl: 12.087, loss: -28.108\n",
      " 39%|███▉      | 391/1000 [16:02<30:32,  3.01s/it]INFO:root:global_step: 391, logpy: 35.706, kl: 12.098, loss: -28.843\n",
      " 39%|███▉      | 392/1000 [16:05<31:24,  3.10s/it]INFO:root:global_step: 392, logpy: 36.443, kl: 12.110, loss: -29.544\n",
      " 39%|███▉      | 393/1000 [16:08<30:50,  3.05s/it]INFO:root:global_step: 393, logpy: 37.217, kl: 12.124, loss: -30.282\n",
      " 39%|███▉      | 394/1000 [16:11<30:53,  3.06s/it]INFO:root:global_step: 394, logpy: 37.977, kl: 12.134, loss: -31.007\n",
      " 40%|███▉      | 395/1000 [16:14<30:52,  3.06s/it]INFO:root:global_step: 395, logpy: 38.717, kl: 12.150, loss: -31.708\n",
      " 40%|███▉      | 396/1000 [16:17<30:43,  3.05s/it]INFO:root:global_step: 396, logpy: 39.449, kl: 12.158, loss: -32.407\n",
      " 40%|███▉      | 397/1000 [16:20<30:19,  3.02s/it]INFO:root:global_step: 397, logpy: 40.182, kl: 12.174, loss: -33.101\n",
      " 40%|███▉      | 398/1000 [16:23<30:04,  3.00s/it]INFO:root:global_step: 398, logpy: 40.898, kl: 12.190, loss: -33.778\n",
      " 40%|███▉      | 399/1000 [16:26<29:55,  2.99s/it]INFO:root:global_step: 399, logpy: 41.597, kl: 12.204, loss: -34.439\n",
      " 40%|████      | 400/1000 [16:29<29:42,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_400.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 400, logpy: 42.362, kl: 12.215, loss: -35.169\n",
      " 40%|████      | 401/1000 [16:33<33:13,  3.33s/it]INFO:root:global_step: 401, logpy: 43.057, kl: 12.230, loss: -35.825\n",
      " 40%|████      | 402/1000 [16:36<32:30,  3.26s/it]INFO:root:global_step: 402, logpy: 43.758, kl: 12.244, loss: -36.489\n",
      " 40%|████      | 403/1000 [16:39<31:43,  3.19s/it]INFO:root:global_step: 403, logpy: 44.469, kl: 12.257, loss: -37.164\n",
      " 40%|████      | 404/1000 [16:42<30:51,  3.11s/it]INFO:root:global_step: 404, logpy: 45.227, kl: 12.270, loss: -37.884\n",
      " 40%|████      | 405/1000 [16:45<30:08,  3.04s/it]INFO:root:global_step: 405, logpy: 45.890, kl: 12.285, loss: -38.510\n",
      " 41%|████      | 406/1000 [16:48<30:07,  3.04s/it]INFO:root:global_step: 406, logpy: 46.595, kl: 12.296, loss: -39.179\n",
      " 41%|████      | 407/1000 [16:51<29:50,  3.02s/it]INFO:root:global_step: 407, logpy: 47.213, kl: 12.311, loss: -39.759\n",
      " 41%|████      | 408/1000 [16:54<29:35,  3.00s/it]INFO:root:global_step: 408, logpy: 47.863, kl: 12.327, loss: -40.369\n",
      " 41%|████      | 409/1000 [16:57<29:17,  2.97s/it]INFO:root:global_step: 409, logpy: 48.494, kl: 12.347, loss: -40.958\n",
      " 41%|████      | 410/1000 [17:00<29:03,  2.95s/it]INFO:root:global_step: 410, logpy: 49.159, kl: 12.362, loss: -41.585\n",
      " 41%|████      | 411/1000 [17:03<28:55,  2.95s/it]INFO:root:global_step: 411, logpy: 49.765, kl: 12.375, loss: -42.153\n",
      " 41%|████      | 412/1000 [17:06<29:01,  2.96s/it]INFO:root:global_step: 412, logpy: 50.381, kl: 12.391, loss: -42.730\n",
      " 41%|████▏     | 413/1000 [17:09<28:54,  2.95s/it]INFO:root:global_step: 413, logpy: 51.006, kl: 12.404, loss: -43.319\n",
      " 41%|████▏     | 414/1000 [17:12<29:00,  2.97s/it]INFO:root:global_step: 414, logpy: 51.670, kl: 12.419, loss: -43.944\n",
      " 42%|████▏     | 415/1000 [17:15<28:52,  2.96s/it]INFO:root:global_step: 415, logpy: 52.305, kl: 12.436, loss: -44.539\n",
      " 42%|████▏     | 416/1000 [17:18<28:42,  2.95s/it]INFO:root:global_step: 416, logpy: 52.958, kl: 12.452, loss: -45.152\n",
      " 42%|████▏     | 417/1000 [17:21<28:56,  2.98s/it]INFO:root:global_step: 417, logpy: 53.626, kl: 12.466, loss: -45.783\n",
      " 42%|████▏     | 418/1000 [17:24<28:39,  2.95s/it]INFO:root:global_step: 418, logpy: 54.225, kl: 12.481, loss: -46.343\n",
      " 42%|████▏     | 419/1000 [17:27<28:35,  2.95s/it]INFO:root:global_step: 419, logpy: 54.872, kl: 12.498, loss: -46.950\n",
      " 42%|████▏     | 420/1000 [17:29<28:31,  2.95s/it]INFO:root:global_step: 420, logpy: 55.479, kl: 12.510, loss: -47.520\n",
      " 42%|████▏     | 421/1000 [17:33<28:49,  2.99s/it]INFO:root:global_step: 421, logpy: 56.078, kl: 12.527, loss: -48.079\n",
      " 42%|████▏     | 422/1000 [17:36<28:43,  2.98s/it]INFO:root:global_step: 422, logpy: 56.619, kl: 12.545, loss: -48.580\n",
      " 42%|████▏     | 423/1000 [17:38<28:32,  2.97s/it]INFO:root:global_step: 423, logpy: 57.222, kl: 12.559, loss: -49.144\n",
      " 42%|████▏     | 424/1000 [17:41<28:34,  2.98s/it]INFO:root:global_step: 424, logpy: 57.853, kl: 12.571, loss: -49.740\n",
      " 42%|████▎     | 425/1000 [17:45<28:51,  3.01s/it]INFO:root:global_step: 425, logpy: 58.465, kl: 12.583, loss: -50.315\n",
      " 43%|████▎     | 426/1000 [17:48<28:45,  3.01s/it]INFO:root:global_step: 426, logpy: 59.095, kl: 12.597, loss: -50.908\n",
      " 43%|████▎     | 427/1000 [17:50<28:27,  2.98s/it]INFO:root:global_step: 427, logpy: 59.669, kl: 12.612, loss: -51.442\n",
      " 43%|████▎     | 428/1000 [17:53<28:34,  3.00s/it]INFO:root:global_step: 428, logpy: 60.267, kl: 12.626, loss: -52.002\n",
      " 43%|████▎     | 429/1000 [17:56<28:25,  2.99s/it]INFO:root:global_step: 429, logpy: 60.846, kl: 12.646, loss: -52.538\n",
      " 43%|████▎     | 430/1000 [17:59<28:10,  2.97s/it]INFO:root:global_step: 430, logpy: 61.426, kl: 12.662, loss: -53.079\n",
      " 43%|████▎     | 431/1000 [18:02<28:02,  2.96s/it]INFO:root:global_step: 431, logpy: 62.000, kl: 12.677, loss: -53.614\n",
      " 43%|████▎     | 432/1000 [18:05<28:03,  2.96s/it]INFO:root:global_step: 432, logpy: 62.536, kl: 12.694, loss: -54.109\n",
      " 43%|████▎     | 433/1000 [18:08<28:01,  2.97s/it]INFO:root:global_step: 433, logpy: 63.101, kl: 12.712, loss: -54.632\n",
      " 43%|████▎     | 434/1000 [18:11<28:01,  2.97s/it]INFO:root:global_step: 434, logpy: 63.636, kl: 12.729, loss: -55.126\n",
      " 44%|████▎     | 435/1000 [18:14<27:48,  2.95s/it]INFO:root:global_step: 435, logpy: 64.172, kl: 12.748, loss: -55.620\n",
      " 44%|████▎     | 436/1000 [18:17<27:46,  2.95s/it]INFO:root:global_step: 436, logpy: 64.678, kl: 12.768, loss: -56.083\n",
      " 44%|████▎     | 437/1000 [18:20<27:43,  2.95s/it]INFO:root:global_step: 437, logpy: 65.161, kl: 12.785, loss: -56.525\n",
      " 44%|████▍     | 438/1000 [18:23<27:44,  2.96s/it]INFO:root:global_step: 438, logpy: 65.715, kl: 12.802, loss: -57.038\n",
      " 44%|████▍     | 439/1000 [18:26<27:55,  2.99s/it]INFO:root:global_step: 439, logpy: 66.207, kl: 12.816, loss: -57.492\n",
      " 44%|████▍     | 440/1000 [18:29<27:45,  2.97s/it]INFO:root:global_step: 440, logpy: 66.712, kl: 12.832, loss: -57.958\n",
      " 44%|████▍     | 441/1000 [18:32<28:42,  3.08s/it]INFO:root:global_step: 441, logpy: 67.215, kl: 12.847, loss: -58.421\n",
      " 44%|████▍     | 442/1000 [18:35<28:49,  3.10s/it]INFO:root:global_step: 442, logpy: 67.751, kl: 12.862, loss: -58.918\n",
      " 44%|████▍     | 443/1000 [18:39<28:39,  3.09s/it]INFO:root:global_step: 443, logpy: 68.272, kl: 12.877, loss: -59.399\n",
      " 44%|████▍     | 444/1000 [18:42<28:22,  3.06s/it]INFO:root:global_step: 444, logpy: 68.812, kl: 12.893, loss: -59.900\n",
      " 44%|████▍     | 445/1000 [18:45<28:12,  3.05s/it]INFO:root:global_step: 445, logpy: 69.334, kl: 12.910, loss: -60.380\n",
      " 45%|████▍     | 446/1000 [18:48<27:57,  3.03s/it]INFO:root:global_step: 446, logpy: 69.865, kl: 12.928, loss: -60.870\n",
      " 45%|████▍     | 447/1000 [18:51<27:44,  3.01s/it]INFO:root:global_step: 447, logpy: 70.376, kl: 12.947, loss: -61.337\n",
      " 45%|████▍     | 448/1000 [18:54<27:37,  3.00s/it]INFO:root:global_step: 448, logpy: 70.860, kl: 12.967, loss: -61.778\n",
      " 45%|████▍     | 449/1000 [18:56<27:27,  2.99s/it]INFO:root:global_step: 449, logpy: 71.338, kl: 12.983, loss: -62.216\n",
      " 45%|████▌     | 450/1000 [19:00<27:45,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_450.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 450, logpy: 71.799, kl: 13.000, loss: -62.635\n",
      " 45%|████▌     | 451/1000 [19:04<31:03,  3.39s/it]INFO:root:global_step: 451, logpy: 72.196, kl: 13.017, loss: -62.991\n",
      " 45%|████▌     | 452/1000 [19:07<30:08,  3.30s/it]INFO:root:global_step: 452, logpy: 72.672, kl: 13.034, loss: -63.426\n",
      " 45%|████▌     | 453/1000 [19:10<29:51,  3.27s/it]INFO:root:global_step: 453, logpy: 73.166, kl: 13.050, loss: -63.879\n",
      " 45%|████▌     | 454/1000 [19:13<29:13,  3.21s/it]INFO:root:global_step: 454, logpy: 73.632, kl: 13.064, loss: -64.306\n",
      " 46%|████▌     | 455/1000 [19:16<28:35,  3.15s/it]INFO:root:global_step: 455, logpy: 74.096, kl: 13.080, loss: -64.730\n",
      " 46%|████▌     | 456/1000 [19:19<28:02,  3.09s/it]INFO:root:global_step: 456, logpy: 74.544, kl: 13.100, loss: -65.134\n",
      " 46%|████▌     | 457/1000 [19:22<27:43,  3.06s/it]INFO:root:global_step: 457, logpy: 75.039, kl: 13.115, loss: -65.589\n",
      " 46%|████▌     | 458/1000 [19:25<27:28,  3.04s/it]INFO:root:global_step: 458, logpy: 75.444, kl: 13.135, loss: -65.950\n",
      " 46%|████▌     | 459/1000 [19:28<27:22,  3.04s/it]INFO:root:global_step: 459, logpy: 75.816, kl: 13.150, loss: -66.283\n",
      " 46%|████▌     | 460/1000 [19:31<27:30,  3.06s/it]INFO:root:global_step: 460, logpy: 76.233, kl: 13.169, loss: -66.656\n",
      " 46%|████▌     | 461/1000 [19:34<27:11,  3.03s/it]INFO:root:global_step: 461, logpy: 76.658, kl: 13.187, loss: -67.039\n",
      " 46%|████▌     | 462/1000 [19:37<27:02,  3.02s/it]INFO:root:global_step: 462, logpy: 77.060, kl: 13.206, loss: -67.397\n",
      " 46%|████▋     | 463/1000 [19:40<26:56,  3.01s/it]INFO:root:global_step: 463, logpy: 77.492, kl: 13.227, loss: -67.783\n",
      " 46%|████▋     | 464/1000 [19:43<27:03,  3.03s/it]INFO:root:global_step: 464, logpy: 77.972, kl: 13.243, loss: -68.223\n",
      " 46%|████▋     | 465/1000 [19:46<27:23,  3.07s/it]INFO:root:global_step: 465, logpy: 78.435, kl: 13.258, loss: -68.646\n",
      " 47%|████▋     | 466/1000 [19:50<27:22,  3.08s/it]INFO:root:global_step: 466, logpy: 78.898, kl: 13.273, loss: -69.069\n",
      " 47%|████▋     | 467/1000 [19:53<27:08,  3.06s/it]INFO:root:global_step: 467, logpy: 79.322, kl: 13.289, loss: -69.452\n",
      " 47%|████▋     | 468/1000 [19:56<27:28,  3.10s/it]INFO:root:global_step: 468, logpy: 79.760, kl: 13.307, loss: -69.847\n",
      " 47%|████▋     | 469/1000 [19:59<27:46,  3.14s/it]INFO:root:global_step: 469, logpy: 80.165, kl: 13.322, loss: -70.213\n",
      " 47%|████▋     | 470/1000 [20:02<28:14,  3.20s/it]INFO:root:global_step: 470, logpy: 80.596, kl: 13.338, loss: -70.601\n",
      " 47%|████▋     | 471/1000 [20:05<28:03,  3.18s/it]INFO:root:global_step: 471, logpy: 81.007, kl: 13.353, loss: -70.973\n",
      " 47%|████▋     | 472/1000 [20:09<29:49,  3.39s/it]INFO:root:global_step: 472, logpy: 81.469, kl: 13.370, loss: -71.392\n",
      " 47%|████▋     | 473/1000 [20:13<29:27,  3.35s/it]INFO:root:global_step: 473, logpy: 81.864, kl: 13.388, loss: -71.745\n",
      " 47%|████▋     | 474/1000 [20:16<28:37,  3.26s/it]INFO:root:global_step: 474, logpy: 82.243, kl: 13.411, loss: -72.076\n",
      " 48%|████▊     | 475/1000 [20:19<28:24,  3.25s/it]INFO:root:global_step: 475, logpy: 82.654, kl: 13.432, loss: -72.441\n",
      " 48%|████▊     | 476/1000 [20:22<28:40,  3.28s/it]INFO:root:global_step: 476, logpy: 83.076, kl: 13.447, loss: -72.822\n",
      " 48%|████▊     | 477/1000 [20:26<28:45,  3.30s/it]INFO:root:global_step: 477, logpy: 83.488, kl: 13.467, loss: -73.189\n",
      " 48%|████▊     | 478/1000 [20:29<28:50,  3.32s/it]INFO:root:global_step: 478, logpy: 83.863, kl: 13.486, loss: -73.519\n",
      " 48%|████▊     | 479/1000 [20:32<28:18,  3.26s/it]INFO:root:global_step: 479, logpy: 84.291, kl: 13.508, loss: -73.900\n",
      " 48%|████▊     | 480/1000 [20:35<28:13,  3.26s/it]INFO:root:global_step: 480, logpy: 84.697, kl: 13.527, loss: -74.263\n",
      " 48%|████▊     | 481/1000 [20:38<27:54,  3.23s/it]INFO:root:global_step: 481, logpy: 85.113, kl: 13.545, loss: -74.635\n",
      " 48%|████▊     | 482/1000 [20:42<28:15,  3.27s/it]INFO:root:global_step: 482, logpy: 85.512, kl: 13.563, loss: -74.991\n",
      " 48%|████▊     | 483/1000 [20:45<28:14,  3.28s/it]INFO:root:global_step: 483, logpy: 85.931, kl: 13.584, loss: -75.363\n",
      " 48%|████▊     | 484/1000 [20:48<27:40,  3.22s/it]INFO:root:global_step: 484, logpy: 86.282, kl: 13.603, loss: -75.670\n",
      " 48%|████▊     | 485/1000 [20:51<27:23,  3.19s/it]INFO:root:global_step: 485, logpy: 86.629, kl: 13.620, loss: -75.975\n",
      " 49%|████▊     | 486/1000 [20:54<26:57,  3.15s/it]INFO:root:global_step: 486, logpy: 87.018, kl: 13.637, loss: -76.321\n",
      " 49%|████▊     | 487/1000 [20:57<26:36,  3.11s/it]INFO:root:global_step: 487, logpy: 87.361, kl: 13.658, loss: -76.617\n",
      " 49%|████▉     | 488/1000 [21:00<26:13,  3.07s/it]INFO:root:global_step: 488, logpy: 87.747, kl: 13.684, loss: -76.952\n",
      " 49%|████▉     | 489/1000 [21:03<25:52,  3.04s/it]INFO:root:global_step: 489, logpy: 88.112, kl: 13.701, loss: -77.274\n",
      " 49%|████▉     | 490/1000 [21:06<25:47,  3.03s/it]INFO:root:global_step: 490, logpy: 88.489, kl: 13.713, loss: -77.612\n",
      " 49%|████▉     | 491/1000 [21:09<25:51,  3.05s/it]INFO:root:global_step: 491, logpy: 88.830, kl: 13.732, loss: -77.908\n",
      " 49%|████▉     | 492/1000 [21:12<25:42,  3.04s/it]INFO:root:global_step: 492, logpy: 89.220, kl: 13.750, loss: -78.254\n",
      " 49%|████▉     | 493/1000 [21:16<25:50,  3.06s/it]INFO:root:global_step: 493, logpy: 89.561, kl: 13.769, loss: -78.551\n",
      " 49%|████▉     | 494/1000 [21:19<25:38,  3.04s/it]INFO:root:global_step: 494, logpy: 89.910, kl: 13.785, loss: -78.858\n",
      " 50%|████▉     | 495/1000 [21:22<25:43,  3.06s/it]INFO:root:global_step: 495, logpy: 90.255, kl: 13.807, loss: -79.155\n",
      " 50%|████▉     | 496/1000 [21:25<25:39,  3.05s/it]INFO:root:global_step: 496, logpy: 90.549, kl: 13.828, loss: -79.402\n",
      " 50%|████▉     | 497/1000 [21:28<25:40,  3.06s/it]INFO:root:global_step: 497, logpy: 90.903, kl: 13.849, loss: -79.708\n",
      " 50%|████▉     | 498/1000 [21:31<26:10,  3.13s/it]INFO:root:global_step: 498, logpy: 91.185, kl: 13.866, loss: -79.948\n",
      " 50%|████▉     | 499/1000 [21:34<26:03,  3.12s/it]INFO:root:global_step: 499, logpy: 91.518, kl: 13.885, loss: -80.236\n",
      " 50%|█████     | 500/1000 [21:38<26:35,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_500.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 500, logpy: 91.844, kl: 13.905, loss: -80.515\n",
      " 50%|█████     | 501/1000 [21:42<30:22,  3.65s/it]INFO:root:global_step: 501, logpy: 92.173, kl: 13.924, loss: -80.799\n",
      " 50%|█████     | 502/1000 [21:46<29:16,  3.53s/it]INFO:root:global_step: 502, logpy: 92.537, kl: 13.943, loss: -81.119\n",
      " 50%|█████     | 503/1000 [21:49<28:29,  3.44s/it]INFO:root:global_step: 503, logpy: 92.824, kl: 13.963, loss: -81.360\n",
      " 50%|█████     | 504/1000 [21:52<27:56,  3.38s/it]INFO:root:global_step: 504, logpy: 93.162, kl: 13.979, loss: -81.658\n",
      " 50%|█████     | 505/1000 [21:55<27:19,  3.31s/it]INFO:root:global_step: 505, logpy: 93.508, kl: 13.997, loss: -81.961\n",
      " 51%|█████     | 506/1000 [21:58<26:39,  3.24s/it]INFO:root:global_step: 506, logpy: 93.831, kl: 14.011, loss: -82.244\n",
      " 51%|█████     | 507/1000 [22:01<26:06,  3.18s/it]INFO:root:global_step: 507, logpy: 94.161, kl: 14.028, loss: -82.533\n",
      " 51%|█████     | 508/1000 [22:04<25:52,  3.16s/it]INFO:root:global_step: 508, logpy: 94.469, kl: 14.049, loss: -82.796\n",
      " 51%|█████     | 509/1000 [22:07<25:31,  3.12s/it]INFO:root:global_step: 509, logpy: 94.777, kl: 14.063, loss: -83.068\n",
      " 51%|█████     | 510/1000 [22:11<26:08,  3.20s/it]INFO:root:global_step: 510, logpy: 95.110, kl: 14.081, loss: -83.358\n",
      " 51%|█████     | 511/1000 [22:14<26:12,  3.22s/it]INFO:root:global_step: 511, logpy: 95.433, kl: 14.100, loss: -83.639\n",
      " 51%|█████     | 512/1000 [22:17<26:04,  3.21s/it]INFO:root:global_step: 512, logpy: 95.785, kl: 14.119, loss: -83.950\n",
      " 51%|█████▏    | 513/1000 [22:21<26:29,  3.26s/it]INFO:root:global_step: 513, logpy: 96.109, kl: 14.134, loss: -84.235\n",
      " 51%|█████▏    | 514/1000 [22:24<27:07,  3.35s/it]INFO:root:global_step: 514, logpy: 96.397, kl: 14.154, loss: -84.480\n",
      " 52%|█████▏    | 515/1000 [22:28<27:10,  3.36s/it]INFO:root:global_step: 515, logpy: 96.730, kl: 14.172, loss: -84.773\n",
      " 52%|█████▏    | 516/1000 [22:31<27:00,  3.35s/it]INFO:root:global_step: 516, logpy: 97.059, kl: 14.185, loss: -85.067\n",
      " 52%|█████▏    | 517/1000 [22:34<26:25,  3.28s/it]INFO:root:global_step: 517, logpy: 97.374, kl: 14.207, loss: -85.339\n",
      " 52%|█████▏    | 518/1000 [22:37<25:45,  3.21s/it]INFO:root:global_step: 518, logpy: 97.664, kl: 14.224, loss: -85.590\n",
      " 52%|█████▏    | 519/1000 [22:40<25:20,  3.16s/it]INFO:root:global_step: 519, logpy: 98.015, kl: 14.245, loss: -85.898\n",
      " 52%|█████▏    | 520/1000 [22:43<25:03,  3.13s/it]INFO:root:global_step: 520, logpy: 98.304, kl: 14.263, loss: -86.148\n",
      " 52%|█████▏    | 521/1000 [22:46<24:53,  3.12s/it]INFO:root:global_step: 521, logpy: 98.613, kl: 14.280, loss: -86.419\n",
      " 52%|█████▏    | 522/1000 [22:49<24:58,  3.13s/it]INFO:root:global_step: 522, logpy: 98.960, kl: 14.302, loss: -86.723\n",
      " 52%|█████▏    | 523/1000 [22:53<24:50,  3.12s/it]INFO:root:global_step: 523, logpy: 99.225, kl: 14.320, loss: -86.949\n",
      " 52%|█████▏    | 524/1000 [22:56<24:46,  3.12s/it]INFO:root:global_step: 524, logpy: 99.506, kl: 14.337, loss: -87.193\n",
      " 52%|█████▎    | 525/1000 [22:59<24:55,  3.15s/it]INFO:root:global_step: 525, logpy: 99.781, kl: 14.354, loss: -87.431\n",
      " 53%|█████▎    | 526/1000 [23:02<24:50,  3.15s/it]INFO:root:global_step: 526, logpy: 100.069, kl: 14.373, loss: -87.680\n",
      " 53%|█████▎    | 527/1000 [23:05<24:32,  3.11s/it]INFO:root:global_step: 527, logpy: 100.347, kl: 14.390, loss: -87.921\n",
      " 53%|█████▎    | 528/1000 [23:08<24:21,  3.10s/it]INFO:root:global_step: 528, logpy: 100.629, kl: 14.407, loss: -88.166\n",
      " 53%|█████▎    | 529/1000 [23:11<24:27,  3.11s/it]INFO:root:global_step: 529, logpy: 100.956, kl: 14.427, loss: -88.454\n",
      " 53%|█████▎    | 530/1000 [23:14<24:15,  3.10s/it]INFO:root:global_step: 530, logpy: 101.226, kl: 14.445, loss: -88.686\n",
      " 53%|█████▎    | 531/1000 [23:17<24:13,  3.10s/it]INFO:root:global_step: 531, logpy: 101.542, kl: 14.462, loss: -88.966\n",
      " 53%|█████▎    | 532/1000 [23:20<24:02,  3.08s/it]INFO:root:global_step: 532, logpy: 101.847, kl: 14.480, loss: -89.235\n",
      " 53%|█████▎    | 533/1000 [23:23<23:55,  3.07s/it]INFO:root:global_step: 533, logpy: 102.108, kl: 14.496, loss: -89.461\n",
      " 53%|█████▎    | 534/1000 [23:27<23:56,  3.08s/it]INFO:root:global_step: 534, logpy: 102.381, kl: 14.517, loss: -89.694\n",
      " 54%|█████▎    | 535/1000 [23:30<23:43,  3.06s/it]INFO:root:global_step: 535, logpy: 102.615, kl: 14.534, loss: -89.893\n",
      " 54%|█████▎    | 536/1000 [23:33<23:41,  3.06s/it]INFO:root:global_step: 536, logpy: 102.891, kl: 14.552, loss: -90.133\n",
      " 54%|█████▎    | 537/1000 [23:36<23:55,  3.10s/it]INFO:root:global_step: 537, logpy: 103.207, kl: 14.566, loss: -90.417\n",
      " 54%|█████▍    | 538/1000 [23:39<23:49,  3.09s/it]INFO:root:global_step: 538, logpy: 103.421, kl: 14.588, loss: -90.591\n",
      " 54%|█████▍    | 539/1000 [23:42<23:55,  3.11s/it]INFO:root:global_step: 539, logpy: 103.700, kl: 14.607, loss: -90.834\n",
      " 54%|█████▍    | 540/1000 [23:45<24:14,  3.16s/it]INFO:root:global_step: 540, logpy: 103.955, kl: 14.626, loss: -91.053\n",
      " 54%|█████▍    | 541/1000 [23:48<24:03,  3.15s/it]INFO:root:global_step: 541, logpy: 104.229, kl: 14.644, loss: -91.290\n",
      " 54%|█████▍    | 542/1000 [23:52<23:46,  3.11s/it]INFO:root:global_step: 542, logpy: 104.514, kl: 14.658, loss: -91.546\n",
      " 54%|█████▍    | 543/1000 [23:55<23:39,  3.11s/it]INFO:root:global_step: 543, logpy: 104.757, kl: 14.676, loss: -91.754\n",
      " 54%|█████▍    | 544/1000 [23:58<23:38,  3.11s/it]INFO:root:global_step: 544, logpy: 105.027, kl: 14.694, loss: -91.988\n",
      " 55%|█████▍    | 545/1000 [24:01<23:26,  3.09s/it]INFO:root:global_step: 545, logpy: 105.261, kl: 14.714, loss: -92.186\n",
      " 55%|█████▍    | 546/1000 [24:04<23:16,  3.08s/it]INFO:root:global_step: 546, logpy: 105.499, kl: 14.732, loss: -92.390\n",
      " 55%|█████▍    | 547/1000 [24:07<23:15,  3.08s/it]INFO:root:global_step: 547, logpy: 105.765, kl: 14.751, loss: -92.620\n",
      " 55%|█████▍    | 548/1000 [24:10<23:30,  3.12s/it]INFO:root:global_step: 548, logpy: 105.987, kl: 14.770, loss: -92.807\n",
      " 55%|█████▍    | 549/1000 [24:13<23:31,  3.13s/it]INFO:root:global_step: 549, logpy: 106.189, kl: 14.786, loss: -92.977\n",
      " 55%|█████▌    | 550/1000 [24:16<23:21,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_550.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 550, logpy: 106.421, kl: 14.802, loss: -93.178\n",
      " 55%|█████▌    | 551/1000 [24:21<26:22,  3.52s/it]INFO:root:global_step: 551, logpy: 106.690, kl: 14.819, loss: -93.414\n",
      " 55%|█████▌    | 552/1000 [24:24<25:23,  3.40s/it]INFO:root:global_step: 552, logpy: 106.937, kl: 14.835, loss: -93.629\n",
      " 55%|█████▌    | 553/1000 [24:27<25:05,  3.37s/it]INFO:root:global_step: 553, logpy: 107.179, kl: 14.853, loss: -93.838\n",
      " 55%|█████▌    | 554/1000 [24:30<24:34,  3.31s/it]INFO:root:global_step: 554, logpy: 107.439, kl: 14.871, loss: -94.065\n",
      " 56%|█████▌    | 555/1000 [24:33<24:01,  3.24s/it]INFO:root:global_step: 555, logpy: 107.681, kl: 14.891, loss: -94.272\n",
      " 56%|█████▌    | 556/1000 [24:37<23:31,  3.18s/it]INFO:root:global_step: 556, logpy: 107.901, kl: 14.909, loss: -94.459\n",
      " 56%|█████▌    | 557/1000 [24:40<23:14,  3.15s/it]INFO:root:global_step: 557, logpy: 108.147, kl: 14.928, loss: -94.671\n",
      " 56%|█████▌    | 558/1000 [24:43<23:13,  3.15s/it]INFO:root:global_step: 558, logpy: 108.384, kl: 14.946, loss: -94.875\n",
      " 56%|█████▌    | 559/1000 [24:46<22:56,  3.12s/it]INFO:root:global_step: 559, logpy: 108.593, kl: 14.968, loss: -95.049\n",
      " 56%|█████▌    | 560/1000 [24:49<22:46,  3.11s/it]INFO:root:global_step: 560, logpy: 108.822, kl: 14.986, loss: -95.245\n",
      " 56%|█████▌    | 561/1000 [24:52<22:41,  3.10s/it]INFO:root:global_step: 561, logpy: 109.095, kl: 15.005, loss: -95.485\n",
      " 56%|█████▌    | 562/1000 [24:55<22:44,  3.12s/it]INFO:root:global_step: 562, logpy: 109.335, kl: 15.027, loss: -95.689\n",
      " 56%|█████▋    | 563/1000 [24:58<22:38,  3.11s/it]INFO:root:global_step: 563, logpy: 109.561, kl: 15.045, loss: -95.883\n",
      " 56%|█████▋    | 564/1000 [25:01<22:41,  3.12s/it]INFO:root:global_step: 564, logpy: 109.795, kl: 15.057, loss: -96.092\n",
      " 56%|█████▋    | 565/1000 [25:05<22:42,  3.13s/it]INFO:root:global_step: 565, logpy: 109.989, kl: 15.080, loss: -96.249\n",
      " 57%|█████▋    | 566/1000 [25:08<22:40,  3.14s/it]INFO:root:global_step: 566, logpy: 110.210, kl: 15.099, loss: -96.438\n",
      " 57%|█████▋    | 567/1000 [25:11<22:46,  3.15s/it]INFO:root:global_step: 567, logpy: 110.447, kl: 15.116, loss: -96.645\n",
      " 57%|█████▋    | 568/1000 [25:14<22:48,  3.17s/it]INFO:root:global_step: 568, logpy: 110.653, kl: 15.135, loss: -96.819\n",
      " 57%|█████▋    | 569/1000 [25:17<22:46,  3.17s/it]INFO:root:global_step: 569, logpy: 110.886, kl: 15.151, loss: -97.023\n",
      " 57%|█████▋    | 570/1000 [25:20<22:52,  3.19s/it]INFO:root:global_step: 570, logpy: 111.079, kl: 15.168, loss: -97.185\n",
      " 57%|█████▋    | 571/1000 [25:24<22:34,  3.16s/it]INFO:root:global_step: 571, logpy: 111.272, kl: 15.190, loss: -97.343\n",
      " 57%|█████▋    | 572/1000 [25:27<22:24,  3.14s/it]INFO:root:global_step: 572, logpy: 111.461, kl: 15.210, loss: -97.500\n",
      " 57%|█████▋    | 573/1000 [25:30<22:16,  3.13s/it]INFO:root:global_step: 573, logpy: 111.674, kl: 15.230, loss: -97.681\n",
      " 57%|█████▋    | 574/1000 [25:33<22:08,  3.12s/it]INFO:root:global_step: 574, logpy: 111.874, kl: 15.250, loss: -97.848\n",
      " 57%|█████▊    | 575/1000 [25:36<22:03,  3.11s/it]INFO:root:global_step: 575, logpy: 112.095, kl: 15.268, loss: -98.039\n",
      " 58%|█████▊    | 576/1000 [25:39<21:52,  3.09s/it]INFO:root:global_step: 576, logpy: 112.259, kl: 15.287, loss: -98.173\n",
      " 58%|█████▊    | 577/1000 [25:42<22:00,  3.12s/it]INFO:root:global_step: 577, logpy: 112.440, kl: 15.305, loss: -98.324\n",
      " 58%|█████▊    | 578/1000 [25:45<22:12,  3.16s/it]INFO:root:global_step: 578, logpy: 112.606, kl: 15.324, loss: -98.458\n",
      " 58%|█████▊    | 579/1000 [25:49<22:11,  3.16s/it]INFO:root:global_step: 579, logpy: 112.790, kl: 15.346, loss: -98.608\n",
      " 58%|█████▊    | 580/1000 [25:52<21:56,  3.13s/it]INFO:root:global_step: 580, logpy: 112.991, kl: 15.366, loss: -98.777\n",
      " 58%|█████▊    | 581/1000 [25:55<21:40,  3.10s/it]INFO:root:global_step: 581, logpy: 113.168, kl: 15.388, loss: -98.922\n",
      " 58%|█████▊    | 582/1000 [25:58<21:49,  3.13s/it]INFO:root:global_step: 582, logpy: 113.357, kl: 15.403, loss: -99.084\n",
      " 58%|█████▊    | 583/1000 [26:01<21:49,  3.14s/it]INFO:root:global_step: 583, logpy: 113.516, kl: 15.424, loss: -99.210\n",
      " 58%|█████▊    | 584/1000 [26:04<21:42,  3.13s/it]INFO:root:global_step: 584, logpy: 113.703, kl: 15.443, loss: -99.366\n",
      " 58%|█████▊    | 585/1000 [26:07<21:31,  3.11s/it]INFO:root:global_step: 585, logpy: 113.917, kl: 15.462, loss: -99.550\n",
      " 59%|█████▊    | 586/1000 [26:10<21:30,  3.12s/it]INFO:root:global_step: 586, logpy: 114.079, kl: 15.484, loss: -99.679\n",
      " 59%|█████▊    | 587/1000 [26:13<21:22,  3.11s/it]INFO:root:global_step: 587, logpy: 114.254, kl: 15.505, loss: -99.823\n",
      " 59%|█████▉    | 588/1000 [26:17<21:16,  3.10s/it]INFO:root:global_step: 588, logpy: 114.416, kl: 15.526, loss: -99.954\n",
      " 59%|█████▉    | 589/1000 [26:20<21:08,  3.09s/it]INFO:root:global_step: 589, logpy: 114.621, kl: 15.543, loss: -100.131\n",
      " 59%|█████▉    | 590/1000 [26:23<21:12,  3.10s/it]INFO:root:global_step: 590, logpy: 114.788, kl: 15.560, loss: -100.271\n",
      " 59%|█████▉    | 591/1000 [26:26<21:13,  3.11s/it]INFO:root:global_step: 591, logpy: 114.994, kl: 15.580, loss: -100.446\n",
      " 59%|█████▉    | 592/1000 [26:29<21:01,  3.09s/it]INFO:root:global_step: 592, logpy: 115.152, kl: 15.598, loss: -100.576\n",
      " 59%|█████▉    | 593/1000 [26:32<21:01,  3.10s/it]INFO:root:global_step: 593, logpy: 115.332, kl: 15.612, loss: -100.731\n",
      " 59%|█████▉    | 594/1000 [26:35<21:15,  3.14s/it]INFO:root:global_step: 594, logpy: 115.534, kl: 15.627, loss: -100.908\n",
      " 60%|█████▉    | 595/1000 [26:38<21:14,  3.15s/it]INFO:root:global_step: 595, logpy: 115.765, kl: 15.646, loss: -101.110\n",
      " 60%|█████▉    | 596/1000 [26:42<21:07,  3.14s/it]INFO:root:global_step: 596, logpy: 116.008, kl: 15.664, loss: -101.325\n",
      " 60%|█████▉    | 597/1000 [26:45<20:57,  3.12s/it]INFO:root:global_step: 597, logpy: 116.210, kl: 15.688, loss: -101.494\n",
      " 60%|█████▉    | 598/1000 [26:48<20:56,  3.12s/it]INFO:root:global_step: 598, logpy: 116.392, kl: 15.709, loss: -101.645\n",
      " 60%|█████▉    | 599/1000 [26:51<20:51,  3.12s/it]INFO:root:global_step: 599, logpy: 116.601, kl: 15.723, loss: -101.830\n",
      " 60%|██████    | 600/1000 [26:54<20:57,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_600.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 600, logpy: 116.753, kl: 15.743, loss: -101.953\n",
      " 60%|██████    | 601/1000 [26:58<23:17,  3.50s/it]INFO:root:global_step: 601, logpy: 116.934, kl: 15.763, loss: -102.104\n",
      " 60%|██████    | 602/1000 [27:02<22:36,  3.41s/it]INFO:root:global_step: 602, logpy: 117.116, kl: 15.781, loss: -102.260\n",
      " 60%|██████    | 603/1000 [27:05<22:13,  3.36s/it]INFO:root:global_step: 603, logpy: 117.293, kl: 15.797, loss: -102.411\n",
      " 60%|██████    | 604/1000 [27:08<21:44,  3.30s/it]INFO:root:global_step: 604, logpy: 117.462, kl: 15.813, loss: -102.555\n",
      " 60%|██████    | 605/1000 [27:11<21:39,  3.29s/it]INFO:root:global_step: 605, logpy: 117.614, kl: 15.832, loss: -102.679\n",
      " 61%|██████    | 606/1000 [27:14<21:21,  3.25s/it]INFO:root:global_step: 606, logpy: 117.831, kl: 15.849, loss: -102.870\n",
      " 61%|██████    | 607/1000 [27:18<21:03,  3.21s/it]INFO:root:global_step: 607, logpy: 118.006, kl: 15.868, loss: -103.017\n",
      " 61%|██████    | 608/1000 [27:21<20:51,  3.19s/it]INFO:root:global_step: 608, logpy: 118.202, kl: 15.885, loss: -103.187\n",
      " 61%|██████    | 609/1000 [27:24<20:38,  3.17s/it]INFO:root:global_step: 609, logpy: 118.366, kl: 15.903, loss: -103.324\n",
      " 61%|██████    | 610/1000 [27:27<20:26,  3.14s/it]INFO:root:global_step: 610, logpy: 118.547, kl: 15.919, loss: -103.481\n",
      " 61%|██████    | 611/1000 [27:30<20:23,  3.15s/it]INFO:root:global_step: 611, logpy: 118.717, kl: 15.937, loss: -103.625\n",
      " 61%|██████    | 612/1000 [27:33<20:19,  3.14s/it]INFO:root:global_step: 612, logpy: 118.867, kl: 15.953, loss: -103.750\n",
      " 61%|██████▏   | 613/1000 [27:36<20:08,  3.12s/it]INFO:root:global_step: 613, logpy: 119.048, kl: 15.972, loss: -103.904\n",
      " 61%|██████▏   | 614/1000 [27:39<19:56,  3.10s/it]INFO:root:global_step: 614, logpy: 119.212, kl: 15.991, loss: -104.041\n",
      " 62%|██████▏   | 615/1000 [27:43<20:09,  3.14s/it]INFO:root:global_step: 615, logpy: 119.387, kl: 16.012, loss: -104.186\n",
      " 62%|██████▏   | 616/1000 [27:46<20:13,  3.16s/it]INFO:root:global_step: 616, logpy: 119.529, kl: 16.030, loss: -104.302\n",
      " 62%|██████▏   | 617/1000 [27:49<20:08,  3.16s/it]INFO:root:global_step: 617, logpy: 119.699, kl: 16.049, loss: -104.444\n",
      " 62%|██████▏   | 618/1000 [27:52<20:12,  3.17s/it]INFO:root:global_step: 618, logpy: 119.860, kl: 16.067, loss: -104.580\n",
      " 62%|██████▏   | 619/1000 [27:55<20:02,  3.16s/it]INFO:root:global_step: 619, logpy: 120.000, kl: 16.089, loss: -104.690\n",
      " 62%|██████▏   | 620/1000 [27:58<19:53,  3.14s/it]INFO:root:global_step: 620, logpy: 120.153, kl: 16.110, loss: -104.814\n",
      " 62%|██████▏   | 621/1000 [28:01<19:52,  3.15s/it]INFO:root:global_step: 621, logpy: 120.314, kl: 16.129, loss: -104.948\n",
      " 62%|██████▏   | 622/1000 [28:05<19:45,  3.14s/it]INFO:root:global_step: 622, logpy: 120.468, kl: 16.148, loss: -105.076\n",
      " 62%|██████▏   | 623/1000 [28:08<19:47,  3.15s/it]INFO:root:global_step: 623, logpy: 120.634, kl: 16.167, loss: -105.216\n",
      " 62%|██████▏   | 624/1000 [28:11<19:46,  3.16s/it]INFO:root:global_step: 624, logpy: 120.788, kl: 16.183, loss: -105.346\n",
      " 62%|██████▎   | 625/1000 [28:14<19:44,  3.16s/it]INFO:root:global_step: 625, logpy: 120.945, kl: 16.202, loss: -105.477\n",
      " 63%|██████▎   | 626/1000 [28:17<19:32,  3.14s/it]INFO:root:global_step: 626, logpy: 121.109, kl: 16.218, loss: -105.617\n",
      " 63%|██████▎   | 627/1000 [28:20<19:37,  3.16s/it]INFO:root:global_step: 627, logpy: 121.258, kl: 16.239, loss: -105.738\n",
      " 63%|██████▎   | 628/1000 [28:23<19:25,  3.13s/it]INFO:root:global_step: 628, logpy: 121.418, kl: 16.255, loss: -105.875\n",
      " 63%|██████▎   | 629/1000 [28:27<19:29,  3.15s/it]INFO:root:global_step: 629, logpy: 121.594, kl: 16.277, loss: -106.022\n",
      " 63%|██████▎   | 630/1000 [28:30<19:35,  3.18s/it]INFO:root:global_step: 630, logpy: 121.753, kl: 16.297, loss: -106.154\n",
      " 63%|██████▎   | 631/1000 [28:33<19:22,  3.15s/it]INFO:root:global_step: 631, logpy: 121.912, kl: 16.313, loss: -106.289\n",
      " 63%|██████▎   | 632/1000 [28:36<19:14,  3.14s/it]INFO:root:global_step: 632, logpy: 122.087, kl: 16.333, loss: -106.438\n",
      " 63%|██████▎   | 633/1000 [28:39<19:12,  3.14s/it]INFO:root:global_step: 633, logpy: 122.251, kl: 16.349, loss: -106.579\n",
      " 63%|██████▎   | 634/1000 [28:42<19:05,  3.13s/it]INFO:root:global_step: 634, logpy: 122.385, kl: 16.366, loss: -106.689\n",
      " 64%|██████▎   | 635/1000 [28:45<18:59,  3.12s/it]INFO:root:global_step: 635, logpy: 122.520, kl: 16.384, loss: -106.798\n",
      " 64%|██████▎   | 636/1000 [28:49<18:58,  3.13s/it]INFO:root:global_step: 636, logpy: 122.638, kl: 16.407, loss: -106.887\n",
      " 64%|██████▎   | 637/1000 [28:52<18:52,  3.12s/it]INFO:root:global_step: 637, logpy: 122.751, kl: 16.426, loss: -106.975\n",
      " 64%|██████▍   | 638/1000 [28:55<18:51,  3.12s/it]INFO:root:global_step: 638, logpy: 122.928, kl: 16.446, loss: -107.126\n",
      " 64%|██████▍   | 639/1000 [28:58<18:45,  3.12s/it]INFO:root:global_step: 639, logpy: 123.083, kl: 16.458, loss: -107.262\n",
      " 64%|██████▍   | 640/1000 [29:01<18:47,  3.13s/it]INFO:root:global_step: 640, logpy: 123.233, kl: 16.476, loss: -107.388\n",
      " 64%|██████▍   | 641/1000 [29:04<18:51,  3.15s/it]INFO:root:global_step: 641, logpy: 123.399, kl: 16.493, loss: -107.531\n",
      " 64%|██████▍   | 642/1000 [29:08<18:55,  3.17s/it]INFO:root:global_step: 642, logpy: 123.572, kl: 16.509, loss: -107.681\n",
      " 64%|██████▍   | 643/1000 [29:11<18:58,  3.19s/it]INFO:root:global_step: 643, logpy: 123.718, kl: 16.525, loss: -107.805\n",
      " 64%|██████▍   | 644/1000 [29:14<18:49,  3.17s/it]INFO:root:global_step: 644, logpy: 123.850, kl: 16.547, loss: -107.909\n",
      " 64%|██████▍   | 645/1000 [29:17<18:43,  3.17s/it]INFO:root:global_step: 645, logpy: 124.035, kl: 16.564, loss: -108.071\n",
      " 65%|██████▍   | 646/1000 [29:20<18:45,  3.18s/it]INFO:root:global_step: 646, logpy: 124.175, kl: 16.584, loss: -108.185\n",
      " 65%|██████▍   | 647/1000 [29:23<18:36,  3.16s/it]INFO:root:global_step: 647, logpy: 124.315, kl: 16.598, loss: -108.304\n",
      " 65%|██████▍   | 648/1000 [29:27<18:29,  3.15s/it]INFO:root:global_step: 648, logpy: 124.458, kl: 16.616, loss: -108.424\n",
      " 65%|██████▍   | 649/1000 [29:30<18:32,  3.17s/it]INFO:root:global_step: 649, logpy: 124.608, kl: 16.634, loss: -108.550\n",
      " 65%|██████▌   | 650/1000 [29:33<18:24,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_650.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 650, logpy: 124.724, kl: 16.655, loss: -108.640\n",
      " 65%|██████▌   | 651/1000 [29:37<20:36,  3.54s/it]INFO:root:global_step: 651, logpy: 124.848, kl: 16.672, loss: -108.741\n",
      " 65%|██████▌   | 652/1000 [29:41<20:15,  3.49s/it]INFO:root:global_step: 652, logpy: 125.003, kl: 16.684, loss: -108.878\n",
      " 65%|██████▌   | 653/1000 [29:44<19:56,  3.45s/it]INFO:root:global_step: 653, logpy: 125.120, kl: 16.706, loss: -108.967\n",
      " 65%|██████▌   | 654/1000 [29:47<19:29,  3.38s/it]INFO:root:global_step: 654, logpy: 125.255, kl: 16.723, loss: -109.080\n",
      " 66%|██████▌   | 655/1000 [29:50<19:01,  3.31s/it]INFO:root:global_step: 655, logpy: 125.417, kl: 16.736, loss: -109.224\n",
      " 66%|██████▌   | 656/1000 [29:54<18:42,  3.26s/it]INFO:root:global_step: 656, logpy: 125.559, kl: 16.760, loss: -109.335\n",
      " 66%|██████▌   | 657/1000 [29:57<18:28,  3.23s/it]INFO:root:global_step: 657, logpy: 125.694, kl: 16.780, loss: -109.446\n",
      " 66%|██████▌   | 658/1000 [30:00<18:17,  3.21s/it]INFO:root:global_step: 658, logpy: 125.814, kl: 16.793, loss: -109.547\n",
      " 66%|██████▌   | 659/1000 [30:03<18:06,  3.19s/it]INFO:root:global_step: 659, logpy: 125.921, kl: 16.808, loss: -109.634\n",
      " 66%|██████▌   | 660/1000 [30:06<18:05,  3.19s/it]INFO:root:global_step: 660, logpy: 126.045, kl: 16.831, loss: -109.730\n",
      " 66%|██████▌   | 661/1000 [30:09<17:59,  3.18s/it]INFO:root:global_step: 661, logpy: 126.148, kl: 16.850, loss: -109.809\n",
      " 66%|██████▌   | 662/1000 [30:13<17:53,  3.18s/it]INFO:root:global_step: 662, logpy: 126.309, kl: 16.867, loss: -109.948\n",
      " 66%|██████▋   | 663/1000 [30:16<17:48,  3.17s/it]INFO:root:global_step: 663, logpy: 126.449, kl: 16.885, loss: -110.064\n",
      " 66%|██████▋   | 664/1000 [30:19<17:41,  3.16s/it]INFO:root:global_step: 664, logpy: 126.609, kl: 16.901, loss: -110.204\n",
      " 66%|██████▋   | 665/1000 [30:22<17:49,  3.19s/it]INFO:root:global_step: 665, logpy: 126.727, kl: 16.918, loss: -110.300\n",
      " 67%|██████▋   | 666/1000 [30:25<17:44,  3.19s/it]INFO:root:global_step: 666, logpy: 126.880, kl: 16.938, loss: -110.428\n",
      " 67%|██████▋   | 667/1000 [30:28<17:35,  3.17s/it]INFO:root:global_step: 667, logpy: 127.014, kl: 16.954, loss: -110.541\n",
      " 67%|██████▋   | 668/1000 [30:32<17:39,  3.19s/it]INFO:root:global_step: 668, logpy: 127.152, kl: 16.970, loss: -110.658\n",
      " 67%|██████▋   | 669/1000 [30:35<17:28,  3.17s/it]INFO:root:global_step: 669, logpy: 127.290, kl: 16.990, loss: -110.772\n",
      " 67%|██████▋   | 670/1000 [30:38<17:26,  3.17s/it]INFO:root:global_step: 670, logpy: 127.411, kl: 17.007, loss: -110.871\n",
      " 67%|██████▋   | 671/1000 [30:41<17:53,  3.26s/it]INFO:root:global_step: 671, logpy: 127.549, kl: 17.024, loss: -110.987\n",
      " 67%|██████▋   | 672/1000 [30:45<18:11,  3.33s/it]INFO:root:global_step: 672, logpy: 127.677, kl: 17.042, loss: -111.093\n",
      " 67%|██████▋   | 673/1000 [30:49<18:47,  3.45s/it]INFO:root:global_step: 673, logpy: 127.830, kl: 17.056, loss: -111.227\n",
      " 67%|██████▋   | 674/1000 [30:52<18:46,  3.46s/it]INFO:root:global_step: 674, logpy: 127.928, kl: 17.075, loss: -111.301\n",
      " 68%|██████▊   | 675/1000 [30:56<18:43,  3.46s/it]INFO:root:global_step: 675, logpy: 128.040, kl: 17.095, loss: -111.388\n",
      " 68%|██████▊   | 676/1000 [30:59<18:19,  3.39s/it]INFO:root:global_step: 676, logpy: 128.141, kl: 17.113, loss: -111.467\n",
      " 68%|██████▊   | 677/1000 [31:02<18:14,  3.39s/it]INFO:root:global_step: 677, logpy: 128.271, kl: 17.129, loss: -111.578\n",
      " 68%|██████▊   | 678/1000 [31:05<17:56,  3.34s/it]INFO:root:global_step: 678, logpy: 128.395, kl: 17.144, loss: -111.681\n",
      " 68%|██████▊   | 679/1000 [31:09<17:43,  3.31s/it]INFO:root:global_step: 679, logpy: 128.506, kl: 17.165, loss: -111.767\n",
      " 68%|██████▊   | 680/1000 [31:12<17:40,  3.32s/it]INFO:root:global_step: 680, logpy: 128.617, kl: 17.182, loss: -111.857\n",
      " 68%|██████▊   | 681/1000 [31:15<17:38,  3.32s/it]INFO:root:global_step: 681, logpy: 128.711, kl: 17.202, loss: -111.927\n",
      " 68%|██████▊   | 682/1000 [31:19<17:31,  3.31s/it]INFO:root:global_step: 682, logpy: 128.829, kl: 17.224, loss: -112.019\n",
      " 68%|██████▊   | 683/1000 [31:22<17:30,  3.31s/it]INFO:root:global_step: 683, logpy: 128.941, kl: 17.243, loss: -112.107\n",
      " 68%|██████▊   | 684/1000 [31:25<17:30,  3.32s/it]INFO:root:global_step: 684, logpy: 129.079, kl: 17.261, loss: -112.223\n",
      " 68%|██████▊   | 685/1000 [31:28<17:12,  3.28s/it]INFO:root:global_step: 685, logpy: 129.182, kl: 17.281, loss: -112.302\n",
      " 69%|██████▊   | 686/1000 [31:32<17:03,  3.26s/it]INFO:root:global_step: 686, logpy: 129.311, kl: 17.301, loss: -112.407\n",
      " 69%|██████▊   | 687/1000 [31:35<16:51,  3.23s/it]INFO:root:global_step: 687, logpy: 129.445, kl: 17.321, loss: -112.518\n",
      " 69%|██████▉   | 688/1000 [31:38<16:43,  3.22s/it]INFO:root:global_step: 688, logpy: 129.541, kl: 17.340, loss: -112.590\n",
      " 69%|██████▉   | 689/1000 [31:41<16:43,  3.23s/it]INFO:root:global_step: 689, logpy: 129.673, kl: 17.358, loss: -112.701\n",
      " 69%|██████▉   | 690/1000 [31:44<16:40,  3.23s/it]INFO:root:global_step: 690, logpy: 129.783, kl: 17.381, loss: -112.783\n",
      " 69%|██████▉   | 691/1000 [31:48<16:44,  3.25s/it]INFO:root:global_step: 691, logpy: 129.900, kl: 17.398, loss: -112.880\n",
      " 69%|██████▉   | 692/1000 [31:51<16:36,  3.24s/it]INFO:root:global_step: 692, logpy: 130.010, kl: 17.410, loss: -112.974\n",
      " 69%|██████▉   | 693/1000 [31:54<16:26,  3.21s/it]INFO:root:global_step: 693, logpy: 130.109, kl: 17.425, loss: -113.054\n",
      " 69%|██████▉   | 694/1000 [31:57<16:20,  3.21s/it]INFO:root:global_step: 694, logpy: 130.238, kl: 17.444, loss: -113.161\n",
      " 70%|██████▉   | 695/1000 [32:00<16:11,  3.19s/it]INFO:root:global_step: 695, logpy: 130.367, kl: 17.463, loss: -113.267\n",
      " 70%|██████▉   | 696/1000 [32:04<16:03,  3.17s/it]INFO:root:global_step: 696, logpy: 130.475, kl: 17.481, loss: -113.353\n",
      " 70%|██████▉   | 697/1000 [32:07<15:59,  3.17s/it]INFO:root:global_step: 697, logpy: 130.621, kl: 17.499, loss: -113.478\n",
      " 70%|██████▉   | 698/1000 [32:10<15:58,  3.17s/it]INFO:root:global_step: 698, logpy: 130.713, kl: 17.519, loss: -113.546\n",
      " 70%|██████▉   | 699/1000 [32:13<15:56,  3.18s/it]INFO:root:global_step: 699, logpy: 130.818, kl: 17.539, loss: -113.627\n",
      " 70%|███████   | 700/1000 [32:16<15:53,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_700.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 700, logpy: 130.921, kl: 17.560, loss: -113.706\n",
      " 70%|███████   | 701/1000 [32:21<17:46,  3.57s/it]INFO:root:global_step: 701, logpy: 131.025, kl: 17.580, loss: -113.787\n",
      " 70%|███████   | 702/1000 [32:24<17:08,  3.45s/it]INFO:root:global_step: 702, logpy: 131.144, kl: 17.596, loss: -113.886\n",
      " 70%|███████   | 703/1000 [32:27<16:53,  3.41s/it]INFO:root:global_step: 703, logpy: 131.246, kl: 17.613, loss: -113.968\n",
      " 70%|███████   | 704/1000 [32:31<16:38,  3.37s/it]INFO:root:global_step: 704, logpy: 131.334, kl: 17.633, loss: -114.033\n",
      " 70%|███████   | 705/1000 [32:34<16:25,  3.34s/it]INFO:root:global_step: 705, logpy: 131.466, kl: 17.647, loss: -114.147\n",
      " 71%|███████   | 706/1000 [32:37<16:15,  3.32s/it]INFO:root:global_step: 706, logpy: 131.579, kl: 17.671, loss: -114.233\n",
      " 71%|███████   | 707/1000 [32:40<15:56,  3.26s/it]INFO:root:global_step: 707, logpy: 131.710, kl: 17.685, loss: -114.347\n",
      " 71%|███████   | 708/1000 [32:44<15:56,  3.28s/it]INFO:root:global_step: 708, logpy: 131.833, kl: 17.700, loss: -114.451\n",
      " 71%|███████   | 709/1000 [32:47<15:44,  3.25s/it]INFO:root:global_step: 709, logpy: 131.963, kl: 17.720, loss: -114.558\n",
      " 71%|███████   | 710/1000 [32:50<15:31,  3.21s/it]INFO:root:global_step: 710, logpy: 132.077, kl: 17.740, loss: -114.649\n",
      " 71%|███████   | 711/1000 [32:53<15:23,  3.20s/it]INFO:root:global_step: 711, logpy: 132.159, kl: 17.760, loss: -114.708\n",
      " 71%|███████   | 712/1000 [32:56<15:15,  3.18s/it]INFO:root:global_step: 712, logpy: 132.267, kl: 17.780, loss: -114.793\n",
      " 71%|███████▏  | 713/1000 [32:59<15:24,  3.22s/it]INFO:root:global_step: 713, logpy: 132.350, kl: 17.796, loss: -114.857\n",
      " 71%|███████▏  | 714/1000 [33:03<15:22,  3.22s/it]INFO:root:global_step: 714, logpy: 132.474, kl: 17.815, loss: -114.958\n",
      " 72%|███████▏  | 715/1000 [33:06<15:14,  3.21s/it]INFO:root:global_step: 715, logpy: 132.581, kl: 17.829, loss: -115.048\n",
      " 72%|███████▏  | 716/1000 [33:09<15:21,  3.25s/it]INFO:root:global_step: 716, logpy: 132.694, kl: 17.851, loss: -115.136\n",
      " 72%|███████▏  | 717/1000 [33:13<15:55,  3.38s/it]INFO:root:global_step: 717, logpy: 132.805, kl: 17.865, loss: -115.231\n",
      " 72%|███████▏  | 718/1000 [33:16<15:43,  3.35s/it]INFO:root:global_step: 718, logpy: 132.913, kl: 17.885, loss: -115.316\n",
      " 72%|███████▏  | 719/1000 [33:19<15:33,  3.32s/it]INFO:root:global_step: 719, logpy: 133.027, kl: 17.903, loss: -115.409\n",
      " 72%|███████▏  | 720/1000 [33:23<15:30,  3.32s/it]INFO:root:global_step: 720, logpy: 133.091, kl: 17.921, loss: -115.452\n",
      " 72%|███████▏  | 721/1000 [33:26<15:21,  3.30s/it]INFO:root:global_step: 721, logpy: 133.206, kl: 17.937, loss: -115.549\n",
      " 72%|███████▏  | 722/1000 [33:29<15:05,  3.26s/it]INFO:root:global_step: 722, logpy: 133.305, kl: 17.955, loss: -115.627\n",
      " 72%|███████▏  | 723/1000 [33:32<14:56,  3.24s/it]INFO:root:global_step: 723, logpy: 133.389, kl: 17.972, loss: -115.691\n",
      " 72%|███████▏  | 724/1000 [33:36<14:49,  3.22s/it]INFO:root:global_step: 724, logpy: 133.467, kl: 17.988, loss: -115.750\n",
      " 72%|███████▎  | 725/1000 [33:39<14:45,  3.22s/it]INFO:root:global_step: 725, logpy: 133.540, kl: 18.011, loss: -115.797\n",
      " 73%|███████▎  | 726/1000 [33:42<14:41,  3.22s/it]INFO:root:global_step: 726, logpy: 133.654, kl: 18.026, loss: -115.894\n",
      " 73%|███████▎  | 727/1000 [33:45<14:59,  3.30s/it]INFO:root:global_step: 727, logpy: 133.755, kl: 18.048, loss: -115.970\n",
      " 73%|███████▎  | 728/1000 [33:49<14:51,  3.28s/it]INFO:root:global_step: 728, logpy: 133.835, kl: 18.066, loss: -116.030\n",
      " 73%|███████▎  | 729/1000 [33:52<14:50,  3.28s/it]INFO:root:global_step: 729, logpy: 133.966, kl: 18.083, loss: -116.141\n",
      " 73%|███████▎  | 730/1000 [33:56<16:01,  3.56s/it]INFO:root:global_step: 730, logpy: 134.079, kl: 18.096, loss: -116.238\n",
      " 73%|███████▎  | 731/1000 [34:00<16:08,  3.60s/it]INFO:root:global_step: 731, logpy: 134.147, kl: 18.110, loss: -116.289\n",
      " 73%|███████▎  | 732/1000 [34:04<16:12,  3.63s/it]INFO:root:global_step: 732, logpy: 134.207, kl: 18.130, loss: -116.328\n",
      " 73%|███████▎  | 733/1000 [34:07<15:54,  3.57s/it]INFO:root:global_step: 733, logpy: 134.285, kl: 18.149, loss: -116.383\n",
      " 73%|███████▎  | 734/1000 [34:10<15:41,  3.54s/it]INFO:root:global_step: 734, logpy: 134.365, kl: 18.168, loss: -116.442\n",
      " 74%|███████▎  | 735/1000 [34:14<15:43,  3.56s/it]INFO:root:global_step: 735, logpy: 134.444, kl: 18.189, loss: -116.498\n",
      " 74%|███████▎  | 736/1000 [34:18<15:48,  3.59s/it]INFO:root:global_step: 736, logpy: 134.556, kl: 18.206, loss: -116.591\n",
      " 74%|███████▎  | 737/1000 [34:22<16:10,  3.69s/it]INFO:root:global_step: 737, logpy: 134.654, kl: 18.221, loss: -116.670\n",
      " 74%|███████▍  | 738/1000 [34:25<16:07,  3.69s/it]INFO:root:global_step: 738, logpy: 134.732, kl: 18.239, loss: -116.728\n",
      " 74%|███████▍  | 739/1000 [34:29<15:49,  3.64s/it]INFO:root:global_step: 739, logpy: 134.851, kl: 18.260, loss: -116.825\n",
      " 74%|███████▍  | 740/1000 [34:32<15:27,  3.57s/it]INFO:root:global_step: 740, logpy: 134.940, kl: 18.281, loss: -116.890\n",
      " 74%|███████▍  | 741/1000 [34:36<15:13,  3.53s/it]INFO:root:global_step: 741, logpy: 135.023, kl: 18.299, loss: -116.953\n",
      " 74%|███████▍  | 742/1000 [34:39<14:59,  3.48s/it]INFO:root:global_step: 742, logpy: 135.124, kl: 18.316, loss: -117.034\n",
      " 74%|███████▍  | 743/1000 [34:42<14:37,  3.41s/it]INFO:root:global_step: 743, logpy: 135.239, kl: 18.332, loss: -117.132\n",
      " 74%|███████▍  | 744/1000 [34:46<14:21,  3.36s/it]INFO:root:global_step: 744, logpy: 135.313, kl: 18.347, loss: -117.188\n",
      " 74%|███████▍  | 745/1000 [34:49<14:12,  3.34s/it]INFO:root:global_step: 745, logpy: 135.369, kl: 18.365, loss: -117.224\n",
      " 75%|███████▍  | 746/1000 [34:52<13:59,  3.31s/it]INFO:root:global_step: 746, logpy: 135.454, kl: 18.382, loss: -117.290\n",
      " 75%|███████▍  | 747/1000 [34:56<14:08,  3.35s/it]INFO:root:global_step: 747, logpy: 135.535, kl: 18.396, loss: -117.354\n",
      " 75%|███████▍  | 748/1000 [34:59<14:21,  3.42s/it]INFO:root:global_step: 748, logpy: 135.615, kl: 18.410, loss: -117.418\n",
      " 75%|███████▍  | 749/1000 [35:02<14:15,  3.41s/it]INFO:root:global_step: 749, logpy: 135.693, kl: 18.426, loss: -117.477\n",
      " 75%|███████▌  | 750/1000 [35:06<14:14,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_750.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 750, logpy: 135.799, kl: 18.442, loss: -117.566\n",
      " 75%|███████▌  | 751/1000 [35:11<16:16,  3.92s/it]INFO:root:global_step: 751, logpy: 135.864, kl: 18.462, loss: -117.610\n",
      " 75%|███████▌  | 752/1000 [35:15<15:45,  3.81s/it]INFO:root:global_step: 752, logpy: 135.953, kl: 18.477, loss: -117.681\n",
      " 75%|███████▌  | 753/1000 [35:18<15:31,  3.77s/it]INFO:root:global_step: 753, logpy: 136.019, kl: 18.492, loss: -117.729\n",
      " 75%|███████▌  | 754/1000 [35:22<15:02,  3.67s/it]INFO:root:global_step: 754, logpy: 136.139, kl: 18.509, loss: -117.830\n",
      " 76%|███████▌  | 755/1000 [35:25<14:34,  3.57s/it]INFO:root:global_step: 755, logpy: 136.217, kl: 18.526, loss: -117.890\n",
      " 76%|███████▌  | 756/1000 [35:28<14:05,  3.47s/it]INFO:root:global_step: 756, logpy: 136.328, kl: 18.540, loss: -117.984\n",
      " 76%|███████▌  | 757/1000 [35:32<14:04,  3.47s/it]INFO:root:global_step: 757, logpy: 136.409, kl: 18.560, loss: -118.044\n",
      " 76%|███████▌  | 758/1000 [35:35<14:07,  3.50s/it]INFO:root:global_step: 758, logpy: 136.522, kl: 18.579, loss: -118.135\n",
      " 76%|███████▌  | 759/1000 [35:39<13:54,  3.46s/it]INFO:root:global_step: 759, logpy: 136.596, kl: 18.599, loss: -118.188\n",
      " 76%|███████▌  | 760/1000 [35:42<13:41,  3.42s/it]INFO:root:global_step: 760, logpy: 136.711, kl: 18.611, loss: -118.289\n",
      " 76%|███████▌  | 761/1000 [35:46<13:52,  3.48s/it]INFO:root:global_step: 761, logpy: 136.784, kl: 18.628, loss: -118.343\n",
      " 76%|███████▌  | 762/1000 [35:49<14:01,  3.54s/it]INFO:root:global_step: 762, logpy: 136.893, kl: 18.646, loss: -118.432\n",
      " 76%|███████▋  | 763/1000 [35:53<13:39,  3.46s/it]INFO:root:global_step: 763, logpy: 137.008, kl: 18.663, loss: -118.528\n",
      " 76%|███████▋  | 764/1000 [35:56<13:20,  3.39s/it]INFO:root:global_step: 764, logpy: 137.085, kl: 18.680, loss: -118.586\n",
      " 76%|███████▋  | 765/1000 [35:59<13:11,  3.37s/it]INFO:root:global_step: 765, logpy: 137.124, kl: 18.703, loss: -118.601\n",
      " 77%|███████▋  | 766/1000 [36:02<13:04,  3.35s/it]INFO:root:global_step: 766, logpy: 137.192, kl: 18.716, loss: -118.654\n",
      " 77%|███████▋  | 767/1000 [36:06<12:54,  3.32s/it]INFO:root:global_step: 767, logpy: 137.279, kl: 18.729, loss: -118.726\n",
      " 77%|███████▋  | 768/1000 [36:09<12:57,  3.35s/it]INFO:root:global_step: 768, logpy: 137.361, kl: 18.747, loss: -118.788\n",
      " 77%|███████▋  | 769/1000 [36:12<12:47,  3.32s/it]INFO:root:global_step: 769, logpy: 137.432, kl: 18.764, loss: -118.841\n",
      " 77%|███████▋  | 770/1000 [36:16<12:37,  3.30s/it]INFO:root:global_step: 770, logpy: 137.545, kl: 18.777, loss: -118.939\n",
      " 77%|███████▋  | 771/1000 [36:19<12:29,  3.27s/it]INFO:root:global_step: 771, logpy: 137.602, kl: 18.793, loss: -118.979\n",
      " 77%|███████▋  | 772/1000 [36:22<12:23,  3.26s/it]INFO:root:global_step: 772, logpy: 137.700, kl: 18.807, loss: -119.060\n",
      " 77%|███████▋  | 773/1000 [36:25<12:22,  3.27s/it]INFO:root:global_step: 773, logpy: 137.796, kl: 18.823, loss: -119.139\n",
      " 77%|███████▋  | 774/1000 [36:29<12:15,  3.26s/it]INFO:root:global_step: 774, logpy: 137.865, kl: 18.839, loss: -119.190\n",
      " 78%|███████▊  | 775/1000 [36:32<12:15,  3.27s/it]INFO:root:global_step: 775, logpy: 137.962, kl: 18.855, loss: -119.269\n",
      " 78%|███████▊  | 776/1000 [36:35<12:20,  3.30s/it]INFO:root:global_step: 776, logpy: 138.068, kl: 18.869, loss: -119.360\n",
      " 78%|███████▊  | 777/1000 [36:39<12:37,  3.40s/it]INFO:root:global_step: 777, logpy: 138.125, kl: 18.885, loss: -119.399\n",
      " 78%|███████▊  | 778/1000 [36:42<12:42,  3.43s/it]INFO:root:global_step: 778, logpy: 138.168, kl: 18.901, loss: -119.424\n",
      " 78%|███████▊  | 779/1000 [36:46<12:32,  3.40s/it]INFO:root:global_step: 779, logpy: 138.261, kl: 18.919, loss: -119.498\n",
      " 78%|███████▊  | 780/1000 [36:49<12:26,  3.40s/it]INFO:root:global_step: 780, logpy: 138.364, kl: 18.935, loss: -119.583\n",
      " 78%|███████▊  | 781/1000 [36:52<12:11,  3.34s/it]INFO:root:global_step: 781, logpy: 138.441, kl: 18.956, loss: -119.638\n",
      " 78%|███████▊  | 782/1000 [36:55<11:56,  3.29s/it]INFO:root:global_step: 782, logpy: 138.513, kl: 18.967, loss: -119.697\n",
      " 78%|███████▊  | 783/1000 [36:59<11:50,  3.28s/it]INFO:root:global_step: 783, logpy: 138.622, kl: 18.983, loss: -119.790\n",
      " 78%|███████▊  | 784/1000 [37:02<11:43,  3.26s/it]INFO:root:global_step: 784, logpy: 138.734, kl: 18.996, loss: -119.887\n",
      " 78%|███████▊  | 785/1000 [37:05<11:38,  3.25s/it]INFO:root:global_step: 785, logpy: 138.851, kl: 19.010, loss: -119.987\n",
      " 79%|███████▊  | 786/1000 [37:08<11:35,  3.25s/it]INFO:root:global_step: 786, logpy: 138.945, kl: 19.024, loss: -120.066\n",
      " 79%|███████▊  | 787/1000 [37:12<11:34,  3.26s/it]INFO:root:global_step: 787, logpy: 139.030, kl: 19.044, loss: -120.130\n",
      " 79%|███████▉  | 788/1000 [37:15<11:28,  3.25s/it]INFO:root:global_step: 788, logpy: 139.122, kl: 19.062, loss: -120.203\n",
      " 79%|███████▉  | 789/1000 [37:18<11:22,  3.24s/it]INFO:root:global_step: 789, logpy: 139.218, kl: 19.078, loss: -120.281\n",
      " 79%|███████▉  | 790/1000 [37:21<11:20,  3.24s/it]INFO:root:global_step: 790, logpy: 139.293, kl: 19.098, loss: -120.335\n",
      " 79%|███████▉  | 791/1000 [37:25<11:13,  3.22s/it]INFO:root:global_step: 791, logpy: 139.357, kl: 19.117, loss: -120.378\n",
      " 79%|███████▉  | 792/1000 [37:28<11:10,  3.22s/it]INFO:root:global_step: 792, logpy: 139.422, kl: 19.132, loss: -120.427\n",
      " 79%|███████▉  | 793/1000 [37:31<11:14,  3.26s/it]INFO:root:global_step: 793, logpy: 139.495, kl: 19.142, loss: -120.488\n",
      " 79%|███████▉  | 794/1000 [37:34<11:11,  3.26s/it]INFO:root:global_step: 794, logpy: 139.566, kl: 19.159, loss: -120.541\n",
      " 80%|███████▉  | 795/1000 [37:38<11:06,  3.25s/it]INFO:root:global_step: 795, logpy: 139.669, kl: 19.175, loss: -120.627\n",
      " 80%|███████▉  | 796/1000 [37:41<11:02,  3.25s/it]INFO:root:global_step: 796, logpy: 139.743, kl: 19.193, loss: -120.681\n",
      " 80%|███████▉  | 797/1000 [37:44<11:02,  3.26s/it]INFO:root:global_step: 797, logpy: 139.864, kl: 19.212, loss: -120.782\n",
      " 80%|███████▉  | 798/1000 [37:48<11:06,  3.30s/it]INFO:root:global_step: 798, logpy: 139.968, kl: 19.225, loss: -120.872\n",
      " 80%|███████▉  | 799/1000 [37:51<11:00,  3.29s/it]INFO:root:global_step: 799, logpy: 140.014, kl: 19.242, loss: -120.899\n",
      " 80%|████████  | 800/1000 [37:54<10:56,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_800.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 800, logpy: 140.078, kl: 19.259, loss: -120.946\n",
      " 80%|████████  | 801/1000 [37:59<12:12,  3.68s/it]INFO:root:global_step: 801, logpy: 140.109, kl: 19.279, loss: -120.955\n",
      " 80%|████████  | 802/1000 [38:02<11:46,  3.57s/it]INFO:root:global_step: 802, logpy: 140.140, kl: 19.298, loss: -120.966\n",
      " 80%|████████  | 803/1000 [38:05<11:33,  3.52s/it]INFO:root:global_step: 803, logpy: 140.210, kl: 19.314, loss: -121.019\n",
      " 80%|████████  | 804/1000 [38:09<11:18,  3.46s/it]INFO:root:global_step: 804, logpy: 140.297, kl: 19.326, loss: -121.092\n",
      " 80%|████████  | 805/1000 [38:12<11:03,  3.40s/it]INFO:root:global_step: 805, logpy: 140.376, kl: 19.345, loss: -121.152\n",
      " 81%|████████  | 806/1000 [38:15<10:56,  3.38s/it]INFO:root:global_step: 806, logpy: 140.462, kl: 19.362, loss: -121.219\n",
      " 81%|████████  | 807/1000 [38:19<10:43,  3.33s/it]INFO:root:global_step: 807, logpy: 140.511, kl: 19.380, loss: -121.248\n",
      " 81%|████████  | 808/1000 [38:22<10:33,  3.30s/it]INFO:root:global_step: 808, logpy: 140.568, kl: 19.395, loss: -121.289\n",
      " 81%|████████  | 809/1000 [38:25<10:23,  3.27s/it]INFO:root:global_step: 809, logpy: 140.607, kl: 19.412, loss: -121.310\n",
      " 81%|████████  | 810/1000 [38:28<10:18,  3.25s/it]INFO:root:global_step: 810, logpy: 140.667, kl: 19.430, loss: -121.351\n",
      " 81%|████████  | 811/1000 [38:31<10:14,  3.25s/it]INFO:root:global_step: 811, logpy: 140.709, kl: 19.447, loss: -121.375\n",
      " 81%|████████  | 812/1000 [38:35<10:17,  3.29s/it]INFO:root:global_step: 812, logpy: 140.762, kl: 19.462, loss: -121.412\n",
      " 81%|████████▏ | 813/1000 [38:38<10:10,  3.26s/it]INFO:root:global_step: 813, logpy: 140.838, kl: 19.476, loss: -121.473\n",
      " 81%|████████▏ | 814/1000 [38:41<10:07,  3.27s/it]INFO:root:global_step: 814, logpy: 140.918, kl: 19.495, loss: -121.533\n",
      " 82%|████████▏ | 815/1000 [38:44<09:57,  3.23s/it]INFO:root:global_step: 815, logpy: 140.987, kl: 19.513, loss: -121.582\n",
      " 82%|████████▏ | 816/1000 [38:48<09:54,  3.23s/it]INFO:root:global_step: 816, logpy: 141.056, kl: 19.532, loss: -121.632\n",
      " 82%|████████▏ | 817/1000 [38:51<09:51,  3.23s/it]INFO:root:global_step: 817, logpy: 141.107, kl: 19.547, loss: -121.667\n",
      " 82%|████████▏ | 818/1000 [38:54<09:50,  3.25s/it]INFO:root:global_step: 818, logpy: 141.191, kl: 19.561, loss: -121.736\n",
      " 82%|████████▏ | 819/1000 [38:57<09:47,  3.24s/it]INFO:root:global_step: 819, logpy: 141.241, kl: 19.577, loss: -121.768\n",
      " 82%|████████▏ | 820/1000 [39:01<09:49,  3.27s/it]INFO:root:global_step: 820, logpy: 141.285, kl: 19.596, loss: -121.792\n",
      " 82%|████████▏ | 821/1000 [39:04<09:44,  3.26s/it]INFO:root:global_step: 821, logpy: 141.358, kl: 19.615, loss: -121.846\n",
      " 82%|████████▏ | 822/1000 [39:07<09:38,  3.25s/it]INFO:root:global_step: 822, logpy: 141.431, kl: 19.631, loss: -121.901\n",
      " 82%|████████▏ | 823/1000 [39:11<09:40,  3.28s/it]INFO:root:global_step: 823, logpy: 141.517, kl: 19.644, loss: -121.973\n",
      " 82%|████████▏ | 824/1000 [39:14<09:34,  3.27s/it]INFO:root:global_step: 824, logpy: 141.591, kl: 19.655, loss: -122.036\n",
      " 82%|████████▎ | 825/1000 [39:17<09:32,  3.27s/it]INFO:root:global_step: 825, logpy: 141.646, kl: 19.671, loss: -122.074\n",
      " 83%|████████▎ | 826/1000 [39:20<09:30,  3.28s/it]INFO:root:global_step: 826, logpy: 141.719, kl: 19.688, loss: -122.129\n",
      " 83%|████████▎ | 827/1000 [39:24<09:28,  3.29s/it]INFO:root:global_step: 827, logpy: 141.792, kl: 19.702, loss: -122.187\n",
      " 83%|████████▎ | 828/1000 [39:27<09:26,  3.29s/it]INFO:root:global_step: 828, logpy: 141.875, kl: 19.713, loss: -122.258\n",
      " 83%|████████▎ | 829/1000 [39:30<09:21,  3.29s/it]INFO:root:global_step: 829, logpy: 141.927, kl: 19.732, loss: -122.289\n",
      " 83%|████████▎ | 830/1000 [39:34<09:18,  3.29s/it]INFO:root:global_step: 830, logpy: 142.006, kl: 19.750, loss: -122.349\n",
      " 83%|████████▎ | 831/1000 [39:37<09:15,  3.29s/it]INFO:root:global_step: 831, logpy: 142.071, kl: 19.764, loss: -122.400\n",
      " 83%|████████▎ | 832/1000 [39:40<09:11,  3.28s/it]INFO:root:global_step: 832, logpy: 142.109, kl: 19.780, loss: -122.421\n",
      " 83%|████████▎ | 833/1000 [39:43<09:10,  3.30s/it]INFO:root:global_step: 833, logpy: 142.187, kl: 19.798, loss: -122.480\n",
      " 83%|████████▎ | 834/1000 [39:47<09:16,  3.35s/it]INFO:root:global_step: 834, logpy: 142.277, kl: 19.811, loss: -122.556\n",
      " 84%|████████▎ | 835/1000 [39:50<09:12,  3.35s/it]INFO:root:global_step: 835, logpy: 142.338, kl: 19.830, loss: -122.597\n",
      " 84%|████████▎ | 836/1000 [39:54<09:07,  3.34s/it]INFO:root:global_step: 836, logpy: 142.395, kl: 19.844, loss: -122.639\n",
      " 84%|████████▎ | 837/1000 [39:57<09:02,  3.33s/it]INFO:root:global_step: 837, logpy: 142.463, kl: 19.862, loss: -122.689\n",
      " 84%|████████▍ | 838/1000 [40:00<08:54,  3.30s/it]INFO:root:global_step: 838, logpy: 142.537, kl: 19.877, loss: -122.746\n",
      " 84%|████████▍ | 839/1000 [40:03<08:51,  3.30s/it]INFO:root:global_step: 839, logpy: 142.607, kl: 19.892, loss: -122.801\n",
      " 84%|████████▍ | 840/1000 [40:07<08:45,  3.28s/it]INFO:root:global_step: 840, logpy: 142.656, kl: 19.904, loss: -122.836\n",
      " 84%|████████▍ | 841/1000 [40:10<08:43,  3.29s/it]INFO:root:global_step: 841, logpy: 142.721, kl: 19.920, loss: -122.885\n",
      " 84%|████████▍ | 842/1000 [40:13<08:40,  3.29s/it]INFO:root:global_step: 842, logpy: 142.791, kl: 19.933, loss: -122.941\n",
      " 84%|████████▍ | 843/1000 [40:17<08:35,  3.28s/it]INFO:root:global_step: 843, logpy: 142.835, kl: 19.951, loss: -122.966\n",
      " 84%|████████▍ | 844/1000 [40:20<08:31,  3.28s/it]INFO:root:global_step: 844, logpy: 142.909, kl: 19.968, loss: -123.022\n",
      " 84%|████████▍ | 845/1000 [40:23<08:26,  3.27s/it]INFO:root:global_step: 845, logpy: 142.977, kl: 19.980, loss: -123.078\n",
      " 85%|████████▍ | 846/1000 [40:26<08:24,  3.28s/it]INFO:root:global_step: 846, logpy: 143.075, kl: 20.002, loss: -123.153\n",
      " 85%|████████▍ | 847/1000 [40:30<08:26,  3.31s/it]INFO:root:global_step: 847, logpy: 143.123, kl: 20.020, loss: -123.183\n",
      " 85%|████████▍ | 848/1000 [40:33<08:19,  3.29s/it]INFO:root:global_step: 848, logpy: 143.173, kl: 20.043, loss: -123.208\n",
      " 85%|████████▍ | 849/1000 [40:36<08:15,  3.28s/it]INFO:root:global_step: 849, logpy: 143.232, kl: 20.058, loss: -123.251\n",
      " 85%|████████▌ | 850/1000 [40:39<08:10,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_850.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 850, logpy: 143.310, kl: 20.073, loss: -123.313\n",
      " 85%|████████▌ | 851/1000 [40:44<09:01,  3.63s/it]INFO:root:global_step: 851, logpy: 143.381, kl: 20.086, loss: -123.370\n",
      " 85%|████████▌ | 852/1000 [40:47<08:48,  3.57s/it]INFO:root:global_step: 852, logpy: 143.439, kl: 20.105, loss: -123.409\n",
      " 85%|████████▌ | 853/1000 [40:51<08:33,  3.49s/it]INFO:root:global_step: 853, logpy: 143.512, kl: 20.122, loss: -123.464\n",
      " 85%|████████▌ | 854/1000 [40:54<08:18,  3.42s/it]INFO:root:global_step: 854, logpy: 143.573, kl: 20.136, loss: -123.510\n",
      " 86%|████████▌ | 855/1000 [40:57<08:12,  3.39s/it]INFO:root:global_step: 855, logpy: 143.629, kl: 20.147, loss: -123.554\n",
      " 86%|████████▌ | 856/1000 [41:01<08:10,  3.40s/it]INFO:root:global_step: 856, logpy: 143.709, kl: 20.159, loss: -123.622\n",
      " 86%|████████▌ | 857/1000 [41:04<07:58,  3.35s/it]INFO:root:global_step: 857, logpy: 143.798, kl: 20.173, loss: -123.696\n",
      " 86%|████████▌ | 858/1000 [41:07<07:50,  3.32s/it]INFO:root:global_step: 858, logpy: 143.870, kl: 20.186, loss: -123.755\n",
      " 86%|████████▌ | 859/1000 [41:11<07:55,  3.37s/it]INFO:root:global_step: 859, logpy: 143.916, kl: 20.202, loss: -123.783\n",
      " 86%|████████▌ | 860/1000 [41:14<07:50,  3.36s/it]INFO:root:global_step: 860, logpy: 143.993, kl: 20.221, loss: -123.841\n",
      " 86%|████████▌ | 861/1000 [41:17<07:42,  3.32s/it]INFO:root:global_step: 861, logpy: 144.035, kl: 20.240, loss: -123.863\n",
      " 86%|████████▌ | 862/1000 [41:21<07:40,  3.33s/it]INFO:root:global_step: 862, logpy: 144.113, kl: 20.252, loss: -123.929\n",
      " 86%|████████▋ | 863/1000 [41:24<07:40,  3.36s/it]INFO:root:global_step: 863, logpy: 144.152, kl: 20.270, loss: -123.950\n",
      " 86%|████████▋ | 864/1000 [41:27<07:33,  3.33s/it]INFO:root:global_step: 864, logpy: 144.208, kl: 20.285, loss: -123.990\n",
      " 86%|████████▋ | 865/1000 [41:31<07:27,  3.31s/it]INFO:root:global_step: 865, logpy: 144.279, kl: 20.297, loss: -124.048\n",
      " 87%|████████▋ | 866/1000 [41:34<07:21,  3.30s/it]INFO:root:global_step: 866, logpy: 144.323, kl: 20.312, loss: -124.076\n",
      " 87%|████████▋ | 867/1000 [41:37<07:19,  3.30s/it]INFO:root:global_step: 867, logpy: 144.381, kl: 20.329, loss: -124.117\n",
      " 87%|████████▋ | 868/1000 [41:40<07:17,  3.31s/it]INFO:root:global_step: 868, logpy: 144.416, kl: 20.344, loss: -124.135\n",
      " 87%|████████▋ | 869/1000 [41:44<07:18,  3.35s/it]INFO:root:global_step: 869, logpy: 144.490, kl: 20.358, loss: -124.195\n",
      " 87%|████████▋ | 870/1000 [41:47<07:19,  3.38s/it]INFO:root:global_step: 870, logpy: 144.537, kl: 20.371, loss: -124.229\n",
      " 87%|████████▋ | 871/1000 [41:51<07:21,  3.42s/it]INFO:root:global_step: 871, logpy: 144.590, kl: 20.386, loss: -124.266\n",
      " 87%|████████▋ | 872/1000 [41:54<07:17,  3.42s/it]INFO:root:global_step: 872, logpy: 144.655, kl: 20.400, loss: -124.316\n",
      " 87%|████████▋ | 873/1000 [41:58<07:16,  3.44s/it]INFO:root:global_step: 873, logpy: 144.712, kl: 20.417, loss: -124.356\n",
      " 87%|████████▋ | 874/1000 [42:01<07:14,  3.45s/it]INFO:root:global_step: 874, logpy: 144.751, kl: 20.432, loss: -124.379\n",
      " 88%|████████▊ | 875/1000 [42:04<07:02,  3.38s/it]INFO:root:global_step: 875, logpy: 144.828, kl: 20.450, loss: -124.437\n",
      " 88%|████████▊ | 876/1000 [42:08<06:54,  3.35s/it]INFO:root:global_step: 876, logpy: 144.873, kl: 20.466, loss: -124.465\n",
      " 88%|████████▊ | 877/1000 [42:11<06:52,  3.36s/it]INFO:root:global_step: 877, logpy: 144.950, kl: 20.481, loss: -124.527\n",
      " 88%|████████▊ | 878/1000 [42:14<06:47,  3.34s/it]INFO:root:global_step: 878, logpy: 144.990, kl: 20.500, loss: -124.548\n",
      " 88%|████████▊ | 879/1000 [42:18<06:41,  3.32s/it]INFO:root:global_step: 879, logpy: 145.068, kl: 20.513, loss: -124.612\n",
      " 88%|████████▊ | 880/1000 [42:21<06:36,  3.30s/it]INFO:root:global_step: 880, logpy: 145.147, kl: 20.530, loss: -124.673\n",
      " 88%|████████▊ | 881/1000 [42:24<06:32,  3.29s/it]INFO:root:global_step: 881, logpy: 145.223, kl: 20.546, loss: -124.733\n",
      " 88%|████████▊ | 882/1000 [42:28<06:30,  3.31s/it]INFO:root:global_step: 882, logpy: 145.303, kl: 20.565, loss: -124.793\n",
      " 88%|████████▊ | 883/1000 [42:31<06:25,  3.30s/it]INFO:root:global_step: 883, logpy: 145.340, kl: 20.579, loss: -124.816\n",
      " 88%|████████▊ | 884/1000 [42:34<06:21,  3.29s/it]INFO:root:global_step: 884, logpy: 145.419, kl: 20.590, loss: -124.884\n",
      " 88%|████████▊ | 885/1000 [42:37<06:18,  3.29s/it]INFO:root:global_step: 885, logpy: 145.473, kl: 20.605, loss: -124.922\n",
      " 89%|████████▊ | 886/1000 [42:41<06:14,  3.28s/it]INFO:root:global_step: 886, logpy: 145.546, kl: 20.614, loss: -124.985\n",
      " 89%|████████▊ | 887/1000 [42:44<06:15,  3.32s/it]INFO:root:global_step: 887, logpy: 145.610, kl: 20.629, loss: -125.033\n",
      " 89%|████████▉ | 888/1000 [42:47<06:10,  3.31s/it]INFO:root:global_step: 888, logpy: 145.680, kl: 20.646, loss: -125.086\n",
      " 89%|████████▉ | 889/1000 [42:51<06:05,  3.29s/it]INFO:root:global_step: 889, logpy: 145.715, kl: 20.664, loss: -125.102\n",
      " 89%|████████▉ | 890/1000 [42:54<06:01,  3.29s/it]INFO:root:global_step: 890, logpy: 145.786, kl: 20.677, loss: -125.160\n",
      " 89%|████████▉ | 891/1000 [42:57<05:59,  3.29s/it]INFO:root:global_step: 891, logpy: 145.866, kl: 20.696, loss: -125.221\n",
      " 89%|████████▉ | 892/1000 [43:00<05:53,  3.27s/it]INFO:root:global_step: 892, logpy: 145.957, kl: 20.712, loss: -125.294\n",
      " 89%|████████▉ | 893/1000 [43:04<05:52,  3.30s/it]INFO:root:global_step: 893, logpy: 146.007, kl: 20.724, loss: -125.332\n",
      " 89%|████████▉ | 894/1000 [43:07<05:54,  3.35s/it]INFO:root:global_step: 894, logpy: 146.077, kl: 20.739, loss: -125.388\n",
      " 90%|████████▉ | 895/1000 [43:11<06:04,  3.47s/it]INFO:root:global_step: 895, logpy: 146.124, kl: 20.754, loss: -125.419\n",
      " 90%|████████▉ | 896/1000 [43:14<05:59,  3.46s/it]INFO:root:global_step: 896, logpy: 146.181, kl: 20.768, loss: -125.461\n",
      " 90%|████████▉ | 897/1000 [43:18<05:54,  3.44s/it]INFO:root:global_step: 897, logpy: 146.225, kl: 20.788, loss: -125.485\n",
      " 90%|████████▉ | 898/1000 [43:21<05:46,  3.39s/it]INFO:root:global_step: 898, logpy: 146.290, kl: 20.801, loss: -125.536\n",
      " 90%|████████▉ | 899/1000 [43:24<05:40,  3.37s/it]INFO:root:global_step: 899, logpy: 146.307, kl: 20.820, loss: -125.533\n",
      " 90%|█████████ | 900/1000 [43:28<05:34,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_900.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 900, logpy: 146.348, kl: 20.834, loss: -125.561\n",
      " 90%|█████████ | 901/1000 [43:33<06:18,  3.82s/it]INFO:root:global_step: 901, logpy: 146.375, kl: 20.854, loss: -125.567\n",
      " 90%|█████████ | 902/1000 [43:36<06:05,  3.73s/it]INFO:root:global_step: 902, logpy: 146.418, kl: 20.867, loss: -125.596\n",
      " 90%|█████████ | 903/1000 [43:40<05:53,  3.65s/it]INFO:root:global_step: 903, logpy: 146.467, kl: 20.886, loss: -125.625\n",
      " 90%|█████████ | 904/1000 [43:43<05:45,  3.60s/it]INFO:root:global_step: 904, logpy: 146.539, kl: 20.904, loss: -125.680\n",
      " 90%|█████████ | 905/1000 [43:47<05:37,  3.55s/it]INFO:root:global_step: 905, logpy: 146.605, kl: 20.917, loss: -125.733\n",
      " 91%|█████████ | 906/1000 [43:50<05:26,  3.47s/it]INFO:root:global_step: 906, logpy: 146.645, kl: 20.930, loss: -125.759\n",
      " 91%|█████████ | 907/1000 [43:53<05:17,  3.41s/it]INFO:root:global_step: 907, logpy: 146.717, kl: 20.945, loss: -125.815\n",
      " 91%|█████████ | 908/1000 [43:56<05:14,  3.41s/it]INFO:root:global_step: 908, logpy: 146.792, kl: 20.962, loss: -125.872\n",
      " 91%|█████████ | 909/1000 [44:00<05:07,  3.38s/it]INFO:root:global_step: 909, logpy: 146.840, kl: 20.978, loss: -125.904\n",
      " 91%|█████████ | 910/1000 [44:03<05:02,  3.36s/it]INFO:root:global_step: 910, logpy: 146.879, kl: 20.998, loss: -125.922\n",
      " 91%|█████████ | 911/1000 [44:06<04:57,  3.34s/it]INFO:root:global_step: 911, logpy: 146.922, kl: 21.008, loss: -125.955\n",
      " 91%|█████████ | 912/1000 [44:10<04:58,  3.39s/it]INFO:root:global_step: 912, logpy: 146.947, kl: 21.026, loss: -125.962\n",
      " 91%|█████████▏| 913/1000 [44:13<04:52,  3.36s/it]INFO:root:global_step: 913, logpy: 146.995, kl: 21.046, loss: -125.990\n",
      " 91%|█████████▏| 914/1000 [44:17<04:51,  3.39s/it]INFO:root:global_step: 914, logpy: 147.056, kl: 21.061, loss: -126.035\n",
      " 92%|█████████▏| 915/1000 [44:20<04:49,  3.41s/it]INFO:root:global_step: 915, logpy: 147.070, kl: 21.083, loss: -126.027\n",
      " 92%|█████████▏| 916/1000 [44:24<04:47,  3.42s/it]INFO:root:global_step: 916, logpy: 147.131, kl: 21.098, loss: -126.073\n",
      " 92%|█████████▏| 917/1000 [44:27<04:42,  3.40s/it]INFO:root:global_step: 917, logpy: 147.220, kl: 21.111, loss: -126.148\n",
      " 92%|█████████▏| 918/1000 [44:30<04:37,  3.39s/it]INFO:root:global_step: 918, logpy: 147.263, kl: 21.124, loss: -126.178\n",
      " 92%|█████████▏| 919/1000 [44:34<04:33,  3.38s/it]INFO:root:global_step: 919, logpy: 147.328, kl: 21.137, loss: -126.229\n",
      " 92%|█████████▏| 920/1000 [44:37<04:29,  3.37s/it]INFO:root:global_step: 920, logpy: 147.426, kl: 21.153, loss: -126.311\n",
      " 92%|█████████▏| 921/1000 [44:40<04:26,  3.37s/it]INFO:root:global_step: 921, logpy: 147.487, kl: 21.170, loss: -126.355\n",
      " 92%|█████████▏| 922/1000 [44:44<04:21,  3.36s/it]INFO:root:global_step: 922, logpy: 147.528, kl: 21.184, loss: -126.381\n",
      " 92%|█████████▏| 923/1000 [44:47<04:18,  3.36s/it]INFO:root:global_step: 923, logpy: 147.566, kl: 21.196, loss: -126.407\n",
      " 92%|█████████▏| 924/1000 [44:50<04:14,  3.35s/it]INFO:root:global_step: 924, logpy: 147.594, kl: 21.212, loss: -126.419\n",
      " 92%|█████████▎| 925/1000 [44:54<04:13,  3.38s/it]INFO:root:global_step: 925, logpy: 147.657, kl: 21.227, loss: -126.465\n",
      " 93%|█████████▎| 926/1000 [44:57<04:09,  3.37s/it]INFO:root:global_step: 926, logpy: 147.707, kl: 21.242, loss: -126.501\n",
      " 93%|█████████▎| 927/1000 [45:01<04:05,  3.36s/it]INFO:root:global_step: 927, logpy: 147.775, kl: 21.255, loss: -126.555\n",
      " 93%|█████████▎| 928/1000 [45:04<04:03,  3.38s/it]INFO:root:global_step: 928, logpy: 147.854, kl: 21.269, loss: -126.620\n",
      " 93%|█████████▎| 929/1000 [45:07<03:59,  3.38s/it]INFO:root:global_step: 929, logpy: 147.912, kl: 21.285, loss: -126.661\n",
      " 93%|█████████▎| 930/1000 [45:11<03:57,  3.39s/it]INFO:root:global_step: 930, logpy: 147.979, kl: 21.302, loss: -126.711\n",
      " 93%|█████████▎| 931/1000 [45:14<03:54,  3.40s/it]INFO:root:global_step: 931, logpy: 148.037, kl: 21.317, loss: -126.754\n",
      " 93%|█████████▎| 932/1000 [45:18<03:50,  3.39s/it]INFO:root:global_step: 932, logpy: 148.082, kl: 21.330, loss: -126.786\n",
      " 93%|█████████▎| 933/1000 [45:21<03:45,  3.37s/it]INFO:root:global_step: 933, logpy: 148.117, kl: 21.344, loss: -126.807\n",
      " 93%|█████████▎| 934/1000 [45:24<03:42,  3.37s/it]INFO:root:global_step: 934, logpy: 148.183, kl: 21.359, loss: -126.857\n",
      " 94%|█████████▎| 935/1000 [45:28<03:39,  3.38s/it]INFO:root:global_step: 935, logpy: 148.203, kl: 21.379, loss: -126.856\n",
      " 94%|█████████▎| 936/1000 [45:31<03:34,  3.36s/it]INFO:root:global_step: 936, logpy: 148.223, kl: 21.394, loss: -126.861\n",
      " 94%|█████████▎| 937/1000 [45:34<03:31,  3.35s/it]INFO:root:global_step: 937, logpy: 148.257, kl: 21.413, loss: -126.875\n",
      " 94%|█████████▍| 938/1000 [45:38<03:27,  3.35s/it]INFO:root:global_step: 938, logpy: 148.281, kl: 21.430, loss: -126.883\n",
      " 94%|█████████▍| 939/1000 [45:41<03:27,  3.39s/it]INFO:root:global_step: 939, logpy: 148.328, kl: 21.452, loss: -126.907\n",
      " 94%|█████████▍| 940/1000 [45:45<03:24,  3.41s/it]INFO:root:global_step: 940, logpy: 148.390, kl: 21.462, loss: -126.959\n",
      " 94%|█████████▍| 941/1000 [45:48<03:22,  3.43s/it]INFO:root:global_step: 941, logpy: 148.426, kl: 21.474, loss: -126.982\n",
      " 94%|█████████▍| 942/1000 [45:51<03:17,  3.40s/it]INFO:root:global_step: 942, logpy: 148.464, kl: 21.487, loss: -127.008\n",
      " 94%|█████████▍| 943/1000 [45:55<03:12,  3.37s/it]INFO:root:global_step: 943, logpy: 148.486, kl: 21.499, loss: -127.017\n",
      " 94%|█████████▍| 944/1000 [45:58<03:08,  3.37s/it]INFO:root:global_step: 944, logpy: 148.522, kl: 21.512, loss: -127.040\n",
      " 94%|█████████▍| 945/1000 [46:01<03:06,  3.38s/it]INFO:root:global_step: 945, logpy: 148.563, kl: 21.528, loss: -127.065\n",
      " 95%|█████████▍| 946/1000 [46:05<03:01,  3.37s/it]INFO:root:global_step: 946, logpy: 148.621, kl: 21.545, loss: -127.105\n",
      " 95%|█████████▍| 947/1000 [46:08<02:58,  3.36s/it]INFO:root:global_step: 947, logpy: 148.694, kl: 21.556, loss: -127.168\n",
      " 95%|█████████▍| 948/1000 [46:12<02:55,  3.37s/it]INFO:root:global_step: 948, logpy: 148.737, kl: 21.574, loss: -127.192\n",
      " 95%|█████████▍| 949/1000 [46:15<02:51,  3.36s/it]INFO:root:global_step: 949, logpy: 148.788, kl: 21.588, loss: -127.229\n",
      " 95%|█████████▌| 950/1000 [46:18<02:47,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n",
      "torch.Size([121, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved figure at: ./img_generation/global_step_950.png\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torchsde/_core/sdeint.py:278: UserWarning: Numerical solution is not guaranteed to converge to the correct solution when using adaptive time-stepping with the Euler--Maruyama method with non-additive noise.\n",
      "  warnings.warn(f\"Numerical solution is not guaranteed to converge to the correct solution when using adaptive \"\n",
      "INFO:root:global_step: 950, logpy: 148.844, kl: 21.606, loss: -127.265\n",
      " 95%|█████████▌| 951/1000 [46:23<03:03,  3.74s/it]INFO:root:global_step: 951, logpy: 148.900, kl: 21.619, loss: -127.309\n",
      " 95%|█████████▌| 952/1000 [46:27<02:58,  3.73s/it]INFO:root:global_step: 952, logpy: 148.957, kl: 21.636, loss: -127.348\n",
      " 95%|█████████▌| 953/1000 [46:30<02:50,  3.63s/it]INFO:root:global_step: 953, logpy: 149.010, kl: 21.647, loss: -127.390\n",
      " 95%|█████████▌| 954/1000 [46:33<02:43,  3.56s/it]INFO:root:global_step: 954, logpy: 149.085, kl: 21.660, loss: -127.453\n",
      " 96%|█████████▌| 955/1000 [46:37<02:37,  3.49s/it]INFO:root:global_step: 955, logpy: 149.124, kl: 21.676, loss: -127.475\n",
      " 96%|█████████▌| 956/1000 [46:40<02:31,  3.44s/it]INFO:root:global_step: 956, logpy: 149.171, kl: 21.690, loss: -127.507\n",
      " 96%|█████████▌| 957/1000 [46:43<02:26,  3.41s/it]INFO:root:global_step: 957, logpy: 149.208, kl: 21.701, loss: -127.534\n",
      " 96%|█████████▌| 958/1000 [46:47<02:22,  3.40s/it]INFO:root:global_step: 958, logpy: 149.265, kl: 21.714, loss: -127.576\n",
      " 96%|█████████▌| 959/1000 [46:50<02:18,  3.39s/it]INFO:root:global_step: 959, logpy: 149.318, kl: 21.729, loss: -127.614\n",
      " 96%|█████████▌| 960/1000 [46:53<02:14,  3.37s/it]INFO:root:global_step: 960, logpy: 149.378, kl: 21.744, loss: -127.659\n",
      " 96%|█████████▌| 961/1000 [46:57<02:11,  3.36s/it]INFO:root:global_step: 961, logpy: 149.424, kl: 21.756, loss: -127.692\n",
      " 96%|█████████▌| 962/1000 [47:00<02:07,  3.36s/it]INFO:root:global_step: 962, logpy: 149.451, kl: 21.767, loss: -127.709\n",
      " 96%|█████████▋| 963/1000 [47:03<02:04,  3.36s/it]INFO:root:global_step: 963, logpy: 149.516, kl: 21.782, loss: -127.759\n",
      " 96%|█████████▋| 964/1000 [47:07<02:00,  3.36s/it]INFO:root:global_step: 964, logpy: 149.555, kl: 21.798, loss: -127.781\n",
      " 96%|█████████▋| 965/1000 [47:10<01:59,  3.41s/it]INFO:root:global_step: 965, logpy: 149.613, kl: 21.811, loss: -127.826\n",
      " 97%|█████████▋| 966/1000 [47:14<01:55,  3.40s/it]INFO:root:global_step: 966, logpy: 149.664, kl: 21.826, loss: -127.862\n",
      " 97%|█████████▋| 967/1000 [47:17<01:52,  3.41s/it]INFO:root:global_step: 967, logpy: 149.732, kl: 21.837, loss: -127.918\n",
      " 97%|█████████▋| 968/1000 [47:21<01:48,  3.40s/it]INFO:root:global_step: 968, logpy: 149.776, kl: 21.852, loss: -127.947\n",
      " 97%|█████████▋| 969/1000 [47:24<01:44,  3.38s/it]INFO:root:global_step: 969, logpy: 149.811, kl: 21.868, loss: -127.966\n",
      " 97%|█████████▋| 970/1000 [47:27<01:42,  3.41s/it]INFO:root:global_step: 970, logpy: 149.867, kl: 21.886, loss: -128.004\n",
      " 97%|█████████▋| 971/1000 [47:31<01:38,  3.38s/it]INFO:root:global_step: 971, logpy: 149.915, kl: 21.900, loss: -128.037\n",
      " 97%|█████████▋| 972/1000 [47:34<01:34,  3.38s/it]INFO:root:global_step: 972, logpy: 149.965, kl: 21.910, loss: -128.078\n",
      " 97%|█████████▋| 973/1000 [47:37<01:31,  3.38s/it]INFO:root:global_step: 973, logpy: 149.998, kl: 21.924, loss: -128.096\n",
      " 97%|█████████▋| 974/1000 [47:41<01:28,  3.40s/it]INFO:root:global_step: 974, logpy: 150.025, kl: 21.939, loss: -128.107\n",
      " 98%|█████████▊| 975/1000 [47:44<01:26,  3.45s/it]INFO:root:global_step: 975, logpy: 150.090, kl: 21.957, loss: -128.155\n",
      " 98%|█████████▊| 976/1000 [47:48<01:22,  3.43s/it]INFO:root:global_step: 976, logpy: 150.116, kl: 21.973, loss: -128.165\n",
      " 98%|█████████▊| 977/1000 [47:51<01:18,  3.41s/it]INFO:root:global_step: 977, logpy: 150.164, kl: 21.981, loss: -128.204\n",
      " 98%|█████████▊| 978/1000 [47:55<01:15,  3.43s/it]INFO:root:global_step: 978, logpy: 150.230, kl: 21.993, loss: -128.258\n",
      " 98%|█████████▊| 979/1000 [47:58<01:11,  3.41s/it]INFO:root:global_step: 979, logpy: 150.253, kl: 22.006, loss: -128.268\n",
      " 98%|█████████▊| 980/1000 [48:01<01:08,  3.40s/it]INFO:root:global_step: 980, logpy: 150.318, kl: 22.022, loss: -128.317\n",
      " 98%|█████████▊| 981/1000 [48:05<01:04,  3.39s/it]INFO:root:global_step: 981, logpy: 150.361, kl: 22.034, loss: -128.347\n",
      " 98%|█████████▊| 982/1000 [48:08<01:01,  3.39s/it]INFO:root:global_step: 982, logpy: 150.396, kl: 22.045, loss: -128.371\n",
      " 98%|█████████▊| 983/1000 [48:12<00:57,  3.40s/it]INFO:root:global_step: 983, logpy: 150.419, kl: 22.057, loss: -128.382\n",
      " 98%|█████████▊| 984/1000 [48:15<00:54,  3.38s/it]INFO:root:global_step: 984, logpy: 150.481, kl: 22.071, loss: -128.430\n",
      " 98%|█████████▊| 985/1000 [48:18<00:50,  3.37s/it]INFO:root:global_step: 985, logpy: 150.550, kl: 22.082, loss: -128.487\n",
      " 99%|█████████▊| 986/1000 [48:22<00:46,  3.35s/it]INFO:root:global_step: 986, logpy: 150.574, kl: 22.102, loss: -128.492\n",
      " 99%|█████████▊| 987/1000 [48:25<00:43,  3.34s/it]INFO:root:global_step: 987, logpy: 150.605, kl: 22.110, loss: -128.514\n",
      " 99%|█████████▉| 988/1000 [48:28<00:40,  3.35s/it]INFO:root:global_step: 988, logpy: 150.639, kl: 22.124, loss: -128.534\n",
      " 99%|█████████▉| 989/1000 [48:32<00:36,  3.36s/it]INFO:root:global_step: 989, logpy: 150.685, kl: 22.134, loss: -128.570\n",
      " 99%|█████████▉| 990/1000 [48:35<00:33,  3.35s/it]INFO:root:global_step: 990, logpy: 150.745, kl: 22.145, loss: -128.619\n",
      " 99%|█████████▉| 991/1000 [48:38<00:30,  3.35s/it]INFO:root:global_step: 991, logpy: 150.787, kl: 22.159, loss: -128.646\n",
      " 99%|█████████▉| 992/1000 [48:42<00:26,  3.36s/it]INFO:root:global_step: 992, logpy: 150.829, kl: 22.172, loss: -128.675\n",
      " 99%|█████████▉| 993/1000 [48:45<00:23,  3.40s/it]INFO:root:global_step: 993, logpy: 150.889, kl: 22.185, loss: -128.722\n",
      " 99%|█████████▉| 994/1000 [48:49<00:20,  3.39s/it]INFO:root:global_step: 994, logpy: 150.939, kl: 22.200, loss: -128.757\n",
      "100%|█████████▉| 995/1000 [48:52<00:16,  3.39s/it]INFO:root:global_step: 995, logpy: 151.012, kl: 22.216, loss: -128.814\n",
      "100%|█████████▉| 996/1000 [48:55<00:13,  3.40s/it]INFO:root:global_step: 996, logpy: 151.066, kl: 22.230, loss: -128.854\n",
      "100%|█████████▉| 997/1000 [48:59<00:10,  3.41s/it]INFO:root:global_step: 997, logpy: 151.105, kl: 22.243, loss: -128.879\n",
      "100%|█████████▉| 998/1000 [49:02<00:06,  3.39s/it]INFO:root:global_step: 998, logpy: 151.119, kl: 22.260, loss: -128.876\n",
      "100%|█████████▉| 999/1000 [49:06<00:03,  3.40s/it]INFO:root:global_step: 999, logpy: 151.145, kl: 22.275, loss: -128.887\n",
      "100%|██████████| 1000/1000 [49:09<00:00,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "manual_seed(args['seed'])\n",
    "\n",
    "if args['debug']:\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "ckpt_dir = os.path.join('./sim/', 'ckpts')\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "sdeint_fn = torchsde.sdeint_adjoint if args['adjoint'] else torchsde.sdeint\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356ad31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
