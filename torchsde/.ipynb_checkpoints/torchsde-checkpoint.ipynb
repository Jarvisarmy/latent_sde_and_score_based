{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e40d71",
   "metadata": {},
   "source": [
    "### torchsde/_core/adjoint.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52835dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import warnings\n",
    "\n",
    "from . import base_sde\n",
    "from . import methods\n",
    "from . import misc\n",
    "from . import sdeint\n",
    "from .adjoint_sde import AdjointSDE\n",
    "from .._brownian import BaseBrownian, ReverseBrownian\n",
    "from ..settings import METHODS, NOISE_TYPES, SDE_TYPES\n",
    "from ..types import Any, Dict, Optional, Scalar, Tensor, Tensors, TensorOrTensors, Vector\n",
    "\n",
    "class _SdeintAdjointMethod(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, sde, ts, dt, bm, solver, method, adjoint_method, adjoint_adaptive, adjoint_rtol, adjoint_atol,\n",
    "                dt_min, adjoint_options, len_extras, y0, *extras_and_adjoint_params):\n",
    "        ctx.sde = sde\n",
    "        ctx.dt = dt\n",
    "        ctx.bm = bm\n",
    "        ctx.adjoint_method = adjoint_method\n",
    "        ctx.adjoint_adaptive = adjoint_adaptive\n",
    "        ctx.adjoint_rtol = adjoint_rtol\n",
    "        ctx.adjoint_atol = adjoint_atol\n",
    "        ctx.dt_min = dt_min\n",
    "        ctx.adjoint_options = adjoint_options\n",
    "        ctx.len_extras = len_extras\n",
    "\n",
    "        extra_solver_state = extras_and_adjoint_params[:len_extras]\n",
    "        adjoint_params = extras_and_adjoint_params[len_extras:]\n",
    "\n",
    "        # This .detach() is VERY IMPORTANT. See adjoint_sde.py::AdjointSDE.get_state.\n",
    "        y0 = y0.detach()\n",
    "        # Necessary for the same reason\n",
    "        extra_solver_state = tuple(x.detach() for x in extra_solver_state)\n",
    "        ys, extra_solver_state = solver.integrate(y0, ts, extra_solver_state)\n",
    "\n",
    "        if method == METHODS.reversible_heun and adjoint_method == METHODS.adjoint_reversible_heun:\n",
    "            ctx.saved_extras_for_backward = True\n",
    "            extras_for_backward = extra_solver_state\n",
    "        else:\n",
    "            # Else just remove the `extra_solver_state` information.\n",
    "            ctx.saved_extras_for_backward = False\n",
    "            extras_for_backward = ()\n",
    "        ctx.save_for_backward(ys, ts, *extras_for_backward, *adjoint_params)\n",
    "        return (ys, *extra_solver_state)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_ys, *grad_extra_solver_state):  # noqa\n",
    "        ys, ts, *extras_and_adjoint_params = ctx.saved_tensors\n",
    "        if ctx.saved_extras_for_backward:\n",
    "            extra_solver_state = extras_and_adjoint_params[:ctx.len_extras]\n",
    "            adjoint_params = extras_and_adjoint_params[ctx.len_extras:]\n",
    "        else:\n",
    "            grad_extra_solver_state = ()\n",
    "            extra_solver_state = None\n",
    "            adjoint_params = extras_and_adjoint_params\n",
    "\n",
    "        aug_state = [ys[-1], grad_ys[-1]] + list(grad_extra_solver_state) + [torch.zeros_like(param)\n",
    "                                                                             for param in adjoint_params]\n",
    "        shapes = [t.size() for t in aug_state]\n",
    "        aug_state = misc.flatten(aug_state)\n",
    "        aug_state = aug_state.unsqueeze(0)  # dummy batch dimension\n",
    "        adjoint_sde = AdjointSDE(ctx.sde, adjoint_params, shapes)\n",
    "        reverse_bm = ReverseBrownian(ctx.bm)\n",
    "\n",
    "        solver_fn = methods.select(method=ctx.adjoint_method, sde_type=adjoint_sde.sde_type)\n",
    "        solver = solver_fn(\n",
    "            sde=adjoint_sde,\n",
    "            bm=reverse_bm,\n",
    "            dt=ctx.dt,\n",
    "            adaptive=ctx.adjoint_adaptive,\n",
    "            rtol=ctx.adjoint_rtol,\n",
    "            atol=ctx.adjoint_atol,\n",
    "            dt_min=ctx.dt_min,\n",
    "            options=ctx.adjoint_options\n",
    "        )\n",
    "        if extra_solver_state is None:\n",
    "            extra_solver_state = solver.init_extra_solver_state(ts[-1], aug_state)\n",
    "\n",
    "        for i in range(ys.size(0) - 1, 0, -1):\n",
    "            (_, aug_state), *extra_solver_state = _SdeintAdjointMethod.apply(adjoint_sde,\n",
    "                                                                             torch.stack([-ts[i], -ts[i - 1]]),\n",
    "                                                                             ctx.dt,\n",
    "                                                                             reverse_bm,\n",
    "                                                                             solver,\n",
    "                                                                             ctx.adjoint_method,\n",
    "                                                                             ctx.adjoint_method,\n",
    "                                                                             ctx.adjoint_adaptive,\n",
    "                                                                             ctx.adjoint_rtol,\n",
    "                                                                             ctx.adjoint_atol,\n",
    "                                                                             ctx.dt_min,\n",
    "                                                                             ctx.adjoint_options,\n",
    "                                                                             len(extra_solver_state),\n",
    "                                                                             aug_state,\n",
    "                                                                             *extra_solver_state,\n",
    "                                                                             *adjoint_params)\n",
    "            aug_state = misc.flat_to_shape(aug_state.squeeze(0), shapes)\n",
    "            aug_state[0] = ys[i - 1]\n",
    "            aug_state[1] = aug_state[1] + grad_ys[i - 1]\n",
    "            if i != 1:\n",
    "                aug_state = misc.flatten(aug_state)\n",
    "                aug_state = aug_state.unsqueeze(0)  # dummy batch dimension\n",
    "\n",
    "        if ctx.saved_extras_for_backward:\n",
    "            out = aug_state[1:]\n",
    "        else:\n",
    "            out = [aug_state[1]] + ([None] * ctx.len_extras) + aug_state[2:]\n",
    "        return (\n",
    "            None, None, None, None, None, None, None, None, None, None, None, None, None, *out,\n",
    "        )\n",
    "\n",
    "\n",
    "def sdeint_adjoint(sde: nn.Module,\n",
    "                   y0: Tensor,\n",
    "                   ts: Vector,\n",
    "                   bm: Optional[BaseBrownian] = None,\n",
    "                   method: Optional[str] = None,\n",
    "                   adjoint_method: Optional[str] = None,\n",
    "                   dt: Scalar = 1e-3,\n",
    "                   adaptive: bool = False,\n",
    "                   adjoint_adaptive: bool = False,\n",
    "                   rtol: Scalar = 1e-5,\n",
    "                   adjoint_rtol: Scalar = 1e-5,\n",
    "                   atol: Scalar = 1e-4,\n",
    "                   adjoint_atol: Scalar = 1e-4,\n",
    "                   dt_min: Scalar = 1e-5,\n",
    "                   options: Optional[Dict[str, Any]] = None,\n",
    "                   adjoint_options: Optional[Dict[str, Any]] = None,\n",
    "                   adjoint_params=None,\n",
    "                   names: Optional[Dict[str, str]] = None,\n",
    "                   logqp: bool = False,\n",
    "                   extra: bool = False,\n",
    "                   extra_solver_state: Optional[Tensors] = None,\n",
    "                   **unused_kwargs) -> TensorOrTensors:\n",
    "    \"\"\"Numerically integrate an SDE with stochastic adjoint support.\n",
    "    Args:\n",
    "        sde (torch.nn.Module): Object with methods `f` and `g` representing the\n",
    "            drift and diffusion. The output of `g` should be a single tensor of\n",
    "            size (batch_size, d) for diagonal noise SDEs or (batch_size, d, m)\n",
    "            for SDEs of other noise types; d is the dimensionality of state and\n",
    "            m is the dimensionality of Brownian motion.\n",
    "        y0 (Tensor): A tensor for the initial state.\n",
    "        ts (Tensor or sequence of float): Query times in non-descending order.\n",
    "            The state at the first time of `ts` should be `y0`.\n",
    "        bm (Brownian, optional): A 'BrownianInterval', `BrownianPath` or\n",
    "            `BrownianTree` object. Should return tensors of size (batch_size, m)\n",
    "            for `__call__`. Defaults to `BrownianInterval`.\n",
    "        method (str, optional): Numerical integration method to use. Must be\n",
    "            compatible with the SDE type (Ito/Stratonovich) and the noise type\n",
    "            (scalar/additive/diagonal/general). Defaults to a sensible choice\n",
    "            depending on the SDE type and noise type of the supplied SDE.\n",
    "        adjoint_method (str, optional): Name of numerical integration method for\n",
    "            backward adjoint solve. Defaults to a sensible choice depending on\n",
    "            the SDE type and noise type of the supplied SDE.\n",
    "        dt (float, optional): The constant step size or initial step size for\n",
    "            adaptive time-stepping.\n",
    "        adaptive (bool, optional): If `True`, use adaptive time-stepping.\n",
    "        adjoint_adaptive (bool, optional): If `True`, use adaptive time-stepping\n",
    "            for the backward adjoint solve.\n",
    "        rtol (float, optional): Relative tolerance.\n",
    "        adjoint_rtol (float, optional): Relative tolerance for backward adjoint\n",
    "            solve.\n",
    "        atol (float, optional): Absolute tolerance.\n",
    "        adjoint_atol (float, optional): Absolute tolerance for backward adjoint\n",
    "            solve.\n",
    "        dt_min (float, optional): Minimum step size during integration.\n",
    "        options (dict, optional): Dict of options for the integration method.\n",
    "        adjoint_options (dict, optional): Dict of options for the integration\n",
    "            method of the backward adjoint solve.\n",
    "        adjoint_params (Sequence of Tensors, optional): Tensors whose gradient\n",
    "            should be obtained with the adjoint. If not specified, defaults to\n",
    "            the parameters of `sde`.\n",
    "        names (dict, optional): Dict of method names for drift and diffusion.\n",
    "            Expected keys are \"drift\" and \"diffusion\". Serves so that users can\n",
    "            use methods with names not in `(\"f\", \"g\")`, e.g. to use the\n",
    "            method \"foo\" for the drift, we supply `names={\"drift\": \"foo\"}`.\n",
    "        logqp (bool, optional): If `True`, also return the log-ratio penalty.\n",
    "        extra (bool, optional): If `True`, also return the extra hidden state\n",
    "            used internally in the solver.\n",
    "        extra_solver_state: (tuple of Tensors, optional): Additional state to\n",
    "            initialise the solver with. Some solvers keep track of additional\n",
    "            state besides y0, and this offers a way to optionally initialise\n",
    "            that state.\n",
    "    Returns:\n",
    "        A single state tensor of size (T, batch_size, d).\n",
    "        if logqp is True, then the log-ratio penalty is also returned.\n",
    "        If extra is True, the any extra internal state of the solver is also\n",
    "        returned.\n",
    "    Raises:\n",
    "        ValueError: An error occurred due to unrecognized noise type/method,\n",
    "            or `sde` is missing required methods.\n",
    "    Note:\n",
    "        The backward pass is much more efficient with Stratonovich SDEs than\n",
    "        with Ito SDEs.\n",
    "    Note:\n",
    "        Double-backward is supported for Stratonovich SDEs. Doing so will use\n",
    "        the adjoint method to compute the gradient of the adjoint. (i.e. rather\n",
    "        than backpropagating through the numerical solver used for the\n",
    "        adjoint.) The same `adjoint_method`, `adjoint_adaptive`, `adjoint_rtol,\n",
    "        `adjoint_atol`, `adjoint_options` will be used for the second-order\n",
    "        adjoint as is used for the first-order adjoint.\n",
    "    \"\"\"\n",
    "    misc.handle_unused_kwargs(unused_kwargs, msg=\"`sdeint_adjoint`\")\n",
    "    del unused_kwargs\n",
    "\n",
    "    if adjoint_params is None and not isinstance(sde, nn.Module):\n",
    "        raise ValueError('`sde` must be an instance of nn.Module to specify the adjoint parameters; alternatively they '\n",
    "                         'can be specified explicitly via the `adjoint_params` argument. If there are no parameters '\n",
    "                         'then it is allowable to set `adjoint_params=()`.')\n",
    "\n",
    "    sde, y0, ts, bm, method, options = sdeint.check_contract(sde, y0, ts, bm, method, adaptive, options, names, logqp)\n",
    "    misc.assert_no_grad(['ts', 'dt', 'rtol', 'adjoint_rtol', 'atol', 'adjoint_atol', 'dt_min'],\n",
    "                        [ts, dt, rtol, adjoint_rtol, atol, adjoint_atol, dt_min])\n",
    "    adjoint_params = tuple(sde.parameters()) if adjoint_params is None else tuple(adjoint_params)\n",
    "    adjoint_params = filter(lambda x: x.requires_grad, adjoint_params)\n",
    "    adjoint_method = _select_default_adjoint_method(sde, method, adjoint_method)\n",
    "    adjoint_options = {} if adjoint_options is None else adjoint_options.copy()\n",
    "\n",
    "    # Note that all of these warnings are only applicable for reversible solvers with sdeint_adjoint; none of them\n",
    "    # apply to sdeint.\n",
    "    if method == METHODS.reversible_heun:\n",
    "        if adjoint_method != METHODS.adjoint_reversible_heun:\n",
    "            warnings.warn(f\"method={repr(method)}, but adjoint_method!={repr(METHODS.adjoint_reversible_heun)}.\")\n",
    "        if adaptive or adjoint_adaptive:\n",
    "            warnings.warn(f\"A limitation of the current method={repr(method)} implementation is \"\n",
    "                          f\"that it does not save the time steps used. This means that it may not be perfectly \"\n",
    "                          f\"accurate when used with `adaptive` or `adjoint_adaptive`.\")\n",
    "        else:\n",
    "            num_steps = (ts - ts[0]) / dt\n",
    "            if not torch.allclose(num_steps, num_steps.round()):\n",
    "                warnings.warn(f\"The spacing between time points `ts` is not an integer multiple of the time step `dt`. \"\n",
    "                              f\"This means that the backward pass (which is forced to step to each of `ts` to get \"\n",
    "                              f\"dL/dy(t) for t in ts) will not perfectly mimick the forward pass (which does not step \"\n",
    "                              f\"to each `ts`, and instead interpolates to them). This means that \"\n",
    "                              f\"method={repr(method)} may not be perfectly accurate.\")\n",
    "\n",
    "    solver_fn = methods.select(method=method, sde_type=sde.sde_type)\n",
    "    solver = solver_fn(\n",
    "        sde=sde,\n",
    "        bm=bm,\n",
    "        dt=dt,\n",
    "        adaptive=adaptive,\n",
    "        rtol=rtol,\n",
    "        atol=atol,\n",
    "        dt_min=dt_min,\n",
    "        options=options\n",
    "    )\n",
    "    if extra_solver_state is None:\n",
    "        extra_solver_state = solver.init_extra_solver_state(ts[0], y0)\n",
    "\n",
    "    ys, *extra_solver_state = _SdeintAdjointMethod.apply(\n",
    "        sde, ts, dt, bm, solver, method, adjoint_method, adjoint_adaptive, adjoint_rtol, adjoint_atol, dt_min,\n",
    "        adjoint_options, len(extra_solver_state), y0, *extra_solver_state, *adjoint_params\n",
    "    )\n",
    "\n",
    "    return sdeint.parse_return(y0, ys, extra_solver_state, extra, logqp)\n",
    "\n",
    "\n",
    "def _select_default_adjoint_method(sde: base_sde.ForwardSDE, method: str, adjoint_method: Optional[str]) -> str:\n",
    "    \"\"\"Select the default method for adjoint computation based on the noise type of the forward SDE.\"\"\"\n",
    "    if adjoint_method is not None:\n",
    "        return adjoint_method\n",
    "    elif method == METHODS.reversible_heun:\n",
    "        return METHODS.adjoint_reversible_heun\n",
    "    else:\n",
    "        return {\n",
    "            SDE_TYPES.ito: {\n",
    "                NOISE_TYPES.diagonal: METHODS.milstein,\n",
    "                NOISE_TYPES.additive: METHODS.euler,\n",
    "                NOISE_TYPES.scalar: METHODS.euler,\n",
    "                NOISE_TYPES.general: METHODS.euler,\n",
    "            }[sde.noise_type],\n",
    "            SDE_TYPES.stratonovich: METHODS.midpoint,\n",
    "        }[sde.sde_type]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
